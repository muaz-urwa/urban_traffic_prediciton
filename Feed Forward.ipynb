{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/urwa/Documents/side_projects/urban/data/featureData/jfk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, scheduler, criterion,epochs = 500):\n",
    "    losses = []\n",
    "    # Main optimization loop\n",
    "    for t in range(epochs):\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        y_predicted = model(X_train)\n",
    "\n",
    "        current_loss = criterion(y_predicted, y_train)\n",
    "\n",
    "        current_loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        print(f\"t = {t}, loss = {current_loss}\")\n",
    "\n",
    "        losses.append(current_loss)\n",
    "\n",
    "        scheduler.step()    \n",
    "    return losses,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8757, 1049)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>...</th>\n",
       "      <th>91_lag_3</th>\n",
       "      <th>92_lag_3</th>\n",
       "      <th>93_lag_3</th>\n",
       "      <th>94_lag_3</th>\n",
       "      <th>95_lag_3</th>\n",
       "      <th>96_lag_3</th>\n",
       "      <th>97_lag_3</th>\n",
       "      <th>98_lag_3</th>\n",
       "      <th>99_lag_3</th>\n",
       "      <th>arrival_lag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 1049 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Hour  1  10  100  101  102  106  107  108  ...  91_lag_3  \\\n",
       "0  2018-01-01     3  0   0    0    0    0    0    0    0  ...       1.0   \n",
       "1  2018-01-01     4  0   3    0    0    1    0    0    1  ...       4.0   \n",
       "2  2018-01-01     5  0   4    0    0    1    2    3    1  ...       0.0   \n",
       "\n",
       "   92_lag_3  93_lag_3  94_lag_3  95_lag_3  96_lag_3  97_lag_3  98_lag_3  \\\n",
       "0       1.0       0.0       1.0       6.0       0.0       1.0       0.0   \n",
       "1       1.0       0.0       0.0       2.0       0.0       0.0       0.0   \n",
       "2       0.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "\n",
       "   99_lag_3  arrival_lag_3  \n",
       "0       0.0            6.0  \n",
       "1       0.0            6.0  \n",
       "2       0.0            2.0  \n",
       "\n",
       "[3 rows x 1049 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Net(nn.Module):\n",
    "    def __init__(self, in_features,out_features):\n",
    "        super(Linear_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=out_features, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Net(nn.Module):\n",
    "    def __init__(self, in_features,out_features):\n",
    "        super(Simple_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=1000, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=1000, out_features=500, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=500, out_features=out_features, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = self.fc3(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag_columns = [c for c in dataset.columns if 'lag' in c]\n",
    "len(lag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DateColumns = ['Date']\n",
    "\n",
    "ext_columns = ['Dow', 'arrival','maxtemp', 'mintemp', 'avgtemp', 'departure', 'hdd',\n",
    "       'cdd', 'participation', 'newsnow', 'snowdepth', 'ifSnow']\n",
    "\n",
    "targetColumns = [c for c in dataset.columns if c not in ext_columns and \\\n",
    "                c not in DateColumns and c not in lag_columns and c != 'Hour']\n",
    "len(targetColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_cols = [c for c in dataset.columns if c not in targetColumns and c not in DateColumns]\n",
    "len(features_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset[features_cols].values\n",
    "y = dataset[targetColumns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_x = StandardScaler()\n",
    "# scaler_y = StandardScaler()\n",
    "\n",
    "# scaler_x.fit(x)\n",
    "# scaler_y.fit(y)\n",
    "\n",
    "# x = scaler_x.transform(x)\n",
    "# y = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8757, 790])\n",
      "torch.Size([8757, 258])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(x).float().to(device)\n",
    "print(x.shape)\n",
    "y = torch.tensor(y).float().to(device)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 3.888812780380249\n",
      "t = 1, loss = 3.4007480144500732\n",
      "t = 2, loss = 3.658876895904541\n",
      "t = 3, loss = 3.73158860206604\n",
      "t = 4, loss = 3.752540111541748\n",
      "t = 5, loss = 3.7756404876708984\n",
      "t = 6, loss = 3.7527732849121094\n",
      "t = 7, loss = 3.797473192214966\n",
      "t = 8, loss = 3.7566685676574707\n",
      "t = 9, loss = 3.8064260482788086\n",
      "t = 10, loss = 3.7481682300567627\n",
      "t = 11, loss = 3.8099567890167236\n",
      "t = 12, loss = 3.74222993850708\n",
      "t = 13, loss = 3.8148205280303955\n",
      "t = 14, loss = 3.7373712062835693\n",
      "t = 15, loss = 3.8204710483551025\n",
      "t = 16, loss = 3.73506236076355\n",
      "t = 17, loss = 3.8265957832336426\n",
      "t = 18, loss = 3.733041286468506\n",
      "t = 19, loss = 3.8308420181274414\n",
      "t = 20, loss = 3.729478359222412\n",
      "t = 21, loss = 3.8324778079986572\n",
      "t = 22, loss = 3.7257165908813477\n",
      "t = 23, loss = 3.8341257572174072\n",
      "t = 24, loss = 3.722379684448242\n",
      "t = 25, loss = 3.8350534439086914\n",
      "t = 26, loss = 3.719533681869507\n",
      "t = 27, loss = 3.836516857147217\n",
      "t = 28, loss = 3.7175676822662354\n",
      "t = 29, loss = 3.838395118713379\n",
      "t = 30, loss = 3.7158443927764893\n",
      "t = 31, loss = 3.8398597240448\n",
      "t = 32, loss = 3.7144196033477783\n",
      "t = 33, loss = 3.841404676437378\n",
      "t = 34, loss = 3.7128989696502686\n",
      "t = 35, loss = 3.8426105976104736\n",
      "t = 36, loss = 3.7117648124694824\n",
      "t = 37, loss = 3.8441436290740967\n",
      "t = 38, loss = 3.7104744911193848\n",
      "t = 39, loss = 3.846411943435669\n",
      "t = 40, loss = 3.710712194442749\n",
      "t = 41, loss = 3.848551034927368\n",
      "t = 42, loss = 3.7108960151672363\n",
      "t = 43, loss = 3.8506011962890625\n",
      "t = 44, loss = 3.7116074562072754\n",
      "t = 45, loss = 3.8526031970977783\n",
      "t = 46, loss = 3.7132813930511475\n",
      "t = 47, loss = 3.8554046154022217\n",
      "t = 48, loss = 3.7152633666992188\n",
      "t = 49, loss = 3.858492374420166\n",
      "t = 50, loss = 3.716946840286255\n",
      "t = 51, loss = 3.8614609241485596\n",
      "t = 52, loss = 3.7184410095214844\n",
      "t = 53, loss = 3.8634374141693115\n",
      "t = 54, loss = 3.7193052768707275\n",
      "t = 55, loss = 3.8654561042785645\n",
      "t = 56, loss = 3.7201695442199707\n",
      "t = 57, loss = 3.8667502403259277\n",
      "t = 58, loss = 3.72084903717041\n",
      "t = 59, loss = 3.8681488037109375\n",
      "t = 60, loss = 3.7216908931732178\n",
      "t = 61, loss = 3.869386911392212\n",
      "t = 62, loss = 3.7226390838623047\n",
      "t = 63, loss = 3.8708512783050537\n",
      "t = 64, loss = 3.7231767177581787\n",
      "t = 65, loss = 3.8719515800476074\n",
      "t = 66, loss = 3.7235419750213623\n",
      "t = 67, loss = 3.87276029586792\n",
      "t = 68, loss = 3.723829507827759\n",
      "t = 69, loss = 3.8737385272979736\n",
      "t = 70, loss = 3.7243735790252686\n",
      "t = 71, loss = 3.874875545501709\n",
      "t = 72, loss = 3.7246532440185547\n",
      "t = 73, loss = 3.8761959075927734\n",
      "t = 74, loss = 3.7247745990753174\n",
      "t = 75, loss = 3.8769371509552\n",
      "t = 76, loss = 3.7250189781188965\n",
      "t = 77, loss = 3.877988815307617\n",
      "t = 78, loss = 3.725433588027954\n",
      "t = 79, loss = 3.878713369369507\n",
      "t = 80, loss = 3.7263216972351074\n",
      "t = 81, loss = 3.8804149627685547\n",
      "t = 82, loss = 3.7274630069732666\n",
      "t = 83, loss = 3.881713628768921\n",
      "t = 84, loss = 3.7282512187957764\n",
      "t = 85, loss = 3.8826780319213867\n",
      "t = 86, loss = 3.729003429412842\n",
      "t = 87, loss = 3.8835182189941406\n",
      "t = 88, loss = 3.729635238647461\n",
      "t = 89, loss = 3.884746789932251\n",
      "t = 90, loss = 3.7304556369781494\n",
      "t = 91, loss = 3.8861827850341797\n",
      "t = 92, loss = 3.731125831604004\n",
      "t = 93, loss = 3.8874967098236084\n",
      "t = 94, loss = 3.731830358505249\n",
      "t = 95, loss = 3.8887112140655518\n",
      "t = 96, loss = 3.732025384902954\n",
      "t = 97, loss = 3.8896148204803467\n",
      "t = 98, loss = 3.7324328422546387\n",
      "t = 99, loss = 3.89037823677063\n",
      "t = 100, loss = 3.7326149940490723\n",
      "t = 101, loss = 3.0928056240081787\n",
      "t = 102, loss = 2.528244733810425\n",
      "t = 103, loss = 2.083333969116211\n",
      "t = 104, loss = 1.7875767946243286\n",
      "t = 105, loss = 1.6253814697265625\n",
      "t = 106, loss = 1.5507808923721313\n",
      "t = 107, loss = 1.5199235677719116\n",
      "t = 108, loss = 1.5068657398223877\n",
      "t = 109, loss = 1.50051748752594\n",
      "t = 110, loss = 1.4967564344406128\n",
      "t = 111, loss = 1.4940921068191528\n",
      "t = 112, loss = 1.4919586181640625\n",
      "t = 113, loss = 1.4901010990142822\n",
      "t = 114, loss = 1.4884090423583984\n",
      "t = 115, loss = 1.4868310689926147\n",
      "t = 116, loss = 1.485341191291809\n",
      "t = 117, loss = 1.4839234352111816\n",
      "t = 118, loss = 1.482568621635437\n",
      "t = 119, loss = 1.4812676906585693\n",
      "t = 120, loss = 1.4800149202346802\n",
      "t = 121, loss = 1.4788075685501099\n",
      "t = 122, loss = 1.477644681930542\n",
      "t = 123, loss = 1.4765231609344482\n",
      "t = 124, loss = 1.4754408597946167\n",
      "t = 125, loss = 1.474395751953125\n",
      "t = 126, loss = 1.4733846187591553\n",
      "t = 127, loss = 1.4724043607711792\n",
      "t = 128, loss = 1.47145414352417\n",
      "t = 129, loss = 1.4705318212509155\n",
      "t = 130, loss = 1.469636082649231\n",
      "t = 131, loss = 1.4687649011611938\n",
      "t = 132, loss = 1.467917799949646\n",
      "t = 133, loss = 1.4670929908752441\n",
      "t = 134, loss = 1.4662891626358032\n",
      "t = 135, loss = 1.465504765510559\n",
      "t = 136, loss = 1.464738130569458\n",
      "t = 137, loss = 1.463988184928894\n",
      "t = 138, loss = 1.4632551670074463\n",
      "t = 139, loss = 1.4625380039215088\n",
      "t = 140, loss = 1.4618358612060547\n",
      "t = 141, loss = 1.461147665977478\n",
      "t = 142, loss = 1.4604734182357788\n",
      "t = 143, loss = 1.4598116874694824\n",
      "t = 144, loss = 1.4591628313064575\n",
      "t = 145, loss = 1.4585262537002563\n",
      "t = 146, loss = 1.457900881767273\n",
      "t = 147, loss = 1.4572863578796387\n",
      "t = 148, loss = 1.456682562828064\n",
      "t = 149, loss = 1.456088662147522\n",
      "t = 150, loss = 1.4555041790008545\n",
      "t = 151, loss = 1.4549294710159302\n",
      "t = 152, loss = 1.4543639421463013\n",
      "t = 153, loss = 1.4538074731826782\n",
      "t = 154, loss = 1.4532594680786133\n",
      "t = 155, loss = 1.4527195692062378\n",
      "t = 156, loss = 1.4521875381469727\n",
      "t = 157, loss = 1.4516637325286865\n",
      "t = 158, loss = 1.4511475563049316\n",
      "t = 159, loss = 1.4506391286849976\n",
      "t = 160, loss = 1.4501372575759888\n",
      "t = 161, loss = 1.4496421813964844\n",
      "t = 162, loss = 1.4491530656814575\n",
      "t = 163, loss = 1.4486703872680664\n",
      "t = 164, loss = 1.448193907737732\n",
      "t = 165, loss = 1.4477237462997437\n",
      "t = 166, loss = 1.447259545326233\n",
      "t = 167, loss = 1.446800708770752\n",
      "t = 168, loss = 1.4463473558425903\n",
      "t = 169, loss = 1.445899248123169\n",
      "t = 170, loss = 1.445456624031067\n",
      "t = 171, loss = 1.4450186491012573\n",
      "t = 172, loss = 1.4445858001708984\n",
      "t = 173, loss = 1.4441574811935425\n",
      "t = 174, loss = 1.443734049797058\n",
      "t = 175, loss = 1.4433146715164185\n",
      "t = 176, loss = 1.4428998231887817\n",
      "t = 177, loss = 1.4424890279769897\n",
      "t = 178, loss = 1.4420822858810425\n",
      "t = 179, loss = 1.4416801929473877\n",
      "t = 180, loss = 1.4412827491760254\n",
      "t = 181, loss = 1.4408893585205078\n",
      "t = 182, loss = 1.4404999017715454\n",
      "t = 183, loss = 1.440114140510559\n",
      "t = 184, loss = 1.4397324323654175\n",
      "t = 185, loss = 1.4393543004989624\n",
      "t = 186, loss = 1.4389797449111938\n",
      "t = 187, loss = 1.4386091232299805\n",
      "t = 188, loss = 1.4382421970367432\n",
      "t = 189, loss = 1.4378786087036133\n",
      "t = 190, loss = 1.4375183582305908\n",
      "t = 191, loss = 1.4371613264083862\n",
      "t = 192, loss = 1.436807632446289\n",
      "t = 193, loss = 1.4364572763442993\n",
      "t = 194, loss = 1.4361097812652588\n",
      "t = 195, loss = 1.4357653856277466\n",
      "t = 196, loss = 1.4354239702224731\n",
      "t = 197, loss = 1.4350850582122803\n",
      "t = 198, loss = 1.4347491264343262\n",
      "t = 199, loss = 1.4344161748886108\n",
      "t = 200, loss = 1.4340859651565552\n",
      "t = 201, loss = 1.4340531826019287\n",
      "t = 202, loss = 1.4340202808380127\n",
      "t = 203, loss = 1.4339876174926758\n",
      "t = 204, loss = 1.4339548349380493\n",
      "t = 205, loss = 1.4339221715927124\n",
      "t = 206, loss = 1.4338895082473755\n",
      "t = 207, loss = 1.4338568449020386\n",
      "t = 208, loss = 1.4338243007659912\n",
      "t = 209, loss = 1.4337915182113647\n",
      "t = 210, loss = 1.4337589740753174\n",
      "t = 211, loss = 1.43372642993927\n",
      "t = 212, loss = 1.4336940050125122\n",
      "t = 213, loss = 1.4336614608764648\n",
      "t = 214, loss = 1.433628797531128\n",
      "t = 215, loss = 1.4335962533950806\n",
      "t = 216, loss = 1.4335640668869019\n",
      "t = 217, loss = 1.433531403541565\n",
      "t = 218, loss = 1.4334992170333862\n",
      "t = 219, loss = 1.4334667921066284\n",
      "t = 220, loss = 1.4334343671798706\n",
      "t = 221, loss = 1.433402180671692\n",
      "t = 222, loss = 1.4333698749542236\n",
      "t = 223, loss = 1.433337688446045\n",
      "t = 224, loss = 1.4333053827285767\n",
      "t = 225, loss = 1.4332730770111084\n",
      "t = 226, loss = 1.4332412481307983\n",
      "t = 227, loss = 1.4332088232040405\n",
      "t = 228, loss = 1.4331766366958618\n",
      "t = 229, loss = 1.4331445693969727\n",
      "t = 230, loss = 1.4331125020980835\n",
      "t = 231, loss = 1.4330804347991943\n",
      "t = 232, loss = 1.4330483675003052\n",
      "t = 233, loss = 1.4330164194107056\n",
      "t = 234, loss = 1.432984471321106\n",
      "t = 235, loss = 1.4329525232315063\n",
      "t = 236, loss = 1.4329205751419067\n",
      "t = 237, loss = 1.4328886270523071\n",
      "t = 238, loss = 1.4328566789627075\n",
      "t = 239, loss = 1.432824969291687\n",
      "t = 240, loss = 1.432793140411377\n",
      "t = 241, loss = 1.4327611923217773\n",
      "t = 242, loss = 1.4327294826507568\n",
      "t = 243, loss = 1.4326976537704468\n",
      "t = 244, loss = 1.4326660633087158\n",
      "t = 245, loss = 1.4326342344284058\n",
      "t = 246, loss = 1.4326025247573853\n",
      "t = 247, loss = 1.4325708150863647\n",
      "t = 248, loss = 1.4325392246246338\n",
      "t = 249, loss = 1.4325076341629028\n",
      "t = 250, loss = 1.4324760437011719\n",
      "t = 251, loss = 1.4324443340301514\n",
      "t = 252, loss = 1.43241286277771\n",
      "t = 253, loss = 1.4323813915252686\n",
      "t = 254, loss = 1.4323498010635376\n",
      "t = 255, loss = 1.4323183298110962\n",
      "t = 256, loss = 1.4322868585586548\n",
      "t = 257, loss = 1.4322556257247925\n",
      "t = 258, loss = 1.4322240352630615\n",
      "t = 259, loss = 1.4321926832199097\n",
      "t = 260, loss = 1.4321613311767578\n",
      "t = 261, loss = 1.4321300983428955\n",
      "t = 262, loss = 1.4320987462997437\n",
      "t = 263, loss = 1.4320672750473022\n",
      "t = 264, loss = 1.43203604221344\n",
      "t = 265, loss = 1.4320048093795776\n",
      "t = 266, loss = 1.4319735765457153\n",
      "t = 267, loss = 1.4319424629211426\n",
      "t = 268, loss = 1.4319112300872803\n",
      "t = 269, loss = 1.431879997253418\n",
      "t = 270, loss = 1.4318490028381348\n",
      "t = 271, loss = 1.4318177700042725\n",
      "t = 272, loss = 1.4317866563796997\n",
      "t = 273, loss = 1.4317556619644165\n",
      "t = 274, loss = 1.4317245483398438\n",
      "t = 275, loss = 1.4316935539245605\n",
      "t = 276, loss = 1.4316625595092773\n",
      "t = 277, loss = 1.4316315650939941\n",
      "t = 278, loss = 1.431600570678711\n",
      "t = 279, loss = 1.4315698146820068\n",
      "t = 280, loss = 1.4315388202667236\n",
      "t = 281, loss = 1.43150794506073\n",
      "t = 282, loss = 1.4314769506454468\n",
      "t = 283, loss = 1.4314460754394531\n",
      "t = 284, loss = 1.431415319442749\n",
      "t = 285, loss = 1.4313844442367554\n",
      "t = 286, loss = 1.4313538074493408\n",
      "t = 287, loss = 1.4313229322433472\n",
      "t = 288, loss = 1.431292176246643\n",
      "t = 289, loss = 1.431261420249939\n",
      "t = 290, loss = 1.4312307834625244\n",
      "t = 291, loss = 1.4312001466751099\n",
      "t = 292, loss = 1.4311695098876953\n",
      "t = 293, loss = 1.4311387538909912\n",
      "t = 294, loss = 1.4311081171035767\n",
      "t = 295, loss = 1.4310775995254517\n",
      "t = 296, loss = 1.4310470819473267\n",
      "t = 297, loss = 1.431016445159912\n",
      "t = 298, loss = 1.4309860467910767\n",
      "t = 299, loss = 1.4309555292129517\n",
      "t = 300, loss = 1.4309250116348267\n",
      "t = 301, loss = 1.4309219121932983\n",
      "t = 302, loss = 1.4309190511703491\n",
      "t = 303, loss = 1.4309158325195312\n",
      "t = 304, loss = 1.430912733078003\n",
      "t = 305, loss = 1.4309098720550537\n",
      "t = 306, loss = 1.4309066534042358\n",
      "t = 307, loss = 1.430903673171997\n",
      "t = 308, loss = 1.4309006929397583\n",
      "t = 309, loss = 1.4308977127075195\n",
      "t = 310, loss = 1.4308947324752808\n",
      "t = 311, loss = 1.430891513824463\n",
      "t = 312, loss = 1.4308885335922241\n",
      "t = 313, loss = 1.4308853149414062\n",
      "t = 314, loss = 1.4308823347091675\n",
      "t = 315, loss = 1.4308792352676392\n",
      "t = 316, loss = 1.43087637424469\n",
      "t = 317, loss = 1.430873155593872\n",
      "t = 318, loss = 1.4308701753616333\n",
      "t = 319, loss = 1.4308671951293945\n",
      "t = 320, loss = 1.4308643341064453\n",
      "t = 321, loss = 1.4308611154556274\n",
      "t = 322, loss = 1.4308580160140991\n",
      "t = 323, loss = 1.4308550357818604\n",
      "t = 324, loss = 1.430851936340332\n",
      "t = 325, loss = 1.4308488368988037\n",
      "t = 326, loss = 1.430845856666565\n",
      "t = 327, loss = 1.4308428764343262\n",
      "t = 328, loss = 1.4308397769927979\n",
      "t = 329, loss = 1.430836796760559\n",
      "t = 330, loss = 1.4308336973190308\n",
      "t = 331, loss = 1.4308308362960815\n",
      "t = 332, loss = 1.4308276176452637\n",
      "t = 333, loss = 1.4308245182037354\n",
      "t = 334, loss = 1.4308216571807861\n",
      "t = 335, loss = 1.4308184385299683\n",
      "t = 336, loss = 1.4308154582977295\n",
      "t = 337, loss = 1.4308124780654907\n",
      "t = 338, loss = 1.4308092594146729\n",
      "t = 339, loss = 1.4308065176010132\n",
      "t = 340, loss = 1.4308032989501953\n",
      "t = 341, loss = 1.4308000802993774\n",
      "t = 342, loss = 1.4307973384857178\n",
      "t = 343, loss = 1.4307942390441895\n",
      "t = 344, loss = 1.4307911396026611\n",
      "t = 345, loss = 1.4307881593704224\n",
      "t = 346, loss = 1.430785059928894\n",
      "t = 347, loss = 1.4307820796966553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 348, loss = 1.430778980255127\n",
      "t = 349, loss = 1.4307760000228882\n",
      "t = 350, loss = 1.4307729005813599\n",
      "t = 351, loss = 1.4307698011398315\n",
      "t = 352, loss = 1.4307669401168823\n",
      "t = 353, loss = 1.4307637214660645\n",
      "t = 354, loss = 1.4307607412338257\n",
      "t = 355, loss = 1.430757761001587\n",
      "t = 356, loss = 1.4307547807693481\n",
      "t = 357, loss = 1.4307518005371094\n",
      "t = 358, loss = 1.4307485818862915\n",
      "t = 359, loss = 1.4307457208633423\n",
      "t = 360, loss = 1.430742621421814\n",
      "t = 361, loss = 1.4307397603988647\n",
      "t = 362, loss = 1.4307365417480469\n",
      "t = 363, loss = 1.430733561515808\n",
      "t = 364, loss = 1.4307305812835693\n",
      "t = 365, loss = 1.4307273626327515\n",
      "t = 366, loss = 1.4307246208190918\n",
      "t = 367, loss = 1.430721402168274\n",
      "t = 368, loss = 1.430718183517456\n",
      "t = 369, loss = 1.4307154417037964\n",
      "t = 370, loss = 1.4307122230529785\n",
      "t = 371, loss = 1.4307093620300293\n",
      "t = 372, loss = 1.430706262588501\n",
      "t = 373, loss = 1.4307031631469727\n",
      "t = 374, loss = 1.4307001829147339\n",
      "t = 375, loss = 1.4306970834732056\n",
      "t = 376, loss = 1.4306942224502563\n",
      "t = 377, loss = 1.4306910037994385\n",
      "t = 378, loss = 1.4306881427764893\n",
      "t = 379, loss = 1.430685043334961\n",
      "t = 380, loss = 1.430681824684143\n",
      "t = 381, loss = 1.4306790828704834\n",
      "t = 382, loss = 1.4306758642196655\n",
      "t = 383, loss = 1.4306730031967163\n",
      "t = 384, loss = 1.430669903755188\n",
      "t = 385, loss = 1.4306670427322388\n",
      "t = 386, loss = 1.430663824081421\n",
      "t = 387, loss = 1.4306607246398926\n",
      "t = 388, loss = 1.4306578636169434\n",
      "t = 389, loss = 1.4306548833847046\n",
      "t = 390, loss = 1.4306516647338867\n",
      "t = 391, loss = 1.430648684501648\n",
      "t = 392, loss = 1.4306458234786987\n",
      "t = 393, loss = 1.4306427240371704\n",
      "t = 394, loss = 1.4306395053863525\n",
      "t = 395, loss = 1.4306366443634033\n",
      "t = 396, loss = 1.430633544921875\n",
      "t = 397, loss = 1.4306306838989258\n",
      "t = 398, loss = 1.430627465248108\n",
      "t = 399, loss = 1.4306244850158691\n",
      "t = 400, loss = 1.4306215047836304\n",
      "t = 401, loss = 1.4306211471557617\n",
      "t = 402, loss = 1.4306209087371826\n",
      "t = 403, loss = 1.430620551109314\n",
      "t = 404, loss = 1.4306204319000244\n",
      "t = 405, loss = 1.4306200742721558\n",
      "t = 406, loss = 1.430619716644287\n",
      "t = 407, loss = 1.430619478225708\n",
      "t = 408, loss = 1.4306191205978394\n",
      "t = 409, loss = 1.4306187629699707\n",
      "t = 410, loss = 1.4306185245513916\n",
      "t = 411, loss = 1.430618166923523\n",
      "t = 412, loss = 1.4306178092956543\n",
      "t = 413, loss = 1.4306175708770752\n",
      "t = 414, loss = 1.4306172132492065\n",
      "t = 415, loss = 1.430616855621338\n",
      "t = 416, loss = 1.4306166172027588\n",
      "t = 417, loss = 1.4306162595748901\n",
      "t = 418, loss = 1.4306161403656006\n",
      "t = 419, loss = 1.430615782737732\n",
      "t = 420, loss = 1.4306154251098633\n",
      "t = 421, loss = 1.4306151866912842\n",
      "t = 422, loss = 1.4306148290634155\n",
      "t = 423, loss = 1.4306144714355469\n",
      "t = 424, loss = 1.4306142330169678\n",
      "t = 425, loss = 1.4306138753890991\n",
      "t = 426, loss = 1.4306137561798096\n",
      "t = 427, loss = 1.4306132793426514\n",
      "t = 428, loss = 1.4306129217147827\n",
      "t = 429, loss = 1.4306128025054932\n",
      "t = 430, loss = 1.4306124448776245\n",
      "t = 431, loss = 1.4306122064590454\n",
      "t = 432, loss = 1.4306118488311768\n",
      "t = 433, loss = 1.430611491203308\n",
      "t = 434, loss = 1.4306113719940186\n",
      "t = 435, loss = 1.43061101436615\n",
      "t = 436, loss = 1.4306107759475708\n",
      "t = 437, loss = 1.4306104183197021\n",
      "t = 438, loss = 1.4306100606918335\n",
      "t = 439, loss = 1.4306098222732544\n",
      "t = 440, loss = 1.4306094646453857\n",
      "t = 441, loss = 1.430609107017517\n",
      "t = 442, loss = 1.430608868598938\n",
      "t = 443, loss = 1.4306085109710693\n",
      "t = 444, loss = 1.4306081533432007\n",
      "t = 445, loss = 1.4306079149246216\n",
      "t = 446, loss = 1.430607557296753\n",
      "t = 447, loss = 1.4306071996688843\n",
      "t = 448, loss = 1.4306069612503052\n",
      "t = 449, loss = 1.4306066036224365\n",
      "t = 450, loss = 1.430606484413147\n",
      "t = 451, loss = 1.4306061267852783\n",
      "t = 452, loss = 1.4306058883666992\n",
      "t = 453, loss = 1.4306055307388306\n",
      "t = 454, loss = 1.430605411529541\n",
      "t = 455, loss = 1.4306050539016724\n",
      "t = 456, loss = 1.4306046962738037\n",
      "t = 457, loss = 1.4306044578552246\n",
      "t = 458, loss = 1.430604100227356\n",
      "t = 459, loss = 1.4306037425994873\n",
      "t = 460, loss = 1.4306035041809082\n",
      "t = 461, loss = 1.4306031465530396\n",
      "t = 462, loss = 1.430602788925171\n",
      "t = 463, loss = 1.4306025505065918\n",
      "t = 464, loss = 1.4306021928787231\n",
      "t = 465, loss = 1.4306018352508545\n",
      "t = 466, loss = 1.4306015968322754\n",
      "t = 467, loss = 1.4306012392044067\n",
      "t = 468, loss = 1.430600881576538\n",
      "t = 469, loss = 1.430600643157959\n",
      "t = 470, loss = 1.4306002855300903\n",
      "t = 471, loss = 1.4305999279022217\n",
      "t = 472, loss = 1.4305998086929321\n",
      "t = 473, loss = 1.4305994510650635\n",
      "t = 474, loss = 1.4305992126464844\n",
      "t = 475, loss = 1.4305988550186157\n",
      "t = 476, loss = 1.4305987358093262\n",
      "t = 477, loss = 1.4305983781814575\n",
      "t = 478, loss = 1.4305981397628784\n",
      "t = 479, loss = 1.4305977821350098\n",
      "t = 480, loss = 1.4305974245071411\n",
      "t = 481, loss = 1.430597186088562\n",
      "t = 482, loss = 1.4305968284606934\n",
      "t = 483, loss = 1.4305964708328247\n",
      "t = 484, loss = 1.4305962324142456\n",
      "t = 485, loss = 1.430595874786377\n",
      "t = 486, loss = 1.4305955171585083\n",
      "t = 487, loss = 1.4305952787399292\n",
      "t = 488, loss = 1.4305949211120605\n",
      "t = 489, loss = 1.430594563484192\n",
      "t = 490, loss = 1.4305943250656128\n",
      "t = 491, loss = 1.4305940866470337\n",
      "t = 492, loss = 1.4305938482284546\n",
      "t = 493, loss = 1.430593490600586\n",
      "t = 494, loss = 1.4305931329727173\n",
      "t = 495, loss = 1.4305928945541382\n",
      "t = 496, loss = 1.4305925369262695\n",
      "t = 497, loss = 1.4305921792984009\n",
      "t = 498, loss = 1.4305919408798218\n",
      "t = 499, loss = 1.4305915832519531\n",
      "Training R2:  0.3961936458646454\n",
      "Test R2:  0.38943107906204677\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_feat = len(features_cols)\n",
    "out_feat = len(targetColumns)\n",
    "model = Linear_Net(in_features=in_feat ,out_features=out_feat).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=100, gamma=0.1)\n",
    "\n",
    "losses,model = train(model, optimiser, scheduler, criterion, epochs = 500)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "    y_pred_train = model(X_train)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "\n",
    "train_r2 = r2_score(y_train.detach().cpu().numpy(), y_pred_train.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "test_r2=r2_score(y_test.detach().cpu().numpy(), y_pred_test.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "\n",
    "print(\"Training R2: \",train_r2)\n",
    "print(\"Test R2: \",test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 2.4907822608947754\n",
      "t = 1, loss = 2.269265651702881\n",
      "t = 2, loss = 2.169722557067871\n",
      "t = 3, loss = 2.0588111877441406\n",
      "t = 4, loss = 1.9257808923721313\n",
      "t = 5, loss = 1.809634804725647\n",
      "t = 6, loss = 1.7300784587860107\n",
      "t = 7, loss = 1.6963605880737305\n",
      "t = 8, loss = 1.6447018384933472\n",
      "t = 9, loss = 1.6009159088134766\n",
      "t = 10, loss = 1.554304838180542\n",
      "t = 11, loss = 1.5119423866271973\n",
      "t = 12, loss = 1.4857685565948486\n",
      "t = 13, loss = 1.4713232517242432\n",
      "t = 14, loss = 1.4607913494110107\n",
      "t = 15, loss = 1.4213865995407104\n",
      "t = 16, loss = 1.4061123132705688\n",
      "t = 17, loss = 1.3891804218292236\n",
      "t = 18, loss = 1.382995367050171\n",
      "t = 19, loss = 1.3720481395721436\n",
      "t = 20, loss = 1.3648608922958374\n",
      "t = 21, loss = 1.354169487953186\n",
      "t = 22, loss = 1.3439302444458008\n",
      "t = 23, loss = 1.3285034894943237\n",
      "t = 24, loss = 1.3168036937713623\n",
      "t = 25, loss = 1.2995115518569946\n",
      "t = 26, loss = 1.2947689294815063\n",
      "t = 27, loss = 1.2894902229309082\n",
      "t = 28, loss = 1.2845678329467773\n",
      "t = 29, loss = 1.2767572402954102\n",
      "t = 30, loss = 1.2671716213226318\n",
      "t = 31, loss = 1.2655528783798218\n",
      "t = 32, loss = 1.2571237087249756\n",
      "t = 33, loss = 1.2654114961624146\n",
      "t = 34, loss = 1.255415916442871\n",
      "t = 35, loss = 1.2900047302246094\n",
      "t = 36, loss = 1.2886725664138794\n",
      "t = 37, loss = 1.4022976160049438\n",
      "t = 38, loss = 1.3705263137817383\n",
      "t = 39, loss = 1.562221646308899\n",
      "t = 40, loss = 1.2748233079910278\n",
      "t = 41, loss = 1.3713032007217407\n",
      "t = 42, loss = 1.2738091945648193\n",
      "t = 43, loss = 1.3598700761795044\n",
      "t = 44, loss = 1.2611712217330933\n",
      "t = 45, loss = 1.3350569009780884\n",
      "t = 46, loss = 1.2610833644866943\n",
      "t = 47, loss = 1.349780797958374\n",
      "t = 48, loss = 1.2551475763320923\n",
      "t = 49, loss = 1.339158535003662\n",
      "t = 50, loss = 1.2519866228103638\n",
      "t = 51, loss = 1.3386046886444092\n",
      "t = 52, loss = 1.244331955909729\n",
      "t = 53, loss = 1.3160085678100586\n",
      "t = 54, loss = 1.2555471658706665\n",
      "t = 55, loss = 1.3296979665756226\n",
      "t = 56, loss = 1.2555930614471436\n",
      "t = 57, loss = 1.3422845602035522\n",
      "t = 58, loss = 1.2413991689682007\n",
      "t = 59, loss = 1.3185003995895386\n",
      "t = 60, loss = 1.2466979026794434\n",
      "t = 61, loss = 1.331040859222412\n",
      "t = 62, loss = 1.2511873245239258\n",
      "t = 63, loss = 1.3357402086257935\n",
      "t = 64, loss = 1.248606562614441\n",
      "t = 65, loss = 1.3356602191925049\n",
      "t = 66, loss = 1.236342430114746\n",
      "t = 67, loss = 1.3145275115966797\n",
      "t = 68, loss = 1.2453199625015259\n",
      "t = 69, loss = 1.3182826042175293\n",
      "t = 70, loss = 1.2318329811096191\n",
      "t = 71, loss = 1.3040932416915894\n",
      "t = 72, loss = 1.240595817565918\n",
      "t = 73, loss = 1.3225969076156616\n",
      "t = 74, loss = 1.2286332845687866\n",
      "t = 75, loss = 1.2924385070800781\n",
      "t = 76, loss = 1.2299418449401855\n",
      "t = 77, loss = 1.305782437324524\n",
      "t = 78, loss = 1.2208762168884277\n",
      "t = 79, loss = 1.286167025566101\n",
      "t = 80, loss = 1.2305184602737427\n",
      "t = 81, loss = 1.294419288635254\n",
      "t = 82, loss = 1.2280648946762085\n",
      "t = 83, loss = 1.2904026508331299\n",
      "t = 84, loss = 1.230715036392212\n",
      "t = 85, loss = 1.30068039894104\n",
      "t = 86, loss = 1.224579930305481\n",
      "t = 87, loss = 1.2926528453826904\n",
      "t = 88, loss = 1.228755235671997\n",
      "t = 89, loss = 1.3023442029953003\n",
      "t = 90, loss = 1.2308789491653442\n",
      "t = 91, loss = 1.306876301765442\n",
      "t = 92, loss = 1.2192323207855225\n",
      "t = 93, loss = 1.2836408615112305\n",
      "t = 94, loss = 1.2275923490524292\n",
      "t = 95, loss = 1.3063123226165771\n",
      "t = 96, loss = 1.2145370244979858\n",
      "t = 97, loss = 1.2831476926803589\n",
      "t = 98, loss = 1.2260210514068604\n",
      "t = 99, loss = 1.2852246761322021\n",
      "t = 100, loss = 1.2164398431777954\n",
      "t = 101, loss = 1.188355565071106\n",
      "t = 102, loss = 1.1726815700531006\n",
      "t = 103, loss = 1.163948655128479\n",
      "t = 104, loss = 1.1590481996536255\n",
      "t = 105, loss = 1.1561182737350464\n",
      "t = 106, loss = 1.1542565822601318\n",
      "t = 107, loss = 1.1529982089996338\n",
      "t = 108, loss = 1.1521376371383667\n",
      "t = 109, loss = 1.1515475511550903\n",
      "t = 110, loss = 1.1511515378952026\n",
      "t = 111, loss = 1.1508864164352417\n",
      "t = 112, loss = 1.1506983041763306\n",
      "t = 113, loss = 1.150554895401001\n",
      "t = 114, loss = 1.1504347324371338\n",
      "t = 115, loss = 1.1503276824951172\n",
      "t = 116, loss = 1.1502294540405273\n",
      "t = 117, loss = 1.1501370668411255\n",
      "t = 118, loss = 1.1500486135482788\n",
      "t = 119, loss = 1.149963140487671\n",
      "t = 120, loss = 1.1498795747756958\n",
      "t = 121, loss = 1.1497976779937744\n",
      "t = 122, loss = 1.1497172117233276\n",
      "t = 123, loss = 1.1496379375457764\n",
      "t = 124, loss = 1.1495596170425415\n",
      "t = 125, loss = 1.1494823694229126\n",
      "t = 126, loss = 1.149405837059021\n",
      "t = 127, loss = 1.1493301391601562\n",
      "t = 128, loss = 1.1492551565170288\n",
      "t = 129, loss = 1.1491806507110596\n",
      "t = 130, loss = 1.1491068601608276\n",
      "t = 131, loss = 1.149033546447754\n",
      "t = 132, loss = 1.1489605903625488\n",
      "t = 133, loss = 1.148888349533081\n",
      "t = 134, loss = 1.148816466331482\n",
      "t = 135, loss = 1.1487449407577515\n",
      "t = 136, loss = 1.1486740112304688\n",
      "t = 137, loss = 1.1486033201217651\n",
      "t = 138, loss = 1.1485332250595093\n",
      "t = 139, loss = 1.1484633684158325\n",
      "t = 140, loss = 1.148393988609314\n",
      "t = 141, loss = 1.1483248472213745\n",
      "t = 142, loss = 1.1482561826705933\n",
      "t = 143, loss = 1.1481876373291016\n",
      "t = 144, loss = 1.1481194496154785\n",
      "t = 145, loss = 1.1480516195297241\n",
      "t = 146, loss = 1.1479841470718384\n",
      "t = 147, loss = 1.1479170322418213\n",
      "t = 148, loss = 1.1478501558303833\n",
      "t = 149, loss = 1.1477835178375244\n",
      "t = 150, loss = 1.1477173566818237\n",
      "t = 151, loss = 1.1476514339447021\n",
      "t = 152, loss = 1.1475857496261597\n",
      "t = 153, loss = 1.1475203037261963\n",
      "t = 154, loss = 1.1474552154541016\n",
      "t = 155, loss = 1.1473902463912964\n",
      "t = 156, loss = 1.1473256349563599\n",
      "t = 157, loss = 1.147261142730713\n",
      "t = 158, loss = 1.147196888923645\n",
      "t = 159, loss = 1.1471328735351562\n",
      "t = 160, loss = 1.1470690965652466\n",
      "t = 161, loss = 1.1470054388046265\n",
      "t = 162, loss = 1.1469420194625854\n",
      "t = 163, loss = 1.146878719329834\n",
      "t = 164, loss = 1.1468158960342407\n",
      "t = 165, loss = 1.1467533111572266\n",
      "t = 166, loss = 1.146690845489502\n",
      "t = 167, loss = 1.1466286182403564\n",
      "t = 168, loss = 1.1465665102005005\n",
      "t = 169, loss = 1.1465047597885132\n",
      "t = 170, loss = 1.1464430093765259\n",
      "t = 171, loss = 1.1463814973831177\n",
      "t = 172, loss = 1.1463203430175781\n",
      "t = 173, loss = 1.1462591886520386\n",
      "t = 174, loss = 1.1461982727050781\n",
      "t = 175, loss = 1.1461374759674072\n",
      "t = 176, loss = 1.1460769176483154\n",
      "t = 177, loss = 1.1460163593292236\n",
      "t = 178, loss = 1.145956039428711\n",
      "t = 179, loss = 1.1458957195281982\n",
      "t = 180, loss = 1.1458356380462646\n",
      "t = 181, loss = 1.145775556564331\n",
      "t = 182, loss = 1.1457158327102661\n",
      "t = 183, loss = 1.1456559896469116\n",
      "t = 184, loss = 1.1455963850021362\n",
      "t = 185, loss = 1.14553701877594\n",
      "t = 186, loss = 1.1454776525497437\n",
      "t = 187, loss = 1.1454185247421265\n",
      "t = 188, loss = 1.1453593969345093\n",
      "t = 189, loss = 1.1453003883361816\n",
      "t = 190, loss = 1.145241618156433\n",
      "t = 191, loss = 1.1451828479766846\n",
      "t = 192, loss = 1.1451241970062256\n",
      "t = 193, loss = 1.1450655460357666\n",
      "t = 194, loss = 1.1450071334838867\n",
      "t = 195, loss = 1.1449488401412964\n",
      "t = 196, loss = 1.144890546798706\n",
      "t = 197, loss = 1.1448323726654053\n",
      "t = 198, loss = 1.1447741985321045\n",
      "t = 199, loss = 1.1447161436080933\n",
      "t = 200, loss = 1.1446582078933716\n",
      "t = 201, loss = 1.1446523666381836\n",
      "t = 202, loss = 1.1446467638015747\n",
      "t = 203, loss = 1.1446409225463867\n",
      "t = 204, loss = 1.1446352005004883\n",
      "t = 205, loss = 1.1446293592453003\n",
      "t = 206, loss = 1.1446235179901123\n",
      "t = 207, loss = 1.1446177959442139\n",
      "t = 208, loss = 1.1446119546890259\n",
      "t = 209, loss = 1.1446062326431274\n",
      "t = 210, loss = 1.1446003913879395\n",
      "t = 211, loss = 1.144594669342041\n",
      "t = 212, loss = 1.144588828086853\n",
      "t = 213, loss = 1.1445831060409546\n",
      "t = 214, loss = 1.1445772647857666\n",
      "t = 215, loss = 1.1445715427398682\n",
      "t = 216, loss = 1.1445658206939697\n",
      "t = 217, loss = 1.1445600986480713\n",
      "t = 218, loss = 1.1445542573928833\n",
      "t = 219, loss = 1.1445485353469849\n",
      "t = 220, loss = 1.1445426940917969\n",
      "t = 221, loss = 1.1445369720458984\n",
      "t = 222, loss = 1.1445311307907104\n",
      "t = 223, loss = 1.144525408744812\n",
      "t = 224, loss = 1.144519567489624\n",
      "t = 225, loss = 1.1445139646530151\n",
      "t = 226, loss = 1.1445081233978271\n",
      "t = 227, loss = 1.1445024013519287\n",
      "t = 228, loss = 1.1444965600967407\n",
      "t = 229, loss = 1.1444908380508423\n",
      "t = 230, loss = 1.1444851160049438\n",
      "t = 231, loss = 1.1444792747497559\n",
      "t = 232, loss = 1.1444735527038574\n",
      "t = 233, loss = 1.144467830657959\n",
      "t = 234, loss = 1.1444621086120605\n",
      "t = 235, loss = 1.144456386566162\n",
      "t = 236, loss = 1.1444505453109741\n",
      "t = 237, loss = 1.1444448232650757\n",
      "t = 238, loss = 1.1444391012191772\n",
      "t = 239, loss = 1.1444332599639893\n",
      "t = 240, loss = 1.1444275379180908\n",
      "t = 241, loss = 1.1444216966629028\n",
      "t = 242, loss = 1.144416093826294\n",
      "t = 243, loss = 1.144410252571106\n",
      "t = 244, loss = 1.1444045305252075\n",
      "t = 245, loss = 1.144398808479309\n",
      "t = 246, loss = 1.1443930864334106\n",
      "t = 247, loss = 1.1443872451782227\n",
      "t = 248, loss = 1.1443816423416138\n",
      "t = 249, loss = 1.1443758010864258\n",
      "t = 250, loss = 1.1443700790405273\n",
      "t = 251, loss = 1.144364356994629\n",
      "t = 252, loss = 1.1443586349487305\n",
      "t = 253, loss = 1.1443527936935425\n",
      "t = 254, loss = 1.144347071647644\n",
      "t = 255, loss = 1.1443414688110352\n",
      "t = 256, loss = 1.1443356275558472\n",
      "t = 257, loss = 1.1443299055099487\n",
      "t = 258, loss = 1.1443241834640503\n",
      "t = 259, loss = 1.1443184614181519\n",
      "t = 260, loss = 1.1443126201629639\n",
      "t = 261, loss = 1.144307017326355\n",
      "t = 262, loss = 1.1443012952804565\n",
      "t = 263, loss = 1.1442954540252686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 264, loss = 1.1442898511886597\n",
      "t = 265, loss = 1.1442840099334717\n",
      "t = 266, loss = 1.1442782878875732\n",
      "t = 267, loss = 1.1442726850509644\n",
      "t = 268, loss = 1.1442668437957764\n",
      "t = 269, loss = 1.1442612409591675\n",
      "t = 270, loss = 1.1442553997039795\n",
      "t = 271, loss = 1.144249677658081\n",
      "t = 272, loss = 1.1442440748214722\n",
      "t = 273, loss = 1.1442382335662842\n",
      "t = 274, loss = 1.1442326307296753\n",
      "t = 275, loss = 1.1442269086837769\n",
      "t = 276, loss = 1.1442210674285889\n",
      "t = 277, loss = 1.14421546459198\n",
      "t = 278, loss = 1.144209623336792\n",
      "t = 279, loss = 1.1442039012908936\n",
      "t = 280, loss = 1.1441982984542847\n",
      "t = 281, loss = 1.1441925764083862\n",
      "t = 282, loss = 1.1441868543624878\n",
      "t = 283, loss = 1.1441811323165894\n",
      "t = 284, loss = 1.144175410270691\n",
      "t = 285, loss = 1.1441696882247925\n",
      "t = 286, loss = 1.1441638469696045\n",
      "t = 287, loss = 1.1441582441329956\n",
      "t = 288, loss = 1.1441525220870972\n",
      "t = 289, loss = 1.1441468000411987\n",
      "t = 290, loss = 1.1441410779953003\n",
      "t = 291, loss = 1.1441354751586914\n",
      "t = 292, loss = 1.144129753112793\n",
      "t = 293, loss = 1.1441240310668945\n",
      "t = 294, loss = 1.144118309020996\n",
      "t = 295, loss = 1.1441125869750977\n",
      "t = 296, loss = 1.1441068649291992\n",
      "t = 297, loss = 1.1441011428833008\n",
      "t = 298, loss = 1.144095540046692\n",
      "t = 299, loss = 1.144089937210083\n",
      "t = 300, loss = 1.144084095954895\n",
      "t = 301, loss = 1.1440836191177368\n",
      "t = 302, loss = 1.1440829038619995\n",
      "t = 303, loss = 1.1440824270248413\n",
      "t = 304, loss = 1.1440818309783936\n",
      "t = 305, loss = 1.1440812349319458\n",
      "t = 306, loss = 1.1440807580947876\n",
      "t = 307, loss = 1.1440801620483398\n",
      "t = 308, loss = 1.144079566001892\n",
      "t = 309, loss = 1.1440790891647339\n",
      "t = 310, loss = 1.1440783739089966\n",
      "t = 311, loss = 1.1440778970718384\n",
      "t = 312, loss = 1.1440773010253906\n",
      "t = 313, loss = 1.1440767049789429\n",
      "t = 314, loss = 1.1440761089324951\n",
      "t = 315, loss = 1.1440755128860474\n",
      "t = 316, loss = 1.1440750360488892\n",
      "t = 317, loss = 1.1440744400024414\n",
      "t = 318, loss = 1.1440738439559937\n",
      "t = 319, loss = 1.144073247909546\n",
      "t = 320, loss = 1.1440727710723877\n",
      "t = 321, loss = 1.14407217502594\n",
      "t = 322, loss = 1.1440715789794922\n",
      "t = 323, loss = 1.144071102142334\n",
      "t = 324, loss = 1.1440703868865967\n",
      "t = 325, loss = 1.1440699100494385\n",
      "t = 326, loss = 1.1440693140029907\n",
      "t = 327, loss = 1.144068717956543\n",
      "t = 328, loss = 1.1440681219100952\n",
      "t = 329, loss = 1.144067645072937\n",
      "t = 330, loss = 1.1440670490264893\n",
      "t = 331, loss = 1.1440664529800415\n",
      "t = 332, loss = 1.1440658569335938\n",
      "t = 333, loss = 1.1440653800964355\n",
      "t = 334, loss = 1.1440647840499878\n",
      "t = 335, loss = 1.14406418800354\n",
      "t = 336, loss = 1.1440635919570923\n",
      "t = 337, loss = 1.1440629959106445\n",
      "t = 338, loss = 1.1440623998641968\n",
      "t = 339, loss = 1.1440619230270386\n",
      "t = 340, loss = 1.1440613269805908\n",
      "t = 341, loss = 1.1440608501434326\n",
      "t = 342, loss = 1.1440601348876953\n",
      "t = 343, loss = 1.144059658050537\n",
      "t = 344, loss = 1.1440590620040894\n",
      "t = 345, loss = 1.1440585851669312\n",
      "t = 346, loss = 1.1440579891204834\n",
      "t = 347, loss = 1.1440573930740356\n",
      "t = 348, loss = 1.144056797027588\n",
      "t = 349, loss = 1.1440562009811401\n",
      "t = 350, loss = 1.1440556049346924\n",
      "t = 351, loss = 1.1440551280975342\n",
      "t = 352, loss = 1.1440545320510864\n",
      "t = 353, loss = 1.1440540552139282\n",
      "t = 354, loss = 1.144053339958191\n",
      "t = 355, loss = 1.1440528631210327\n",
      "t = 356, loss = 1.144052267074585\n",
      "t = 357, loss = 1.1440516710281372\n",
      "t = 358, loss = 1.144051194190979\n",
      "t = 359, loss = 1.1440504789352417\n",
      "t = 360, loss = 1.1440500020980835\n",
      "t = 361, loss = 1.1440494060516357\n",
      "t = 362, loss = 1.144048810005188\n",
      "t = 363, loss = 1.1440483331680298\n",
      "t = 364, loss = 1.1440476179122925\n",
      "t = 365, loss = 1.1440472602844238\n",
      "t = 366, loss = 1.1440465450286865\n",
      "t = 367, loss = 1.1440460681915283\n",
      "t = 368, loss = 1.144045352935791\n",
      "t = 369, loss = 1.1440448760986328\n",
      "t = 370, loss = 1.1440443992614746\n",
      "t = 371, loss = 1.1440436840057373\n",
      "t = 372, loss = 1.144043207168579\n",
      "t = 373, loss = 1.1440426111221313\n",
      "t = 374, loss = 1.1440420150756836\n",
      "t = 375, loss = 1.1440415382385254\n",
      "t = 376, loss = 1.144040822982788\n",
      "t = 377, loss = 1.1440402269363403\n",
      "t = 378, loss = 1.1440397500991821\n",
      "t = 379, loss = 1.144039273262024\n",
      "t = 380, loss = 1.1440385580062866\n",
      "t = 381, loss = 1.1440380811691284\n",
      "t = 382, loss = 1.1440376043319702\n",
      "t = 383, loss = 1.144036889076233\n",
      "t = 384, loss = 1.1440362930297852\n",
      "t = 385, loss = 1.144035816192627\n",
      "t = 386, loss = 1.1440352201461792\n",
      "t = 387, loss = 1.144034743309021\n",
      "t = 388, loss = 1.1440340280532837\n",
      "t = 389, loss = 1.144033432006836\n",
      "t = 390, loss = 1.1440329551696777\n",
      "t = 391, loss = 1.14403235912323\n",
      "t = 392, loss = 1.1440317630767822\n",
      "t = 393, loss = 1.144031286239624\n",
      "t = 394, loss = 1.1440305709838867\n",
      "t = 395, loss = 1.1440300941467285\n",
      "t = 396, loss = 1.1440294981002808\n",
      "t = 397, loss = 1.1440290212631226\n",
      "t = 398, loss = 1.1440284252166748\n",
      "t = 399, loss = 1.144027829170227\n",
      "t = 400, loss = 1.1440272331237793\n",
      "t = 401, loss = 1.1440272331237793\n",
      "t = 402, loss = 1.1440271139144897\n",
      "t = 403, loss = 1.1440271139144897\n",
      "t = 404, loss = 1.1440269947052002\n",
      "t = 405, loss = 1.1440269947052002\n",
      "t = 406, loss = 1.1440268754959106\n",
      "t = 407, loss = 1.1440268754959106\n",
      "t = 408, loss = 1.144026756286621\n",
      "t = 409, loss = 1.144026756286621\n",
      "t = 410, loss = 1.144026756286621\n",
      "t = 411, loss = 1.1440266370773315\n",
      "t = 412, loss = 1.144026517868042\n",
      "t = 413, loss = 1.144026517868042\n",
      "t = 414, loss = 1.144026517868042\n",
      "t = 415, loss = 1.1440263986587524\n",
      "t = 416, loss = 1.1440263986587524\n",
      "t = 417, loss = 1.144026279449463\n",
      "t = 418, loss = 1.144026279449463\n",
      "t = 419, loss = 1.1440261602401733\n",
      "t = 420, loss = 1.1440260410308838\n",
      "t = 421, loss = 1.1440260410308838\n",
      "t = 422, loss = 1.1440260410308838\n",
      "t = 423, loss = 1.1440260410308838\n",
      "t = 424, loss = 1.1440259218215942\n",
      "t = 425, loss = 1.1440258026123047\n",
      "t = 426, loss = 1.1440258026123047\n",
      "t = 427, loss = 1.1440258026123047\n",
      "t = 428, loss = 1.1440258026123047\n",
      "t = 429, loss = 1.1440256834030151\n",
      "t = 430, loss = 1.1440255641937256\n",
      "t = 431, loss = 1.1440255641937256\n",
      "t = 432, loss = 1.144025444984436\n",
      "t = 433, loss = 1.144025444984436\n",
      "t = 434, loss = 1.1440253257751465\n",
      "t = 435, loss = 1.1440253257751465\n",
      "t = 436, loss = 1.1440253257751465\n",
      "t = 437, loss = 1.144025206565857\n",
      "t = 438, loss = 1.144025206565857\n",
      "t = 439, loss = 1.1440250873565674\n",
      "t = 440, loss = 1.1440250873565674\n",
      "t = 441, loss = 1.1440249681472778\n",
      "t = 442, loss = 1.1440249681472778\n",
      "t = 443, loss = 1.1440249681472778\n",
      "t = 444, loss = 1.1440248489379883\n",
      "t = 445, loss = 1.1440247297286987\n",
      "t = 446, loss = 1.1440247297286987\n",
      "t = 447, loss = 1.1440247297286987\n",
      "t = 448, loss = 1.1440246105194092\n",
      "t = 449, loss = 1.1440246105194092\n",
      "t = 450, loss = 1.1440244913101196\n",
      "t = 451, loss = 1.1440244913101196\n",
      "t = 452, loss = 1.1440244913101196\n",
      "t = 453, loss = 1.14402437210083\n",
      "t = 454, loss = 1.1440242528915405\n",
      "t = 455, loss = 1.1440242528915405\n",
      "t = 456, loss = 1.1440242528915405\n",
      "t = 457, loss = 1.144024133682251\n",
      "t = 458, loss = 1.144024133682251\n",
      "t = 459, loss = 1.1440240144729614\n",
      "t = 460, loss = 1.1440240144729614\n",
      "t = 461, loss = 1.1440240144729614\n",
      "t = 462, loss = 1.1440240144729614\n",
      "t = 463, loss = 1.1440237760543823\n",
      "t = 464, loss = 1.1440237760543823\n",
      "t = 465, loss = 1.1440237760543823\n",
      "t = 466, loss = 1.1440236568450928\n",
      "t = 467, loss = 1.1440236568450928\n",
      "t = 468, loss = 1.1440236568450928\n",
      "t = 469, loss = 1.1440235376358032\n",
      "t = 470, loss = 1.1440235376358032\n",
      "t = 471, loss = 1.1440232992172241\n",
      "t = 472, loss = 1.1440232992172241\n",
      "t = 473, loss = 1.1440232992172241\n",
      "t = 474, loss = 1.1440232992172241\n",
      "t = 475, loss = 1.1440231800079346\n",
      "t = 476, loss = 1.1440231800079346\n",
      "t = 477, loss = 1.144023060798645\n",
      "t = 478, loss = 1.144023060798645\n",
      "t = 479, loss = 1.144023060798645\n",
      "t = 480, loss = 1.1440229415893555\n",
      "t = 481, loss = 1.144022822380066\n",
      "t = 482, loss = 1.144022822380066\n",
      "t = 483, loss = 1.1440227031707764\n",
      "t = 484, loss = 1.1440227031707764\n",
      "t = 485, loss = 1.1440227031707764\n",
      "t = 486, loss = 1.1440225839614868\n",
      "t = 487, loss = 1.1440225839614868\n",
      "t = 488, loss = 1.1440224647521973\n",
      "t = 489, loss = 1.1440223455429077\n",
      "t = 490, loss = 1.1440223455429077\n",
      "t = 491, loss = 1.1440223455429077\n",
      "t = 492, loss = 1.1440222263336182\n",
      "t = 493, loss = 1.1440222263336182\n",
      "t = 494, loss = 1.1440222263336182\n",
      "t = 495, loss = 1.1440221071243286\n",
      "t = 496, loss = 1.1440221071243286\n",
      "t = 497, loss = 1.144021987915039\n",
      "t = 498, loss = 1.1440218687057495\n",
      "t = 499, loss = 1.1440218687057495\n",
      "Training R2:  0.4366898355835653\n",
      "Test R2:  0.4362253793596799\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_feat = len(features_cols)\n",
    "out_feat = len(targetColumns)\n",
    "model = Simple_Net(in_features=in_feat ,out_features=out_feat).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=100, gamma=0.1)\n",
    "\n",
    "losses,model = train(model, optimiser, scheduler, criterion, epochs = 500)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "    y_pred_train = model(X_train)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "\n",
    "train_r2 = r2_score(y_train.detach().cpu().numpy(), y_pred_train.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "test_r2=r2_score(y_test.detach().cpu().numpy(), y_pred_test.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "\n",
    "print(\"Training R2: \",train_r2)\n",
    "print(\"Test R2: \",test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
