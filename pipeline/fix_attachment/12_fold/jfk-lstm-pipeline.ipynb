{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2020)\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'lr': 0.00034439316653688684,\n",
    " 'layers': 3,\n",
    " 'step_size': 11,\n",
    " 'gamma': 0.761795969995615,\n",
    " 'bptt': 19,\n",
    " 'dropout': 0.1227497445640586}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00011598697153799081"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['lr'] = config['lr'] * config['gamma'] **4\n",
    "config['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/urwa/Documents/side_projects/urban/data/featureData/jfk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8757, 1049)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>...</th>\n",
       "      <th>91_lag_3</th>\n",
       "      <th>92_lag_3</th>\n",
       "      <th>93_lag_3</th>\n",
       "      <th>94_lag_3</th>\n",
       "      <th>95_lag_3</th>\n",
       "      <th>96_lag_3</th>\n",
       "      <th>97_lag_3</th>\n",
       "      <th>98_lag_3</th>\n",
       "      <th>99_lag_3</th>\n",
       "      <th>arrival_lag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 1049 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Hour  1  10  100  101  102  106  107  108  ...  91_lag_3  \\\n",
       "0  2018-01-01     3  0   0    0    0    0    0    0    0  ...       1.0   \n",
       "1  2018-01-01     4  0   3    0    0    1    0    0    1  ...       4.0   \n",
       "2  2018-01-01     5  0   4    0    0    1    2    3    1  ...       0.0   \n",
       "\n",
       "   92_lag_3  93_lag_3  94_lag_3  95_lag_3  96_lag_3  97_lag_3  98_lag_3  \\\n",
       "0       1.0       0.0       1.0       6.0       0.0       1.0       0.0   \n",
       "1       1.0       0.0       0.0       2.0       0.0       0.0       0.0   \n",
       "2       0.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "\n",
       "   99_lag_3  arrival_lag_3  \n",
       "0       0.0            6.0  \n",
       "1       0.0            6.0  \n",
       "2       0.0            2.0  \n",
       "\n",
       "[3 rows x 1049 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag_columns = [c for c in dataset.columns if 'lag' in c]\n",
    "len(lag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8757, 272)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[[c for c in dataset.columns if c not in lag_columns]]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DateColumns = ['Date']\n",
    "\n",
    "ext_columns = ['Dow', 'arrival','maxtemp', 'mintemp', 'avgtemp', 'departure', 'hdd',\n",
    "       'cdd', 'participation', 'newsnow', 'snowdepth', 'ifSnow']\n",
    "\n",
    "targetColumns = [c for c in dataset.columns if c not in ext_columns and \\\n",
    "                c not in DateColumns and c not in lag_columns and c != 'Hour']\n",
    "len(targetColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_cols = [c for c in dataset.columns if c not in targetColumns and c not in DateColumns]\n",
    "len(features_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, feat_size=1, hidden_layer_size=100, network_size=1, layers=1, communities=10, dropout=0, at_mat=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # aggregation\n",
    "        if at_mat != None:\n",
    "            self.attachment_matrix = torch.nn.Parameter(at_mat)\n",
    "            self.attachment_matrix.requires_grad = False\n",
    "        else:\n",
    "            self.attachment_matrix = torch.nn.Parameter(torch.randn(network_size,communities))\n",
    "            self.attachment_matrix.requires_grad = True\n",
    "        \n",
    "        \n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.hidden_cell = (torch.zeros(layers,1,self.hidden_layer_size),\n",
    "                    torch.zeros(layers,1,self.hidden_layer_size))\n",
    "        \n",
    "        lstm_input = communities + feat_size\n",
    "        self.lstm = nn.LSTM(input_size=lstm_input, hidden_size=hidden_layer_size, num_layers=layers, dropout=dropout)\n",
    "\n",
    "        #disaggregation\n",
    "#         self.linear_1 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.linear_2 = nn.Linear(hidden_layer_size, network_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_seq, feat):\n",
    "        \n",
    "        w = F.softmax(self.attachment_matrix, dim=1)\n",
    "        x = torch.matmul(input_seq, self.attachment_matrix)\n",
    "        x = torch.cat((x,feat),axis=1)\n",
    "\n",
    "        \n",
    "        lstm_out, self.hidden_cell = self.lstm(x.view(len(input_seq) ,1, -1), self.hidden_cell)\n",
    "        \n",
    "        predictions = self.linear_2(lstm_out.view(len(input_seq), -1))\n",
    "#         predictions = F.relu(predictions)\n",
    "#         predictions = self.linear_2(predictions)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        for feat,seq, labels in test_inout_seq:\n",
    "            model.hidden_cell = (torch.zeros(layers, 1, model.hidden_layer_size).to(device),\n",
    "                            torch.zeros(layers, 1, model.hidden_layer_size).to(device))\n",
    "            prediction.append(model(seq,feat)[-1])\n",
    "\n",
    "    y_test_ = torch.stack([labels[-1] for feat,seq, labels in test_inout_seq], axis=0).detach().cpu().numpy()\n",
    "    y_pred_ = torch.stack(prediction).detach().cpu().numpy()\n",
    "\n",
    "    res = y_pred_ - y_test_\n",
    "    r2 = r2_score(y_test_, y_pred_, multioutput='variance_weighted')\n",
    "    rmse = mean_squared_error(y_test_, y_pred_)\n",
    "    mae = mean_absolute_error(y_test_, y_pred_)\n",
    "#     print(\"r2: \",r2)\n",
    "    return (res, r2, rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_at_mat(targetColumns):\n",
    "    comms = pd.read_csv('/home/urwa/Documents/side_projects/urban/UrbanTemporalNetworks/Data/ZonetoComm.csv')  \n",
    "    communities = list(set(comms.start_community))\n",
    "\n",
    "    mapping = dict(zip(comms.start_id, comms.start_community))\n",
    "    comm_to_index = dict(zip(communities,range(len(communities))))\n",
    "    col_to_index = dict(zip(targetColumns,range(len(targetColumns))))\n",
    "\n",
    "    attach = torch.zeros(len(targetColumns), len(communities))\n",
    "\n",
    "    for t_c in targetColumns:\n",
    "        com = mapping[int(t_c)]\n",
    "        x_i = col_to_index[t_c]\n",
    "        y_i = comm_to_index[com]\n",
    "\n",
    "        attach[x_i,y_i] = 1\n",
    "\n",
    "    return attach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inout_sequences(x,y, tw):\n",
    "    inout_seq = []\n",
    "    L = len(x)\n",
    "    for i in range(L-tw):\n",
    "        train_seq_x = x[i:i+tw]\n",
    "        train_seq_y = y[i:i+tw]\n",
    "#         train_seq = torch.cat((train_seq_x,train_seq_y),axis=1)\n",
    "        \n",
    "#         train_label = y[i+tw:i+tw+1]\n",
    "        train_label = y[i+1:i+tw+1]\n",
    "        inout_seq.append((train_seq_x, train_seq_y ,train_label))\n",
    "    return inout_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = '/home/urwa/Documents/side_projects/urban/urban_traffic_prediciton/pipeline/fix_attachment/jfk.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  2\n",
      "\n",
      " train test split\n",
      "(8085, 272)\n",
      "(672, 272)\n",
      "\n",
      " \n",
      "torch.Size([8085, 13])\n",
      "torch.Size([8085, 258])\n",
      "torch.Size([672, 13])\n",
      "torch.Size([672, 258])\n",
      "\n",
      " sequences\n",
      "torch.Size([19, 13]) torch.Size([19, 258]) torch.Size([19, 258])\n",
      "\n",
      "attachment matrix\n",
      "torch.Size([258, 24])\n",
      "\n",
      " model inititalized\n",
      "\n",
      " model loaded\n",
      "Pretraining R2:  0.529468481632609\n",
      "\n",
      " Training model...\n",
      "epoch:   0 loss: 1.45043051 r2: 0.482 rmse: 3.307 mae: 1.013\n",
      "epoch:   1 loss: 1.31491649 r2: 0.501 rmse: 3.184 mae: 1.002\n",
      "epoch:   2 loss: 1.28961730 r2: 0.514 rmse: 3.101 mae: 0.986\n",
      "epoch:   3 loss: 1.30294001 r2: 0.530 rmse: 2.999 mae: 0.976\n",
      "epoch:   4 loss: 1.29502463 r2: 0.516 rmse: 3.088 mae: 0.985\n",
      "epoch:   5 loss: 1.29154372 r2: 0.524 rmse: 3.038 mae: 0.980\n",
      "epoch:   6 loss: 1.28005242 r2: 0.535 rmse: 2.968 mae: 0.970\n",
      "epoch:   7 loss: 1.34761381 r2: 0.520 rmse: 3.063 mae: 0.979\n",
      "epoch:   8 loss: 1.29694831 r2: 0.516 rmse: 3.088 mae: 0.986\n",
      "epoch:   9 loss: 1.27607036 r2: 0.527 rmse: 3.019 mae: 0.973\n",
      "epoch:  10 loss: 1.27070081 r2: 0.522 rmse: 3.053 mae: 0.976\n",
      "epoch:  11 loss: 1.29791987 r2: 0.522 rmse: 3.051 mae: 0.979\n",
      "epoch:  12 loss: 1.28208470 r2: 0.529 rmse: 3.006 mae: 0.973\n",
      "epoch:  13 loss: 1.27120900 r2: 0.526 rmse: 3.028 mae: 0.972\n",
      "epoch:  14 loss: 1.29849339 r2: 0.521 rmse: 3.058 mae: 0.977\n",
      "epoch:  15 loss: 1.29376745 r2: 0.522 rmse: 3.048 mae: 0.975\n",
      "epoch:  16 loss: 1.30834401 r2: 0.529 rmse: 3.009 mae: 0.971\n",
      "epoch:  17 loss: 1.32372963 r2: 0.519 rmse: 3.070 mae: 0.980\n",
      "epoch:  18 loss: 1.25211143 r2: 0.521 rmse: 3.056 mae: 0.977\n",
      "epoch:  19 loss: 1.26361704 r2: 0.523 rmse: 3.046 mae: 0.976\n",
      "epoch:  20 loss: 1.28010190 r2: 0.521 rmse: 3.055 mae: 0.979\n",
      "epoch:  21 loss: 1.25818694 r2: 0.514 rmse: 3.099 mae: 0.980\n",
      "epoch:  22 loss: 1.29067743 r2: 0.531 rmse: 2.996 mae: 0.966\n",
      "epoch:  23 loss: 1.29210842 r2: 0.523 rmse: 3.047 mae: 0.971\n",
      "epoch:  24 loss: 1.28738832 r2: 0.524 rmse: 3.037 mae: 0.971\n",
      "epoch:  25 loss: 1.26114476 r2: 0.533 rmse: 2.980 mae: 0.963\n",
      "epoch:  26 loss: 1.26144576 r2: 0.530 rmse: 3.002 mae: 0.965\n",
      "epoch:  27 loss: 1.26608300 r2: 0.533 rmse: 2.983 mae: 0.962\n",
      "epoch:  28 loss: 1.27673113 r2: 0.527 rmse: 3.022 mae: 0.969\n",
      "epoch:  29 loss: 1.28136730 r2: 0.534 rmse: 2.973 mae: 0.962\n",
      "epoch:  30 loss: 1.27656186 r2: 0.533 rmse: 2.982 mae: 0.963\n",
      "epoch:  31 loss: 1.29000437 r2: 0.538 rmse: 2.948 mae: 0.958\n",
      "epoch:  32 loss: 1.30026531 r2: 0.531 rmse: 2.992 mae: 0.964\n",
      "epoch:  33 loss: 1.29455507 r2: 0.538 rmse: 2.946 mae: 0.957\n",
      "epoch:  34 loss: 1.29516518 r2: 0.540 rmse: 2.939 mae: 0.955\n",
      "epoch:  35 loss: 1.28176761 r2: 0.544 rmse: 2.912 mae: 0.951\n",
      "epoch:  36 loss: 1.29045498 r2: 0.537 rmse: 2.956 mae: 0.957\n",
      "epoch:  37 loss: 1.29302156 r2: 0.541 rmse: 2.930 mae: 0.955\n",
      "epoch:  38 loss: 1.26213074 r2: 0.542 rmse: 2.924 mae: 0.955\n",
      "epoch:  39 loss: 1.26593113 r2: 0.544 rmse: 2.912 mae: 0.951\n",
      "epoch:  40 loss: 1.28478515 r2: 0.542 rmse: 2.925 mae: 0.954\n",
      "epoch:  41 loss: 1.28066111 r2: 0.541 rmse: 2.928 mae: 0.954\n",
      "epoch:  42 loss: 1.25829136 r2: 0.542 rmse: 2.926 mae: 0.954\n",
      "epoch:  43 loss: 1.28246844 r2: 0.539 rmse: 2.942 mae: 0.957\n",
      "epoch:  44 loss: 1.24928677 r2: 0.545 rmse: 2.903 mae: 0.950\n",
      "epoch:  45 loss: 1.24668205 r2: 0.546 rmse: 2.901 mae: 0.949\n",
      "epoch:  46 loss: 1.28745937 r2: 0.546 rmse: 2.897 mae: 0.949\n",
      "epoch:  47 loss: 1.28723884 r2: 0.544 rmse: 2.910 mae: 0.951\n",
      "epoch:  48 loss: 1.25742829 r2: 0.546 rmse: 2.900 mae: 0.949\n",
      "epoch:  49 loss: 1.26176572 r2: 0.543 rmse: 2.916 mae: 0.951\n",
      "epoch:  50 loss: 1.26273370 r2: 0.547 rmse: 2.891 mae: 0.948\n",
      "epoch:  51 loss: 1.25876081 r2: 0.545 rmse: 2.904 mae: 0.950\n",
      "epoch:  52 loss: 1.26213896 r2: 0.546 rmse: 2.898 mae: 0.948\n",
      "epoch:  53 loss: 1.24585950 r2: 0.549 rmse: 2.881 mae: 0.947\n",
      "epoch:  54 loss: 1.27489030 r2: 0.545 rmse: 2.907 mae: 0.949\n",
      "epoch:  55 loss: 1.29427433 r2: 0.550 rmse: 2.871 mae: 0.944\n",
      "epoch:  56 loss: 1.26531625 r2: 0.548 rmse: 2.887 mae: 0.946\n",
      "epoch:  57 loss: 1.28793609 r2: 0.551 rmse: 2.869 mae: 0.944\n",
      "epoch:  58 loss: 1.27734303 r2: 0.549 rmse: 2.877 mae: 0.945\n",
      "epoch:  59 loss: 1.26644492 r2: 0.550 rmse: 2.870 mae: 0.944\n",
      "epoch:  59 loss: 1.2664449215\n",
      "bet_r2:  0.5506309348043397\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  3\n",
      "\n",
      " train test split\n",
      "(8013, 272)\n",
      "(744, 272)\n",
      "\n",
      " \n",
      "torch.Size([8013, 13])\n",
      "torch.Size([8013, 258])\n",
      "torch.Size([744, 13])\n",
      "torch.Size([744, 258])\n",
      "\n",
      " sequences\n",
      "torch.Size([19, 13]) torch.Size([19, 258]) torch.Size([19, 258])\n",
      "\n",
      "attachment matrix\n",
      "torch.Size([258, 24])\n",
      "\n",
      " model inititalized\n",
      "\n",
      " model loaded\n",
      "Pretraining R2:  0.5972399097890982\n",
      "\n",
      " Training model...\n",
      "epoch:   0 loss: 1.39591885 r2: 0.576 rmse: 3.201 mae: 0.983\n",
      "epoch:   1 loss: 1.38102949 r2: 0.568 rmse: 3.260 mae: 0.993\n",
      "epoch:   2 loss: 1.33010042 r2: 0.573 rmse: 3.227 mae: 0.990\n",
      "epoch:   3 loss: 1.31851113 r2: 0.577 rmse: 3.195 mae: 0.984\n",
      "epoch:   4 loss: 1.34068310 r2: 0.574 rmse: 3.219 mae: 0.984\n",
      "epoch:   5 loss: 1.34812808 r2: 0.571 rmse: 3.241 mae: 0.989\n",
      "epoch:   6 loss: 1.40325689 r2: 0.561 rmse: 3.319 mae: 1.001\n",
      "epoch:   7 loss: 1.27757823 r2: 0.573 rmse: 3.228 mae: 0.987\n",
      "epoch:   8 loss: 1.29141688 r2: 0.574 rmse: 3.216 mae: 0.987\n",
      "epoch:   9 loss: 1.30410004 r2: 0.568 rmse: 3.260 mae: 0.993\n",
      "epoch:  10 loss: 1.28991377 r2: 0.573 rmse: 3.225 mae: 0.987\n",
      "epoch:  11 loss: 1.37374818 r2: 0.572 rmse: 3.230 mae: 0.991\n",
      "epoch:  12 loss: 1.34477723 r2: 0.573 rmse: 3.224 mae: 0.990\n",
      "epoch:  13 loss: 1.30950058 r2: 0.573 rmse: 3.223 mae: 0.992\n",
      "epoch:  14 loss: 1.28764045 r2: 0.576 rmse: 3.204 mae: 0.987\n",
      "epoch:  15 loss: 1.28370655 r2: 0.573 rmse: 3.225 mae: 0.989\n",
      "epoch:  16 loss: 1.26967692 r2: 0.572 rmse: 3.233 mae: 0.992\n",
      "epoch:  17 loss: 1.28600359 r2: 0.574 rmse: 3.218 mae: 0.991\n",
      "epoch:  18 loss: 1.32531834 r2: 0.573 rmse: 3.223 mae: 0.990\n",
      "epoch:  19 loss: 1.28594184 r2: 0.575 rmse: 3.214 mae: 0.987\n",
      "epoch:  20 loss: 1.26928759 r2: 0.568 rmse: 3.268 mae: 0.998\n",
      "epoch:  21 loss: 1.31679022 r2: 0.573 rmse: 3.224 mae: 0.989\n",
      "epoch:  22 loss: 1.31170118 r2: 0.576 rmse: 3.203 mae: 0.986\n",
      "epoch:  23 loss: 1.32570887 r2: 0.574 rmse: 3.215 mae: 0.988\n",
      "epoch:  24 loss: 1.30165792 r2: 0.575 rmse: 3.211 mae: 0.989\n",
      "epoch:  25 loss: 1.29696774 r2: 0.573 rmse: 3.228 mae: 0.990\n",
      "epoch:  26 loss: 1.25897527 r2: 0.573 rmse: 3.225 mae: 0.990\n",
      "epoch:  27 loss: 1.27802765 r2: 0.570 rmse: 3.247 mae: 0.993\n",
      "epoch:  28 loss: 1.29683948 r2: 0.571 rmse: 3.238 mae: 0.992\n",
      "epoch:  29 loss: 1.27300906 r2: 0.572 rmse: 3.232 mae: 0.991\n",
      "epoch:  30 loss: 1.26241839 r2: 0.572 rmse: 3.236 mae: 0.993\n",
      "epoch:  31 loss: 1.27849221 r2: 0.570 rmse: 3.248 mae: 0.994\n",
      "epoch:  32 loss: 1.25320005 r2: 0.568 rmse: 3.260 mae: 0.997\n",
      "epoch:  33 loss: 1.28424180 r2: 0.573 rmse: 3.228 mae: 0.990\n",
      "epoch:  34 loss: 1.28203058 r2: 0.572 rmse: 3.235 mae: 0.991\n",
      "epoch:  35 loss: 1.30365908 r2: 0.569 rmse: 3.253 mae: 0.994\n",
      "epoch:  36 loss: 1.31080091 r2: 0.570 rmse: 3.247 mae: 0.994\n",
      "epoch:  37 loss: 1.26933742 r2: 0.571 rmse: 3.241 mae: 0.993\n",
      "epoch:  38 loss: 1.28505540 r2: 0.570 rmse: 3.251 mae: 0.994\n",
      "epoch:  39 loss: 1.26201093 r2: 0.568 rmse: 3.267 mae: 0.996\n",
      "epoch:  40 loss: 1.29707551 r2: 0.569 rmse: 3.260 mae: 0.996\n",
      "epoch:  41 loss: 1.30616748 r2: 0.567 rmse: 3.273 mae: 0.996\n",
      "epoch:  42 loss: 1.27416646 r2: 0.567 rmse: 3.270 mae: 0.996\n",
      "epoch:  43 loss: 1.27278256 r2: 0.566 rmse: 3.281 mae: 0.998\n",
      "epoch:  44 loss: 1.27179611 r2: 0.568 rmse: 3.264 mae: 0.994\n",
      "epoch:  45 loss: 1.25078547 r2: 0.570 rmse: 3.252 mae: 0.992\n",
      "epoch:  46 loss: 1.27219462 r2: 0.568 rmse: 3.267 mae: 0.995\n",
      "epoch:  47 loss: 1.25070393 r2: 0.569 rmse: 3.256 mae: 0.993\n",
      "epoch:  48 loss: 1.24991882 r2: 0.568 rmse: 3.263 mae: 0.994\n",
      "epoch:  49 loss: 1.27450132 r2: 0.569 rmse: 3.260 mae: 0.993\n",
      "epoch:  50 loss: 1.25336838 r2: 0.568 rmse: 3.261 mae: 0.993\n",
      "epoch:  51 loss: 1.24921131 r2: 0.568 rmse: 3.264 mae: 0.994\n",
      "epoch:  52 loss: 1.25389040 r2: 0.569 rmse: 3.259 mae: 0.993\n",
      "epoch:  53 loss: 1.24578595 r2: 0.568 rmse: 3.264 mae: 0.994\n",
      "epoch:  54 loss: 1.26835966 r2: 0.567 rmse: 3.270 mae: 0.995\n",
      "epoch:  55 loss: 1.24473882 r2: 0.568 rmse: 3.263 mae: 0.993\n",
      "epoch:  56 loss: 1.24902856 r2: 0.569 rmse: 3.260 mae: 0.992\n",
      "epoch:  57 loss: 1.26335585 r2: 0.568 rmse: 3.261 mae: 0.992\n",
      "epoch:  58 loss: 1.24111474 r2: 0.568 rmse: 3.266 mae: 0.993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  59 loss: 1.25896513 r2: 0.568 rmse: 3.264 mae: 0.992\n",
      "epoch:  59 loss: 1.2589651346\n",
      "bet_r2:  0.5972399097890982\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  4\n",
      "\n",
      " train test split\n",
      "(8037, 272)\n",
      "(720, 272)\n",
      "\n",
      " \n",
      "torch.Size([8037, 13])\n",
      "torch.Size([8037, 258])\n",
      "torch.Size([720, 13])\n",
      "torch.Size([720, 258])\n",
      "\n",
      " sequences\n",
      "torch.Size([19, 13]) torch.Size([19, 258]) torch.Size([19, 258])\n",
      "\n",
      "attachment matrix\n",
      "torch.Size([258, 24])\n",
      "\n",
      " model inititalized\n",
      "\n",
      " model loaded\n",
      "Pretraining R2:  0.6083282515195536\n",
      "\n",
      " Training model...\n",
      "epoch:   0 loss: 1.33773279 r2: 0.566 rmse: 3.282 mae: 1.020\n",
      "epoch:   1 loss: 1.31826878 r2: 0.572 rmse: 3.241 mae: 1.009\n",
      "epoch:   2 loss: 1.30559552 r2: 0.561 rmse: 3.325 mae: 1.017\n",
      "epoch:   3 loss: 1.33061659 r2: 0.571 rmse: 3.248 mae: 1.007\n",
      "epoch:   4 loss: 1.41485465 r2: 0.570 rmse: 3.251 mae: 1.020\n",
      "epoch:   5 loss: 1.48819971 r2: 0.578 rmse: 3.195 mae: 1.011\n",
      "epoch:   6 loss: 1.32695758 r2: 0.569 rmse: 3.258 mae: 1.011\n",
      "epoch:   7 loss: 1.44479454 r2: 0.578 rmse: 3.191 mae: 1.011\n",
      "epoch:   8 loss: 1.37502098 r2: 0.573 rmse: 3.230 mae: 1.010\n",
      "epoch:   9 loss: 1.31050718 r2: 0.572 rmse: 3.239 mae: 1.013\n",
      "epoch:  10 loss: 1.32779562 r2: 0.575 rmse: 3.218 mae: 1.017\n",
      "epoch:  11 loss: 1.30093944 r2: 0.581 rmse: 3.170 mae: 1.005\n",
      "epoch:  12 loss: 1.30462193 r2: 0.576 rmse: 3.206 mae: 1.009\n",
      "epoch:  13 loss: 1.35936487 r2: 0.579 rmse: 3.186 mae: 1.007\n",
      "epoch:  14 loss: 1.30399942 r2: 0.577 rmse: 3.205 mae: 1.011\n",
      "epoch:  15 loss: 1.28460026 r2: 0.576 rmse: 3.209 mae: 1.009\n",
      "epoch:  16 loss: 1.31422698 r2: 0.580 rmse: 3.181 mae: 1.006\n",
      "epoch:  17 loss: 1.32177937 r2: 0.579 rmse: 3.189 mae: 1.009\n",
      "epoch:  18 loss: 1.27974629 r2: 0.574 rmse: 3.221 mae: 1.010\n",
      "epoch:  19 loss: 1.30465579 r2: 0.575 rmse: 3.214 mae: 1.007\n",
      "epoch:  20 loss: 1.31575060 r2: 0.575 rmse: 3.217 mae: 1.008\n",
      "epoch:  21 loss: 1.29900873 r2: 0.577 rmse: 3.199 mae: 1.007\n",
      "epoch:  22 loss: 1.32533562 r2: 0.581 rmse: 3.174 mae: 1.003\n",
      "epoch:  23 loss: 1.31004667 r2: 0.581 rmse: 3.171 mae: 1.003\n",
      "epoch:  24 loss: 1.30731773 r2: 0.580 rmse: 3.176 mae: 1.003\n",
      "epoch:  25 loss: 1.34072185 r2: 0.580 rmse: 3.177 mae: 1.003\n",
      "epoch:  26 loss: 1.27285683 r2: 0.583 rmse: 3.154 mae: 1.000\n",
      "epoch:  27 loss: 1.27633417 r2: 0.582 rmse: 3.164 mae: 1.001\n",
      "epoch:  28 loss: 1.28906333 r2: 0.582 rmse: 3.161 mae: 1.002\n",
      "epoch:  29 loss: 1.30942690 r2: 0.582 rmse: 3.165 mae: 1.001\n",
      "epoch:  30 loss: 1.28374982 r2: 0.581 rmse: 3.172 mae: 1.002\n",
      "epoch:  31 loss: 1.25473678 r2: 0.582 rmse: 3.165 mae: 1.002\n",
      "epoch:  32 loss: 1.27214110 r2: 0.582 rmse: 3.162 mae: 1.002\n",
      "epoch:  33 loss: 1.26061749 r2: 0.586 rmse: 3.132 mae: 0.998\n",
      "epoch:  34 loss: 1.26135886 r2: 0.586 rmse: 3.136 mae: 0.999\n",
      "epoch:  35 loss: 1.26776004 r2: 0.586 rmse: 3.133 mae: 1.000\n",
      "epoch:  36 loss: 1.28981817 r2: 0.585 rmse: 3.138 mae: 0.999\n",
      "epoch:  37 loss: 1.29739857 r2: 0.585 rmse: 3.141 mae: 1.000\n",
      "epoch:  38 loss: 1.26160574 r2: 0.585 rmse: 3.144 mae: 1.000\n",
      "epoch:  39 loss: 1.27276146 r2: 0.586 rmse: 3.133 mae: 0.999\n",
      "epoch:  40 loss: 1.29107440 r2: 0.586 rmse: 3.135 mae: 1.000\n",
      "epoch:  41 loss: 1.25978935 r2: 0.586 rmse: 3.130 mae: 0.998\n",
      "epoch:  42 loss: 1.27621436 r2: 0.586 rmse: 3.132 mae: 0.999\n",
      "epoch:  43 loss: 1.24488723 r2: 0.586 rmse: 3.136 mae: 0.999\n",
      "epoch:  44 loss: 1.25338089 r2: 0.589 rmse: 3.108 mae: 0.996\n",
      "epoch:  45 loss: 1.25477099 r2: 0.589 rmse: 3.108 mae: 0.995\n",
      "epoch:  46 loss: 1.25772583 r2: 0.588 rmse: 3.119 mae: 0.997\n",
      "epoch:  47 loss: 1.26464283 r2: 0.591 rmse: 3.097 mae: 0.995\n",
      "epoch:  48 loss: 1.28349972 r2: 0.590 rmse: 3.106 mae: 0.996\n",
      "epoch:  49 loss: 1.26656139 r2: 0.590 rmse: 3.106 mae: 0.996\n",
      "epoch:  50 loss: 1.27658761 r2: 0.589 rmse: 3.113 mae: 0.997\n",
      "epoch:  51 loss: 1.30936599 r2: 0.589 rmse: 3.110 mae: 0.995\n",
      "epoch:  52 loss: 1.24442077 r2: 0.589 rmse: 3.110 mae: 0.996\n",
      "epoch:  53 loss: 1.25503862 r2: 0.588 rmse: 3.119 mae: 0.997\n",
      "epoch:  54 loss: 1.29255176 r2: 0.588 rmse: 3.116 mae: 0.997\n",
      "epoch:  55 loss: 1.26687992 r2: 0.591 rmse: 3.096 mae: 0.995\n",
      "epoch:  56 loss: 1.26613045 r2: 0.592 rmse: 3.090 mae: 0.994\n",
      "epoch:  57 loss: 1.32714307 r2: 0.590 rmse: 3.099 mae: 0.995\n",
      "epoch:  58 loss: 1.26622701 r2: 0.590 rmse: 3.101 mae: 0.995\n",
      "epoch:  59 loss: 1.26785040 r2: 0.591 rmse: 3.098 mae: 0.995\n",
      "epoch:  59 loss: 1.2678503990\n",
      "bet_r2:  0.6083282515195536\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  5\n",
      "\n",
      " train test split\n",
      "(8013, 272)\n",
      "(744, 272)\n",
      "\n",
      " \n",
      "torch.Size([8013, 13])\n",
      "torch.Size([8013, 258])\n",
      "torch.Size([744, 13])\n",
      "torch.Size([744, 258])\n",
      "\n",
      " sequences\n",
      "torch.Size([19, 13]) torch.Size([19, 258]) torch.Size([19, 258])\n",
      "\n",
      "attachment matrix\n",
      "torch.Size([258, 24])\n",
      "\n",
      " model inititalized\n",
      "\n",
      " model loaded\n",
      "Pretraining R2:  0.6267548650792119\n",
      "\n",
      " Training model...\n",
      "epoch:   0 loss: 1.32719648 r2: 0.594 rmse: 3.237 mae: 0.999\n",
      "epoch:   1 loss: 1.38149250 r2: 0.595 rmse: 3.232 mae: 1.004\n",
      "epoch:   2 loss: 1.33826768 r2: 0.598 rmse: 3.207 mae: 1.005\n",
      "epoch:   3 loss: 1.30719829 r2: 0.594 rmse: 3.233 mae: 1.002\n",
      "epoch:   4 loss: 1.35371387 r2: 0.600 rmse: 3.192 mae: 1.004\n",
      "epoch:   5 loss: 1.32232130 r2: 0.596 rmse: 3.220 mae: 1.005\n",
      "epoch:   6 loss: 1.30066848 r2: 0.597 rmse: 3.210 mae: 1.004\n",
      "epoch:   7 loss: 1.32197845 r2: 0.600 rmse: 3.192 mae: 1.002\n",
      "epoch:   8 loss: 1.31431282 r2: 0.602 rmse: 3.175 mae: 1.002\n",
      "epoch:   9 loss: 1.27198172 r2: 0.596 rmse: 3.224 mae: 1.003\n",
      "epoch:  10 loss: 1.30857813 r2: 0.595 rmse: 3.226 mae: 1.006\n",
      "epoch:  11 loss: 1.28840828 r2: 0.597 rmse: 3.212 mae: 1.004\n",
      "epoch:  12 loss: 1.27073109 r2: 0.596 rmse: 3.223 mae: 1.005\n",
      "epoch:  13 loss: 1.30313802 r2: 0.600 rmse: 3.185 mae: 1.005\n",
      "epoch:  14 loss: 1.33199024 r2: 0.595 rmse: 3.226 mae: 1.004\n",
      "epoch:  15 loss: 1.32613552 r2: 0.601 rmse: 3.182 mae: 1.003\n",
      "epoch:  16 loss: 1.28822362 r2: 0.596 rmse: 3.217 mae: 1.004\n",
      "epoch:  17 loss: 1.32072222 r2: 0.599 rmse: 3.200 mae: 1.005\n",
      "epoch:  18 loss: 1.29591143 r2: 0.598 rmse: 3.202 mae: 1.005\n",
      "epoch:  19 loss: 1.27235699 r2: 0.599 rmse: 3.198 mae: 1.005\n",
      "epoch:  20 loss: 1.30026412 r2: 0.597 rmse: 3.214 mae: 1.006\n",
      "epoch:  21 loss: 1.27373278 r2: 0.600 rmse: 3.190 mae: 1.001\n",
      "epoch:  22 loss: 1.26670933 r2: 0.597 rmse: 3.209 mae: 1.002\n",
      "epoch:  23 loss: 1.26415634 r2: 0.599 rmse: 3.198 mae: 1.002\n",
      "epoch:  24 loss: 1.28831339 r2: 0.599 rmse: 3.198 mae: 1.002\n",
      "epoch:  25 loss: 1.34722078 r2: 0.600 rmse: 3.188 mae: 1.002\n",
      "epoch:  26 loss: 1.26072955 r2: 0.600 rmse: 3.192 mae: 1.001\n",
      "epoch:  27 loss: 1.29540932 r2: 0.598 rmse: 3.204 mae: 1.001\n",
      "epoch:  28 loss: 1.28819549 r2: 0.600 rmse: 3.190 mae: 1.000\n",
      "epoch:  29 loss: 1.29251015 r2: 0.603 rmse: 3.167 mae: 0.998\n",
      "epoch:  30 loss: 1.31817067 r2: 0.601 rmse: 3.181 mae: 0.998\n",
      "epoch:  31 loss: 1.28804731 r2: 0.602 rmse: 3.176 mae: 0.999\n",
      "epoch:  32 loss: 1.26358080 r2: 0.601 rmse: 3.181 mae: 0.999\n",
      "epoch:  33 loss: 1.28236091 r2: 0.602 rmse: 3.170 mae: 0.999\n",
      "epoch:  34 loss: 1.27151513 r2: 0.603 rmse: 3.167 mae: 0.996\n",
      "epoch:  35 loss: 1.32904315 r2: 0.603 rmse: 3.167 mae: 0.997\n",
      "epoch:  36 loss: 1.27504587 r2: 0.603 rmse: 3.167 mae: 0.996\n",
      "epoch:  37 loss: 1.28071046 r2: 0.604 rmse: 3.160 mae: 0.997\n",
      "epoch:  38 loss: 1.27826631 r2: 0.604 rmse: 3.155 mae: 0.996\n",
      "epoch:  39 loss: 1.25847268 r2: 0.603 rmse: 3.162 mae: 0.996\n",
      "epoch:  40 loss: 1.28174818 r2: 0.603 rmse: 3.164 mae: 0.997\n",
      "epoch:  41 loss: 1.30483472 r2: 0.603 rmse: 3.168 mae: 0.998\n",
      "epoch:  42 loss: 1.29455650 r2: 0.603 rmse: 3.166 mae: 0.997\n",
      "epoch:  43 loss: 1.27047157 r2: 0.603 rmse: 3.164 mae: 0.996\n",
      "epoch:  44 loss: 1.27218914 r2: 0.604 rmse: 3.153 mae: 0.996\n",
      "epoch:  45 loss: 1.30393708 r2: 0.604 rmse: 3.155 mae: 0.996\n",
      "epoch:  46 loss: 1.28332484 r2: 0.604 rmse: 3.159 mae: 0.996\n",
      "epoch:  47 loss: 1.25764489 r2: 0.603 rmse: 3.161 mae: 0.997\n",
      "epoch:  48 loss: 1.27478337 r2: 0.604 rmse: 3.154 mae: 0.996\n",
      "epoch:  49 loss: 1.27263260 r2: 0.604 rmse: 3.158 mae: 0.996\n",
      "epoch:  50 loss: 1.29283762 r2: 0.604 rmse: 3.155 mae: 0.997\n",
      "epoch:  51 loss: 1.28159928 r2: 0.604 rmse: 3.156 mae: 0.996\n",
      "epoch:  52 loss: 1.28193688 r2: 0.605 rmse: 3.150 mae: 0.996\n",
      "epoch:  53 loss: 1.25286388 r2: 0.604 rmse: 3.160 mae: 0.997\n",
      "epoch:  54 loss: 1.25970435 r2: 0.604 rmse: 3.156 mae: 0.997\n",
      "epoch:  55 loss: 1.26622307 r2: 0.604 rmse: 3.153 mae: 0.996\n",
      "epoch:  56 loss: 1.25997221 r2: 0.605 rmse: 3.146 mae: 0.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  57 loss: 1.26664054 r2: 0.604 rmse: 3.156 mae: 0.996\n",
      "epoch:  58 loss: 1.26752377 r2: 0.605 rmse: 3.150 mae: 0.995\n",
      "epoch:  59 loss: 1.27900386 r2: 0.604 rmse: 3.154 mae: 0.996\n",
      "epoch:  59 loss: 1.2790038586\n",
      "bet_r2:  0.6267548650792119\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  6\n",
      "\n",
      " train test split\n",
      "(8037, 272)\n",
      "(720, 272)\n",
      "\n",
      " \n",
      "torch.Size([8037, 13])\n",
      "torch.Size([8037, 258])\n",
      "torch.Size([720, 13])\n",
      "torch.Size([720, 258])\n",
      "\n",
      " sequences\n",
      "torch.Size([19, 13]) torch.Size([19, 258]) torch.Size([19, 258])\n",
      "\n",
      "attachment matrix\n",
      "torch.Size([258, 24])\n",
      "\n",
      " model inititalized\n",
      "\n",
      " model loaded\n",
      "Pretraining R2:  0.5933234870001999\n",
      "\n",
      " Training model...\n",
      "epoch:   0 loss: 1.32166719 r2: 0.571 rmse: 3.142 mae: 0.998\n",
      "epoch:   1 loss: 1.32582808 r2: 0.572 rmse: 3.136 mae: 0.998\n",
      "epoch:   2 loss: 1.41923749 r2: 0.574 rmse: 3.122 mae: 1.003\n",
      "epoch:   3 loss: 1.32975638 r2: 0.576 rmse: 3.112 mae: 0.999\n",
      "epoch:   4 loss: 1.30326009 r2: 0.557 rmse: 3.245 mae: 1.006\n",
      "epoch:   5 loss: 1.29223299 r2: 0.566 rmse: 3.181 mae: 1.009\n",
      "epoch:   6 loss: 1.33869052 r2: 0.560 rmse: 3.227 mae: 1.001\n",
      "epoch:   7 loss: 1.31142652 r2: 0.560 rmse: 3.229 mae: 1.002\n",
      "epoch:   8 loss: 1.32653165 r2: 0.555 rmse: 3.266 mae: 1.007\n",
      "epoch:   9 loss: 1.31772590 r2: 0.567 rmse: 3.173 mae: 1.005\n",
      "epoch:  10 loss: 1.32515490 r2: 0.570 rmse: 3.153 mae: 1.007\n",
      "epoch:  11 loss: 1.29894769 r2: 0.564 rmse: 3.195 mae: 1.007\n",
      "epoch:  12 loss: 1.29403472 r2: 0.565 rmse: 3.186 mae: 1.009\n",
      "epoch:  13 loss: 1.32090724 r2: 0.556 rmse: 3.256 mae: 1.006\n",
      "epoch:  14 loss: 1.31079304 r2: 0.567 rmse: 3.178 mae: 1.003\n",
      "epoch:  15 loss: 1.31731606 r2: 0.564 rmse: 3.197 mae: 1.007\n",
      "epoch:  16 loss: 1.31064427 r2: 0.567 rmse: 3.176 mae: 1.002\n",
      "epoch:  17 loss: 1.29337251 r2: 0.568 rmse: 3.171 mae: 1.002\n",
      "epoch:  18 loss: 1.26521087 r2: 0.565 rmse: 3.192 mae: 1.008\n",
      "epoch:  19 loss: 1.31464601 r2: 0.567 rmse: 3.172 mae: 1.004\n",
      "epoch:  20 loss: 1.30565643 r2: 0.569 rmse: 3.162 mae: 1.002\n",
      "epoch:  21 loss: 1.37137079 r2: 0.567 rmse: 3.178 mae: 1.004\n",
      "epoch:  22 loss: 1.29316461 r2: 0.567 rmse: 3.175 mae: 1.003\n",
      "epoch:  23 loss: 1.31510508 r2: 0.571 rmse: 3.149 mae: 1.001\n",
      "epoch:  24 loss: 1.33291137 r2: 0.568 rmse: 3.169 mae: 1.003\n",
      "epoch:  25 loss: 1.29801977 r2: 0.568 rmse: 3.167 mae: 1.003\n",
      "epoch:  26 loss: 1.27616894 r2: 0.571 rmse: 3.146 mae: 1.002\n",
      "epoch:  27 loss: 1.32853389 r2: 0.568 rmse: 3.164 mae: 1.002\n",
      "epoch:  28 loss: 1.28281140 r2: 0.568 rmse: 3.169 mae: 1.002\n",
      "epoch:  29 loss: 1.31472325 r2: 0.569 rmse: 3.162 mae: 1.001\n",
      "epoch:  30 loss: 1.33905888 r2: 0.570 rmse: 3.155 mae: 1.001\n",
      "epoch:  31 loss: 1.25763500 r2: 0.569 rmse: 3.163 mae: 1.003\n",
      "epoch:  32 loss: 1.29469001 r2: 0.569 rmse: 3.157 mae: 1.001\n",
      "epoch:  33 loss: 1.26860094 r2: 0.569 rmse: 3.157 mae: 1.001\n",
      "epoch:  34 loss: 1.29682267 r2: 0.571 rmse: 3.148 mae: 0.999\n",
      "epoch:  35 loss: 1.29311752 r2: 0.571 rmse: 3.146 mae: 1.000\n",
      "epoch:  36 loss: 1.30205917 r2: 0.573 rmse: 3.129 mae: 0.998\n",
      "epoch:  37 loss: 1.28707349 r2: 0.571 rmse: 3.147 mae: 1.000\n",
      "epoch:  38 loss: 1.28269362 r2: 0.572 rmse: 3.136 mae: 0.998\n",
      "epoch:  39 loss: 1.26338303 r2: 0.571 rmse: 3.148 mae: 1.000\n",
      "epoch:  40 loss: 1.29586375 r2: 0.570 rmse: 3.154 mae: 1.001\n",
      "epoch:  41 loss: 1.28369999 r2: 0.571 rmse: 3.147 mae: 1.000\n",
      "epoch:  42 loss: 1.28216243 r2: 0.571 rmse: 3.147 mae: 1.000\n",
      "epoch:  43 loss: 1.29668927 r2: 0.570 rmse: 3.151 mae: 1.001\n",
      "epoch:  44 loss: 1.29136324 r2: 0.570 rmse: 3.151 mae: 1.000\n",
      "epoch:  45 loss: 1.24442732 r2: 0.573 rmse: 3.130 mae: 0.998\n",
      "epoch:  46 loss: 1.30590630 r2: 0.573 rmse: 3.134 mae: 0.998\n",
      "epoch:  47 loss: 1.27595103 r2: 0.574 rmse: 3.126 mae: 0.997\n",
      "epoch:  48 loss: 1.27127731 r2: 0.573 rmse: 3.132 mae: 0.998\n",
      "epoch:  49 loss: 1.28440714 r2: 0.574 rmse: 3.122 mae: 0.996\n",
      "epoch:  50 loss: 1.28471076 r2: 0.573 rmse: 3.133 mae: 0.998\n",
      "epoch:  51 loss: 1.26125467 r2: 0.573 rmse: 3.131 mae: 0.997\n",
      "epoch:  52 loss: 1.28217983 r2: 0.575 rmse: 3.114 mae: 0.996\n",
      "epoch:  53 loss: 1.27827144 r2: 0.572 rmse: 3.136 mae: 0.999\n",
      "epoch:  54 loss: 1.26884258 r2: 0.575 rmse: 3.118 mae: 0.997\n",
      "epoch:  55 loss: 1.28924215 r2: 0.576 rmse: 3.110 mae: 0.995\n",
      "epoch:  56 loss: 1.27238560 r2: 0.576 rmse: 3.107 mae: 0.995\n",
      "epoch:  57 loss: 1.27142537 r2: 0.576 rmse: 3.107 mae: 0.996\n",
      "epoch:  58 loss: 1.29713058 r2: 0.576 rmse: 3.112 mae: 0.996\n",
      "epoch:  59 loss: 1.31229937 r2: 0.574 rmse: 3.126 mae: 0.997\n",
      "epoch:  59 loss: 1.3122993708\n",
      "bet_r2:  0.5933234870001999\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  7\n",
      "\n",
      " train test split\n",
      "(8013, 272)\n",
      "(744, 272)\n",
      "\n",
      " \n",
      "torch.Size([8013, 13])\n",
      "torch.Size([8013, 258])\n",
      "torch.Size([744, 13])\n",
      "torch.Size([744, 258])\n",
      "\n",
      " sequences\n",
      "torch.Size([19, 13]) torch.Size([19, 258]) torch.Size([19, 258])\n",
      "\n",
      "attachment matrix\n",
      "torch.Size([258, 24])\n",
      "\n",
      " model inititalized\n",
      "\n",
      " model loaded\n",
      "Pretraining R2:  0.5512411542818053\n",
      "\n",
      " Training model...\n",
      "epoch:   0 loss: 1.34551978 r2: 0.526 rmse: 3.332 mae: 1.026\n",
      "epoch:   1 loss: 1.31356585 r2: 0.510 rmse: 3.448 mae: 1.050\n",
      "epoch:   2 loss: 1.31184673 r2: 0.509 rmse: 3.456 mae: 1.053\n",
      "epoch:   3 loss: 1.32188094 r2: 0.512 rmse: 3.435 mae: 1.048\n",
      "epoch:   4 loss: 1.37784779 r2: 0.512 rmse: 3.431 mae: 1.047\n",
      "epoch:   5 loss: 1.33251381 r2: 0.508 rmse: 3.461 mae: 1.046\n",
      "epoch:   6 loss: 1.31332719 r2: 0.520 rmse: 3.374 mae: 1.039\n",
      "epoch:   7 loss: 1.32219207 r2: 0.514 rmse: 3.422 mae: 1.045\n",
      "epoch:   8 loss: 1.29695845 r2: 0.514 rmse: 3.418 mae: 1.042\n",
      "epoch:   9 loss: 1.32780170 r2: 0.508 rmse: 3.463 mae: 1.047\n",
      "epoch:  10 loss: 1.28194749 r2: 0.509 rmse: 3.457 mae: 1.048\n",
      "epoch:  11 loss: 1.26381874 r2: 0.514 rmse: 3.416 mae: 1.042\n",
      "epoch:  12 loss: 1.27281308 r2: 0.520 rmse: 3.380 mae: 1.039\n",
      "epoch:  13 loss: 1.26116884 r2: 0.518 rmse: 3.393 mae: 1.042\n",
      "epoch:  14 loss: 1.25089872 r2: 0.502 rmse: 3.500 mae: 1.059\n",
      "epoch:  15 loss: 1.28570151 r2: 0.517 rmse: 3.396 mae: 1.041\n",
      "epoch:  16 loss: 1.28780818 r2: 0.516 rmse: 3.406 mae: 1.041\n",
      "epoch:  17 loss: 1.27997100 r2: 0.504 rmse: 3.491 mae: 1.060\n",
      "epoch:  18 loss: 1.29448235 r2: 0.523 rmse: 3.356 mae: 1.033\n",
      "epoch:  19 loss: 1.28819633 r2: 0.519 rmse: 3.381 mae: 1.037\n",
      "epoch:  20 loss: 1.31295562 r2: 0.513 rmse: 3.425 mae: 1.045\n",
      "epoch:  21 loss: 1.28240967 r2: 0.517 rmse: 3.397 mae: 1.041\n",
      "epoch:  22 loss: 1.27215886 r2: 0.517 rmse: 3.399 mae: 1.042\n",
      "epoch:  23 loss: 1.28491914 r2: 0.518 rmse: 3.388 mae: 1.040\n",
      "epoch:  24 loss: 1.28511715 r2: 0.520 rmse: 3.377 mae: 1.039\n",
      "epoch:  25 loss: 1.26531696 r2: 0.521 rmse: 3.371 mae: 1.037\n",
      "epoch:  26 loss: 1.28121507 r2: 0.517 rmse: 3.397 mae: 1.042\n",
      "epoch:  27 loss: 1.28429830 r2: 0.520 rmse: 3.380 mae: 1.039\n",
      "epoch:  28 loss: 1.29624176 r2: 0.523 rmse: 3.355 mae: 1.036\n",
      "epoch:  29 loss: 1.28020966 r2: 0.521 rmse: 3.370 mae: 1.039\n",
      "epoch:  30 loss: 1.28007495 r2: 0.518 rmse: 3.394 mae: 1.044\n",
      "epoch:  31 loss: 1.25833666 r2: 0.523 rmse: 3.357 mae: 1.040\n",
      "epoch:  32 loss: 1.27630782 r2: 0.520 rmse: 3.374 mae: 1.040\n",
      "epoch:  33 loss: 1.25110435 r2: 0.524 rmse: 3.349 mae: 1.036\n",
      "epoch:  34 loss: 1.28018653 r2: 0.522 rmse: 3.366 mae: 1.038\n",
      "epoch:  35 loss: 1.28038454 r2: 0.524 rmse: 3.351 mae: 1.037\n",
      "epoch:  36 loss: 1.28441441 r2: 0.521 rmse: 3.367 mae: 1.040\n",
      "epoch:  37 loss: 1.28978395 r2: 0.523 rmse: 3.358 mae: 1.039\n",
      "epoch:  38 loss: 1.30280626 r2: 0.525 rmse: 3.338 mae: 1.036\n",
      "epoch:  39 loss: 1.28344953 r2: 0.525 rmse: 3.340 mae: 1.035\n",
      "epoch:  40 loss: 1.29252458 r2: 0.528 rmse: 3.318 mae: 1.033\n",
      "epoch:  41 loss: 1.26752627 r2: 0.526 rmse: 3.338 mae: 1.037\n",
      "epoch:  42 loss: 1.29143560 r2: 0.522 rmse: 3.365 mae: 1.041\n",
      "epoch:  43 loss: 1.33768582 r2: 0.524 rmse: 3.348 mae: 1.038\n",
      "epoch:  44 loss: 1.27520967 r2: 0.527 rmse: 3.330 mae: 1.035\n",
      "epoch:  45 loss: 1.28231287 r2: 0.528 rmse: 3.322 mae: 1.034\n",
      "epoch:  46 loss: 1.26512551 r2: 0.529 rmse: 3.311 mae: 1.032\n",
      "epoch:  47 loss: 1.30282009 r2: 0.530 rmse: 3.308 mae: 1.032\n",
      "epoch:  48 loss: 1.25995088 r2: 0.529 rmse: 3.313 mae: 1.033\n",
      "epoch:  49 loss: 1.26726198 r2: 0.526 rmse: 3.333 mae: 1.036\n",
      "epoch:  50 loss: 1.27791405 r2: 0.528 rmse: 3.318 mae: 1.033\n",
      "epoch:  51 loss: 1.27453184 r2: 0.529 rmse: 3.311 mae: 1.033\n",
      "epoch:  52 loss: 1.30573630 r2: 0.527 rmse: 3.330 mae: 1.035\n",
      "epoch:  53 loss: 1.28643346 r2: 0.527 rmse: 3.331 mae: 1.035\n",
      "epoch:  54 loss: 1.30011511 r2: 0.531 rmse: 3.300 mae: 1.031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  55 loss: 1.28004336 r2: 0.533 rmse: 3.287 mae: 1.029\n",
      "epoch:  56 loss: 1.27638471 r2: 0.532 rmse: 3.290 mae: 1.029\n",
      "epoch:  57 loss: 1.25225246 r2: 0.534 rmse: 3.279 mae: 1.028\n",
      "epoch:  58 loss: 1.27978718 r2: 0.534 rmse: 3.281 mae: 1.028\n",
      "epoch:  59 loss: 1.29239357 r2: 0.532 rmse: 3.295 mae: 1.030\n",
      "epoch:  59 loss: 1.2923935652\n",
      "bet_r2:  0.5512411542818053\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  8\n",
      "\n",
      " train test split\n",
      "(8013, 272)\n",
      "(744, 272)\n",
      "\n",
      " \n",
      "torch.Size([8013, 13])\n",
      "torch.Size([8013, 258])\n",
      "torch.Size([744, 13])\n",
      "torch.Size([744, 258])\n",
      "\n",
      " sequences\n",
      "torch.Size([19, 13]) torch.Size([19, 258]) torch.Size([19, 258])\n",
      "\n",
      "attachment matrix\n",
      "torch.Size([258, 24])\n",
      "\n",
      " model inititalized\n",
      "\n",
      " model loaded\n",
      "Pretraining R2:  0.5207402905815595\n",
      "\n",
      " Training model...\n",
      "epoch:   0 loss: 1.45408428 r2: 0.488 rmse: 3.455 mae: 1.078\n",
      "epoch:   1 loss: 1.44571555 r2: 0.497 rmse: 3.395 mae: 1.069\n",
      "epoch:   2 loss: 1.36047864 r2: 0.501 rmse: 3.367 mae: 1.069\n",
      "epoch:   3 loss: 1.30024910 r2: 0.484 rmse: 3.484 mae: 1.077\n",
      "epoch:   4 loss: 1.37208843 r2: 0.494 rmse: 3.417 mae: 1.075\n",
      "epoch:   5 loss: 1.32529306 r2: 0.479 rmse: 3.515 mae: 1.084\n",
      "epoch:   6 loss: 1.39632916 r2: 0.480 rmse: 3.512 mae: 1.082\n",
      "epoch:   7 loss: 1.32248616 r2: 0.481 rmse: 3.499 mae: 1.082\n",
      "epoch:   8 loss: 1.33873188 r2: 0.481 rmse: 3.500 mae: 1.083\n",
      "epoch:   9 loss: 1.31300986 r2: 0.474 rmse: 3.552 mae: 1.086\n",
      "epoch:  10 loss: 1.38495028 r2: 0.470 rmse: 3.576 mae: 1.090\n",
      "epoch:  11 loss: 1.32097697 r2: 0.463 rmse: 3.621 mae: 1.094\n",
      "epoch:  12 loss: 1.26203275 r2: 0.478 rmse: 3.522 mae: 1.089\n",
      "epoch:  13 loss: 1.32240057 r2: 0.480 rmse: 3.508 mae: 1.087\n",
      "epoch:  14 loss: 1.27669442 r2: 0.477 rmse: 3.527 mae: 1.091\n",
      "epoch:  15 loss: 1.32420576 r2: 0.478 rmse: 3.524 mae: 1.090\n",
      "epoch:  16 loss: 1.30514669 r2: 0.474 rmse: 3.551 mae: 1.096\n",
      "epoch:  17 loss: 1.29100609 r2: 0.470 rmse: 3.576 mae: 1.094\n",
      "epoch:  18 loss: 1.33780217 r2: 0.474 rmse: 3.547 mae: 1.090\n",
      "epoch:  19 loss: 1.28323376 r2: 0.476 rmse: 3.535 mae: 1.091\n",
      "epoch:  20 loss: 1.27578211 r2: 0.474 rmse: 3.546 mae: 1.097\n",
      "epoch:  21 loss: 1.30264318 r2: 0.477 rmse: 3.528 mae: 1.091\n",
      "epoch:  22 loss: 1.30857909 r2: 0.478 rmse: 3.519 mae: 1.090\n",
      "epoch:  23 loss: 1.30890751 r2: 0.475 rmse: 3.544 mae: 1.092\n",
      "epoch:  24 loss: 1.29236448 r2: 0.472 rmse: 3.563 mae: 1.100\n",
      "epoch:  25 loss: 1.29068267 r2: 0.482 rmse: 3.497 mae: 1.086\n",
      "epoch:  26 loss: 1.29636180 r2: 0.469 rmse: 3.581 mae: 1.099\n",
      "epoch:  27 loss: 1.28639698 r2: 0.473 rmse: 3.555 mae: 1.097\n",
      "epoch:  28 loss: 1.29575109 r2: 0.480 rmse: 3.508 mae: 1.092\n",
      "epoch:  29 loss: 1.28310645 r2: 0.473 rmse: 3.556 mae: 1.100\n",
      "epoch:  30 loss: 1.29447043 r2: 0.475 rmse: 3.544 mae: 1.096\n",
      "epoch:  31 loss: 1.29900408 r2: 0.474 rmse: 3.548 mae: 1.096\n",
      "epoch:  32 loss: 1.28703272 r2: 0.473 rmse: 3.558 mae: 1.097\n",
      "epoch:  33 loss: 1.25632191 r2: 0.478 rmse: 3.523 mae: 1.090\n",
      "epoch:  34 loss: 1.29368079 r2: 0.485 rmse: 3.473 mae: 1.084\n",
      "epoch:  35 loss: 1.27733910 r2: 0.476 rmse: 3.538 mae: 1.093\n",
      "epoch:  36 loss: 1.31116843 r2: 0.476 rmse: 3.538 mae: 1.092\n",
      "epoch:  37 loss: 1.26641762 r2: 0.473 rmse: 3.557 mae: 1.096\n",
      "epoch:  38 loss: 1.26201224 r2: 0.475 rmse: 3.539 mae: 1.093\n",
      "epoch:  39 loss: 1.27268946 r2: 0.476 rmse: 3.533 mae: 1.092\n",
      "epoch:  40 loss: 1.28096128 r2: 0.480 rmse: 3.508 mae: 1.088\n",
      "epoch:  41 loss: 1.27546608 r2: 0.476 rmse: 3.538 mae: 1.092\n",
      "epoch:  42 loss: 1.28758490 r2: 0.475 rmse: 3.541 mae: 1.093\n",
      "epoch:  43 loss: 1.27373397 r2: 0.473 rmse: 3.558 mae: 1.093\n",
      "epoch:  44 loss: 1.28526366 r2: 0.475 rmse: 3.541 mae: 1.092\n",
      "epoch:  45 loss: 1.27147400 r2: 0.475 rmse: 3.544 mae: 1.092\n",
      "epoch:  46 loss: 1.27117121 r2: 0.475 rmse: 3.542 mae: 1.093\n",
      "epoch:  47 loss: 1.25371552 r2: 0.476 rmse: 3.532 mae: 1.092\n",
      "epoch:  48 loss: 1.25825799 r2: 0.476 rmse: 3.537 mae: 1.092\n",
      "epoch:  49 loss: 1.29252708 r2: 0.480 rmse: 3.510 mae: 1.089\n",
      "epoch:  50 loss: 1.26588476 r2: 0.474 rmse: 3.549 mae: 1.093\n",
      "epoch:  51 loss: 1.27026832 r2: 0.475 rmse: 3.545 mae: 1.093\n",
      "epoch:  52 loss: 1.25266373 r2: 0.476 rmse: 3.538 mae: 1.092\n",
      "epoch:  53 loss: 1.24466312 r2: 0.476 rmse: 3.532 mae: 1.091\n",
      "epoch:  54 loss: 1.28593600 r2: 0.475 rmse: 3.542 mae: 1.092\n",
      "epoch:  55 loss: 1.28409421 r2: 0.478 rmse: 3.524 mae: 1.090\n",
      "epoch:  56 loss: 1.25013530 r2: 0.479 rmse: 3.514 mae: 1.088\n",
      "epoch:  57 loss: 1.27538455 r2: 0.479 rmse: 3.513 mae: 1.089\n",
      "epoch:  58 loss: 1.25486052 r2: 0.479 rmse: 3.513 mae: 1.088\n",
      "epoch:  59 loss: 1.28476274 r2: 0.482 rmse: 3.493 mae: 1.086\n",
      "epoch:  59 loss: 1.2847627401\n",
      "bet_r2:  0.5207402905815595\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  9\n",
      "\n",
      " train test split\n",
      "(8037, 272)\n",
      "(720, 272)\n",
      "\n",
      " \n",
      "torch.Size([8037, 13])\n",
      "torch.Size([8037, 258])\n",
      "torch.Size([720, 13])\n",
      "torch.Size([720, 258])\n",
      "\n",
      " sequences\n",
      "torch.Size([19, 13]) torch.Size([19, 258]) torch.Size([19, 258])\n",
      "\n",
      "attachment matrix\n",
      "torch.Size([258, 24])\n",
      "\n",
      " model inititalized\n",
      "\n",
      " model loaded\n",
      "Pretraining R2:  0.6349401338057566\n",
      "\n",
      " Training model...\n",
      "epoch:   0 loss: 1.37282336 r2: 0.601 rmse: 3.432 mae: 1.039\n",
      "epoch:   1 loss: 1.30973327 r2: 0.601 rmse: 3.434 mae: 1.042\n",
      "epoch:   2 loss: 1.35735977 r2: 0.603 rmse: 3.417 mae: 1.039\n",
      "epoch:   3 loss: 1.32648194 r2: 0.607 rmse: 3.382 mae: 1.037\n",
      "epoch:   4 loss: 1.32101429 r2: 0.601 rmse: 3.431 mae: 1.043\n",
      "epoch:   5 loss: 1.31302750 r2: 0.607 rmse: 3.382 mae: 1.038\n",
      "epoch:   6 loss: 1.31150591 r2: 0.595 rmse: 3.483 mae: 1.056\n",
      "epoch:   7 loss: 1.32993007 r2: 0.606 rmse: 3.393 mae: 1.041\n",
      "epoch:   8 loss: 1.33574450 r2: 0.606 rmse: 3.393 mae: 1.040\n",
      "epoch:   9 loss: 1.32384348 r2: 0.597 rmse: 3.465 mae: 1.044\n",
      "epoch:  10 loss: 1.28288198 r2: 0.601 rmse: 3.435 mae: 1.044\n",
      "epoch:  11 loss: 1.28320134 r2: 0.605 rmse: 3.396 mae: 1.040\n",
      "epoch:  12 loss: 1.29703271 r2: 0.606 rmse: 3.395 mae: 1.041\n",
      "epoch:  13 loss: 1.32561135 r2: 0.611 rmse: 3.344 mae: 1.033\n",
      "epoch:  14 loss: 1.29792690 r2: 0.608 rmse: 3.371 mae: 1.035\n",
      "epoch:  15 loss: 1.28083718 r2: 0.604 rmse: 3.408 mae: 1.039\n",
      "epoch:  16 loss: 1.30960274 r2: 0.604 rmse: 3.407 mae: 1.036\n",
      "epoch:  17 loss: 1.27215028 r2: 0.605 rmse: 3.395 mae: 1.037\n",
      "epoch:  18 loss: 1.30148113 r2: 0.606 rmse: 3.386 mae: 1.036\n",
      "epoch:  19 loss: 1.30797076 r2: 0.606 rmse: 3.390 mae: 1.038\n",
      "epoch:  20 loss: 1.27362335 r2: 0.605 rmse: 3.401 mae: 1.039\n",
      "epoch:  21 loss: 1.31719160 r2: 0.606 rmse: 3.390 mae: 1.039\n",
      "epoch:  22 loss: 1.27851379 r2: 0.603 rmse: 3.413 mae: 1.037\n",
      "epoch:  23 loss: 1.30629826 r2: 0.607 rmse: 3.385 mae: 1.036\n",
      "epoch:  24 loss: 1.27003610 r2: 0.607 rmse: 3.381 mae: 1.035\n",
      "epoch:  25 loss: 1.29845285 r2: 0.606 rmse: 3.388 mae: 1.036\n",
      "epoch:  26 loss: 1.27963233 r2: 0.607 rmse: 3.379 mae: 1.033\n",
      "epoch:  27 loss: 1.28559530 r2: 0.607 rmse: 3.383 mae: 1.035\n",
      "epoch:  28 loss: 1.30120373 r2: 0.608 rmse: 3.370 mae: 1.034\n",
      "epoch:  29 loss: 1.27940106 r2: 0.607 rmse: 3.381 mae: 1.034\n",
      "epoch:  30 loss: 1.26488042 r2: 0.608 rmse: 3.377 mae: 1.034\n",
      "epoch:  31 loss: 1.30257022 r2: 0.610 rmse: 3.360 mae: 1.033\n",
      "epoch:  32 loss: 1.28721046 r2: 0.606 rmse: 3.387 mae: 1.034\n",
      "epoch:  33 loss: 1.29517245 r2: 0.608 rmse: 3.378 mae: 1.035\n",
      "epoch:  34 loss: 1.31073844 r2: 0.607 rmse: 3.379 mae: 1.034\n",
      "epoch:  35 loss: 1.26448154 r2: 0.609 rmse: 3.364 mae: 1.032\n",
      "epoch:  36 loss: 1.29031360 r2: 0.607 rmse: 3.381 mae: 1.035\n",
      "epoch:  37 loss: 1.32869959 r2: 0.610 rmse: 3.359 mae: 1.033\n",
      "epoch:  38 loss: 1.26865971 r2: 0.608 rmse: 3.373 mae: 1.033\n",
      "epoch:  39 loss: 1.26651311 r2: 0.610 rmse: 3.359 mae: 1.033\n",
      "epoch:  40 loss: 1.31561935 r2: 0.609 rmse: 3.364 mae: 1.033\n",
      "epoch:  41 loss: 1.31536877 r2: 0.611 rmse: 3.349 mae: 1.032\n",
      "epoch:  42 loss: 1.28556049 r2: 0.609 rmse: 3.368 mae: 1.033\n",
      "epoch:  43 loss: 1.29121363 r2: 0.610 rmse: 3.356 mae: 1.032\n",
      "epoch:  44 loss: 1.25522470 r2: 0.612 rmse: 3.342 mae: 1.031\n",
      "epoch:  45 loss: 1.25953448 r2: 0.612 rmse: 3.341 mae: 1.030\n",
      "epoch:  46 loss: 1.28952289 r2: 0.613 rmse: 3.331 mae: 1.029\n",
      "epoch:  47 loss: 1.27155471 r2: 0.612 rmse: 3.339 mae: 1.030\n",
      "epoch:  48 loss: 1.28137589 r2: 0.612 rmse: 3.336 mae: 1.030\n",
      "epoch:  49 loss: 1.30112624 r2: 0.613 rmse: 3.326 mae: 1.029\n",
      "epoch:  50 loss: 1.28083420 r2: 0.613 rmse: 3.328 mae: 1.029\n",
      "epoch:  51 loss: 1.28438163 r2: 0.613 rmse: 3.328 mae: 1.030\n",
      "epoch:  52 loss: 1.35376549 r2: 0.613 rmse: 3.333 mae: 1.029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  53 loss: 1.26068616 r2: 0.612 rmse: 3.341 mae: 1.031\n",
      "epoch:  54 loss: 1.24865568 r2: 0.612 rmse: 3.336 mae: 1.030\n",
      "epoch:  55 loss: 1.27210343 r2: 0.614 rmse: 3.319 mae: 1.028\n",
      "epoch:  56 loss: 1.28986943 r2: 0.615 rmse: 3.316 mae: 1.027\n",
      "epoch:  57 loss: 1.28985429 r2: 0.614 rmse: 3.318 mae: 1.028\n",
      "epoch:  58 loss: 1.29340327 r2: 0.615 rmse: 3.312 mae: 1.026\n",
      "epoch:  59 loss: 1.25854576 r2: 0.616 rmse: 3.306 mae: 1.027\n",
      "epoch:  59 loss: 1.2585457563\n",
      "bet_r2:  0.6349401338057566\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  10\n",
      "\n",
      " train test split\n",
      "(8013, 272)\n",
      "(744, 272)\n",
      "\n",
      " \n",
      "torch.Size([8013, 13])\n",
      "torch.Size([8013, 258])\n",
      "torch.Size([744, 13])\n",
      "torch.Size([744, 258])\n",
      "\n",
      " sequences\n",
      "torch.Size([19, 13]) torch.Size([19, 258]) torch.Size([19, 258])\n",
      "\n",
      "attachment matrix\n",
      "torch.Size([258, 24])\n",
      "\n",
      " model inititalized\n",
      "\n",
      " model loaded\n",
      "Pretraining R2:  0.6334234351737693\n",
      "\n",
      " Training model...\n",
      "epoch:   0 loss: 1.41324997 r2: 0.602 rmse: 3.608 mae: 1.041\n",
      "epoch:   1 loss: 1.33825254 r2: 0.599 rmse: 3.635 mae: 1.041\n",
      "epoch:   2 loss: 1.32114840 r2: 0.592 rmse: 3.698 mae: 1.044\n",
      "epoch:   3 loss: 1.31144798 r2: 0.605 rmse: 3.574 mae: 1.035\n",
      "epoch:   4 loss: 1.35317111 r2: 0.590 rmse: 3.712 mae: 1.048\n",
      "epoch:   5 loss: 1.32882524 r2: 0.606 rmse: 3.573 mae: 1.036\n",
      "epoch:   6 loss: 1.32667828 r2: 0.598 rmse: 3.645 mae: 1.037\n",
      "epoch:   7 loss: 1.29236770 r2: 0.600 rmse: 3.620 mae: 1.037\n",
      "epoch:   8 loss: 1.28989244 r2: 0.604 rmse: 3.588 mae: 1.036\n",
      "epoch:   9 loss: 1.25635219 r2: 0.604 rmse: 3.584 mae: 1.037\n",
      "epoch:  10 loss: 1.29247487 r2: 0.609 rmse: 3.544 mae: 1.033\n",
      "epoch:  11 loss: 1.30530608 r2: 0.609 rmse: 3.542 mae: 1.030\n",
      "epoch:  12 loss: 1.29166341 r2: 0.606 rmse: 3.567 mae: 1.032\n",
      "epoch:  13 loss: 1.29324377 r2: 0.610 rmse: 3.533 mae: 1.028\n",
      "epoch:  14 loss: 1.28929138 r2: 0.607 rmse: 3.558 mae: 1.032\n",
      "epoch:  15 loss: 1.27579904 r2: 0.612 rmse: 3.519 mae: 1.029\n",
      "epoch:  16 loss: 1.33384466 r2: 0.615 rmse: 3.487 mae: 1.027\n",
      "epoch:  17 loss: 1.26580608 r2: 0.608 rmse: 3.550 mae: 1.034\n",
      "epoch:  18 loss: 1.27073348 r2: 0.612 rmse: 3.510 mae: 1.029\n",
      "epoch:  19 loss: 1.28874493 r2: 0.616 rmse: 3.479 mae: 1.027\n",
      "epoch:  20 loss: 1.28640437 r2: 0.614 rmse: 3.497 mae: 1.028\n",
      "epoch:  21 loss: 1.26729882 r2: 0.610 rmse: 3.528 mae: 1.030\n",
      "epoch:  22 loss: 1.29772604 r2: 0.614 rmse: 3.499 mae: 1.024\n",
      "epoch:  23 loss: 1.27075160 r2: 0.616 rmse: 3.474 mae: 1.024\n",
      "epoch:  24 loss: 1.25911748 r2: 0.617 rmse: 3.469 mae: 1.024\n",
      "epoch:  25 loss: 1.24681592 r2: 0.615 rmse: 3.488 mae: 1.027\n",
      "epoch:  26 loss: 1.26020265 r2: 0.620 rmse: 3.440 mae: 1.022\n",
      "epoch:  27 loss: 1.30130136 r2: 0.616 rmse: 3.476 mae: 1.024\n",
      "epoch:  28 loss: 1.32060575 r2: 0.621 rmse: 3.430 mae: 1.022\n",
      "epoch:  29 loss: 1.26850867 r2: 0.616 rmse: 3.481 mae: 1.023\n",
      "epoch:  30 loss: 1.24955630 r2: 0.621 rmse: 3.436 mae: 1.021\n",
      "epoch:  31 loss: 1.27343750 r2: 0.615 rmse: 3.485 mae: 1.025\n",
      "epoch:  32 loss: 1.27012074 r2: 0.618 rmse: 3.461 mae: 1.023\n",
      "epoch:  33 loss: 1.25571120 r2: 0.617 rmse: 3.468 mae: 1.020\n",
      "epoch:  34 loss: 1.24574959 r2: 0.621 rmse: 3.435 mae: 1.019\n",
      "epoch:  35 loss: 1.26498497 r2: 0.624 rmse: 3.405 mae: 1.019\n",
      "epoch:  36 loss: 1.29660928 r2: 0.620 rmse: 3.445 mae: 1.020\n",
      "epoch:  37 loss: 1.29393828 r2: 0.624 rmse: 3.408 mae: 1.018\n",
      "epoch:  38 loss: 1.26391792 r2: 0.621 rmse: 3.435 mae: 1.019\n",
      "epoch:  39 loss: 1.28814363 r2: 0.623 rmse: 3.416 mae: 1.019\n",
      "epoch:  40 loss: 1.28539574 r2: 0.623 rmse: 3.410 mae: 1.018\n",
      "epoch:  41 loss: 1.28255582 r2: 0.625 rmse: 3.400 mae: 1.016\n",
      "epoch:  42 loss: 1.28088558 r2: 0.622 rmse: 3.422 mae: 1.018\n",
      "epoch:  43 loss: 1.26193666 r2: 0.621 rmse: 3.434 mae: 1.019\n",
      "epoch:  44 loss: 1.24475181 r2: 0.624 rmse: 3.409 mae: 1.017\n",
      "epoch:  45 loss: 1.27129078 r2: 0.624 rmse: 3.406 mae: 1.017\n",
      "epoch:  46 loss: 1.26288664 r2: 0.626 rmse: 3.390 mae: 1.016\n",
      "epoch:  47 loss: 1.27420986 r2: 0.625 rmse: 3.397 mae: 1.015\n",
      "epoch:  48 loss: 1.26970577 r2: 0.625 rmse: 3.392 mae: 1.015\n",
      "epoch:  49 loss: 1.24105132 r2: 0.625 rmse: 3.400 mae: 1.016\n",
      "epoch:  50 loss: 1.26211500 r2: 0.625 rmse: 3.401 mae: 1.016\n",
      "epoch:  51 loss: 1.28512847 r2: 0.623 rmse: 3.410 mae: 1.016\n",
      "epoch:  52 loss: 1.28060925 r2: 0.625 rmse: 3.400 mae: 1.016\n",
      "epoch:  53 loss: 1.26680505 r2: 0.625 rmse: 3.394 mae: 1.015\n",
      "epoch:  54 loss: 1.26673782 r2: 0.625 rmse: 3.397 mae: 1.016\n",
      "epoch:  55 loss: 1.24770236 r2: 0.625 rmse: 3.400 mae: 1.015\n",
      "epoch:  56 loss: 1.27426469 r2: 0.625 rmse: 3.396 mae: 1.015\n",
      "epoch:  57 loss: 1.27432907 r2: 0.626 rmse: 3.388 mae: 1.014\n",
      "epoch:  58 loss: 1.25265360 r2: 0.625 rmse: 3.398 mae: 1.015\n",
      "epoch:  59 loss: 1.22640884 r2: 0.626 rmse: 3.383 mae: 1.014\n",
      "epoch:  59 loss: 1.2264088392\n",
      "bet_r2:  0.6334234351737693\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  11\n",
      "\n",
      " train test split\n",
      "(8037, 272)\n",
      "(720, 272)\n",
      "\n",
      " \n",
      "torch.Size([8037, 13])\n",
      "torch.Size([8037, 258])\n",
      "torch.Size([720, 13])\n",
      "torch.Size([720, 258])\n",
      "\n",
      " sequences\n",
      "torch.Size([19, 13]) torch.Size([19, 258]) torch.Size([19, 258])\n",
      "\n",
      "attachment matrix\n",
      "torch.Size([258, 24])\n",
      "\n",
      " model inititalized\n",
      "\n",
      " model loaded\n",
      "Pretraining R2:  0.5680117483218043\n",
      "\n",
      " Training model...\n",
      "epoch:   0 loss: 1.30237019 r2: 0.543 rmse: 4.125 mae: 1.084\n",
      "epoch:   1 loss: 1.40016448 r2: 0.558 rmse: 3.991 mae: 1.075\n",
      "epoch:   2 loss: 1.31267202 r2: 0.557 rmse: 4.005 mae: 1.083\n",
      "epoch:   3 loss: 1.31357861 r2: 0.562 rmse: 3.954 mae: 1.065\n",
      "epoch:   4 loss: 1.31558740 r2: 0.575 rmse: 3.841 mae: 1.063\n",
      "epoch:   5 loss: 1.33419013 r2: 0.568 rmse: 3.903 mae: 1.071\n",
      "epoch:   6 loss: 1.28795862 r2: 0.559 rmse: 3.986 mae: 1.081\n",
      "epoch:   7 loss: 1.30481410 r2: 0.566 rmse: 3.917 mae: 1.071\n",
      "epoch:   8 loss: 1.29669106 r2: 0.564 rmse: 3.935 mae: 1.070\n",
      "epoch:   9 loss: 1.32421613 r2: 0.566 rmse: 3.916 mae: 1.067\n",
      "epoch:  10 loss: 1.29339325 r2: 0.563 rmse: 3.943 mae: 1.069\n",
      "epoch:  11 loss: 1.29337978 r2: 0.567 rmse: 3.911 mae: 1.066\n",
      "epoch:  12 loss: 1.29449558 r2: 0.562 rmse: 3.952 mae: 1.070\n",
      "epoch:  13 loss: 1.29366207 r2: 0.567 rmse: 3.912 mae: 1.065\n",
      "epoch:  14 loss: 1.30451679 r2: 0.567 rmse: 3.915 mae: 1.064\n",
      "epoch:  15 loss: 1.28622758 r2: 0.572 rmse: 3.865 mae: 1.062\n",
      "epoch:  16 loss: 1.27950823 r2: 0.568 rmse: 3.903 mae: 1.062\n",
      "epoch:  17 loss: 1.29085529 r2: 0.571 rmse: 3.874 mae: 1.062\n",
      "epoch:  18 loss: 1.28205490 r2: 0.567 rmse: 3.908 mae: 1.063\n",
      "epoch:  19 loss: 1.29580498 r2: 0.569 rmse: 3.896 mae: 1.064\n",
      "epoch:  20 loss: 1.28497791 r2: 0.573 rmse: 3.855 mae: 1.061\n",
      "epoch:  21 loss: 1.26372647 r2: 0.571 rmse: 3.876 mae: 1.064\n",
      "epoch:  22 loss: 1.31363630 r2: 0.567 rmse: 3.914 mae: 1.065\n",
      "epoch:  23 loss: 1.30534494 r2: 0.574 rmse: 3.850 mae: 1.060\n",
      "epoch:  24 loss: 1.27482593 r2: 0.575 rmse: 3.836 mae: 1.058\n",
      "epoch:  25 loss: 1.27477026 r2: 0.575 rmse: 3.836 mae: 1.056\n",
      "epoch:  26 loss: 1.29542351 r2: 0.574 rmse: 3.847 mae: 1.058\n",
      "epoch:  27 loss: 1.26089895 r2: 0.572 rmse: 3.868 mae: 1.058\n",
      "epoch:  28 loss: 1.25949645 r2: 0.571 rmse: 3.873 mae: 1.060\n",
      "epoch:  29 loss: 1.25160551 r2: 0.569 rmse: 3.897 mae: 1.058\n",
      "epoch:  30 loss: 1.25351429 r2: 0.575 rmse: 3.842 mae: 1.056\n",
      "epoch:  31 loss: 1.24389184 r2: 0.571 rmse: 3.877 mae: 1.060\n",
      "epoch:  32 loss: 1.27019835 r2: 0.573 rmse: 3.853 mae: 1.055\n",
      "epoch:  33 loss: 1.26201391 r2: 0.574 rmse: 3.850 mae: 1.055\n",
      "epoch:  34 loss: 1.27494240 r2: 0.571 rmse: 3.873 mae: 1.056\n",
      "epoch:  35 loss: 1.26309967 r2: 0.574 rmse: 3.849 mae: 1.056\n",
      "epoch:  36 loss: 1.27432060 r2: 0.574 rmse: 3.848 mae: 1.055\n",
      "epoch:  37 loss: 1.27225626 r2: 0.576 rmse: 3.829 mae: 1.053\n",
      "epoch:  38 loss: 1.27285302 r2: 0.576 rmse: 3.826 mae: 1.053\n",
      "epoch:  39 loss: 1.28338206 r2: 0.575 rmse: 3.837 mae: 1.054\n",
      "epoch:  40 loss: 1.29112577 r2: 0.572 rmse: 3.862 mae: 1.054\n",
      "epoch:  41 loss: 1.26716697 r2: 0.578 rmse: 3.816 mae: 1.053\n",
      "epoch:  42 loss: 1.26077759 r2: 0.576 rmse: 3.825 mae: 1.053\n",
      "epoch:  43 loss: 1.28071964 r2: 0.577 rmse: 3.819 mae: 1.052\n",
      "epoch:  44 loss: 1.24788272 r2: 0.576 rmse: 3.826 mae: 1.052\n",
      "epoch:  45 loss: 1.27195537 r2: 0.576 rmse: 3.828 mae: 1.053\n",
      "epoch:  46 loss: 1.27850676 r2: 0.575 rmse: 3.836 mae: 1.053\n",
      "epoch:  47 loss: 1.27194858 r2: 0.577 rmse: 3.820 mae: 1.053\n",
      "epoch:  48 loss: 1.27807748 r2: 0.574 rmse: 3.848 mae: 1.053\n",
      "epoch:  49 loss: 1.27477086 r2: 0.576 rmse: 3.831 mae: 1.053\n",
      "epoch:  50 loss: 1.27153659 r2: 0.576 rmse: 3.827 mae: 1.053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  51 loss: 1.26965928 r2: 0.576 rmse: 3.830 mae: 1.053\n",
      "epoch:  52 loss: 1.29015815 r2: 0.576 rmse: 3.826 mae: 1.053\n",
      "epoch:  53 loss: 1.27964044 r2: 0.575 rmse: 3.836 mae: 1.053\n",
      "epoch:  54 loss: 1.24988806 r2: 0.576 rmse: 3.832 mae: 1.053\n",
      "epoch:  55 loss: 1.28018224 r2: 0.575 rmse: 3.842 mae: 1.054\n",
      "epoch:  56 loss: 1.28104019 r2: 0.575 rmse: 3.841 mae: 1.054\n",
      "epoch:  57 loss: 1.27064764 r2: 0.576 rmse: 3.834 mae: 1.053\n",
      "epoch:  58 loss: 1.25706339 r2: 0.574 rmse: 3.844 mae: 1.054\n",
      "epoch:  59 loss: 1.26212490 r2: 0.576 rmse: 3.832 mae: 1.053\n",
      "epoch:  59 loss: 1.2621248960\n",
      "bet_r2:  0.5775233248991875\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  12\n",
      "\n",
      " train test split\n",
      "(8013, 272)\n",
      "(744, 272)\n",
      "\n",
      " \n",
      "torch.Size([8013, 13])\n",
      "torch.Size([8013, 258])\n",
      "torch.Size([744, 13])\n",
      "torch.Size([744, 258])\n",
      "\n",
      " sequences\n",
      "torch.Size([19, 13]) torch.Size([19, 258]) torch.Size([19, 258])\n",
      "\n",
      "attachment matrix\n",
      "torch.Size([258, 24])\n",
      "\n",
      " model inititalized\n",
      "\n",
      " model loaded\n",
      "Pretraining R2:  0.5479528175842562\n",
      "\n",
      " Training model...\n",
      "epoch:   0 loss: 1.19211030 r2: 0.547 rmse: 3.315 mae: 0.996\n",
      "epoch:   1 loss: 1.17759514 r2: 0.549 rmse: 3.298 mae: 0.993\n",
      "epoch:   2 loss: 1.17972851 r2: 0.553 rmse: 3.274 mae: 0.994\n",
      "epoch:   3 loss: 1.15686345 r2: 0.548 rmse: 3.308 mae: 0.993\n",
      "epoch:   4 loss: 1.18690908 r2: 0.552 rmse: 3.276 mae: 0.993\n",
      "epoch:   5 loss: 1.19606149 r2: 0.548 rmse: 3.303 mae: 0.996\n",
      "epoch:   6 loss: 1.18700385 r2: 0.543 rmse: 3.346 mae: 0.997\n",
      "epoch:   7 loss: 1.17410302 r2: 0.552 rmse: 3.279 mae: 0.992\n",
      "epoch:   8 loss: 1.18662286 r2: 0.550 rmse: 3.289 mae: 0.994\n",
      "epoch:   9 loss: 1.16629493 r2: 0.553 rmse: 3.269 mae: 0.992\n",
      "epoch:  10 loss: 1.17554808 r2: 0.552 rmse: 3.279 mae: 0.992\n",
      "epoch:  11 loss: 1.17749858 r2: 0.545 rmse: 3.332 mae: 0.999\n",
      "epoch:  12 loss: 1.17795455 r2: 0.546 rmse: 3.320 mae: 0.997\n",
      "epoch:  13 loss: 1.17751181 r2: 0.548 rmse: 3.310 mae: 0.995\n",
      "epoch:  14 loss: 1.17574334 r2: 0.548 rmse: 3.305 mae: 0.997\n",
      "epoch:  15 loss: 1.19170964 r2: 0.549 rmse: 3.302 mae: 0.996\n",
      "epoch:  16 loss: 1.16248822 r2: 0.544 rmse: 3.334 mae: 0.996\n",
      "epoch:  17 loss: 1.17140388 r2: 0.545 rmse: 3.327 mae: 0.998\n",
      "epoch:  18 loss: 1.15737700 r2: 0.550 rmse: 3.291 mae: 0.995\n",
      "epoch:  19 loss: 1.17846608 r2: 0.545 rmse: 3.329 mae: 0.999\n",
      "epoch:  20 loss: 1.15621436 r2: 0.545 rmse: 3.326 mae: 0.998\n",
      "epoch:  21 loss: 1.17272949 r2: 0.544 rmse: 3.339 mae: 0.999\n",
      "epoch:  22 loss: 1.16028428 r2: 0.545 rmse: 3.331 mae: 0.999\n",
      "epoch:  23 loss: 1.17696524 r2: 0.546 rmse: 3.324 mae: 1.000\n",
      "epoch:  24 loss: 1.18910384 r2: 0.545 rmse: 3.330 mae: 0.999\n",
      "epoch:  25 loss: 1.17640495 r2: 0.546 rmse: 3.325 mae: 0.998\n",
      "epoch:  26 loss: 1.17412639 r2: 0.545 rmse: 3.325 mae: 0.999\n",
      "epoch:  27 loss: 1.16751885 r2: 0.548 rmse: 3.309 mae: 0.997\n",
      "epoch:  28 loss: 1.16034496 r2: 0.545 rmse: 3.326 mae: 0.999\n",
      "epoch:  29 loss: 1.16295052 r2: 0.545 rmse: 3.326 mae: 0.997\n",
      "epoch:  30 loss: 1.18699217 r2: 0.547 rmse: 3.317 mae: 0.996\n",
      "epoch:  31 loss: 1.18640327 r2: 0.541 rmse: 3.360 mae: 1.002\n",
      "epoch:  32 loss: 1.18345439 r2: 0.543 rmse: 3.345 mae: 0.999\n",
      "epoch:  33 loss: 1.15439391 r2: 0.547 rmse: 3.317 mae: 0.998\n",
      "epoch:  34 loss: 1.17573047 r2: 0.545 rmse: 3.327 mae: 0.998\n",
      "epoch:  35 loss: 1.19542885 r2: 0.548 rmse: 3.306 mae: 0.996\n",
      "epoch:  36 loss: 1.18146920 r2: 0.550 rmse: 3.290 mae: 0.995\n",
      "epoch:  37 loss: 1.17934120 r2: 0.547 rmse: 3.313 mae: 0.996\n",
      "epoch:  38 loss: 1.16899276 r2: 0.545 rmse: 3.326 mae: 0.997\n",
      "epoch:  39 loss: 1.16343629 r2: 0.546 rmse: 3.322 mae: 0.996\n",
      "epoch:  40 loss: 1.15428555 r2: 0.544 rmse: 3.335 mae: 0.998\n",
      "epoch:  41 loss: 1.16128075 r2: 0.549 rmse: 3.301 mae: 0.995\n",
      "epoch:  42 loss: 1.18660903 r2: 0.548 rmse: 3.310 mae: 0.996\n",
      "epoch:  43 loss: 1.17065763 r2: 0.546 rmse: 3.324 mae: 0.997\n",
      "epoch:  44 loss: 1.19529712 r2: 0.550 rmse: 3.290 mae: 0.994\n",
      "epoch:  45 loss: 1.16990340 r2: 0.549 rmse: 3.299 mae: 0.995\n",
      "epoch:  46 loss: 1.17390454 r2: 0.549 rmse: 3.299 mae: 0.994\n",
      "epoch:  47 loss: 1.16518152 r2: 0.549 rmse: 3.297 mae: 0.993\n",
      "epoch:  48 loss: 1.18000519 r2: 0.550 rmse: 3.289 mae: 0.993\n",
      "epoch:  49 loss: 1.17421734 r2: 0.549 rmse: 3.297 mae: 0.994\n",
      "epoch:  50 loss: 1.17351520 r2: 0.549 rmse: 3.296 mae: 0.994\n",
      "epoch:  51 loss: 1.16086471 r2: 0.548 rmse: 3.306 mae: 0.996\n",
      "epoch:  52 loss: 1.17694151 r2: 0.550 rmse: 3.293 mae: 0.994\n",
      "epoch:  53 loss: 1.17330122 r2: 0.548 rmse: 3.305 mae: 0.995\n",
      "epoch:  54 loss: 1.14819479 r2: 0.549 rmse: 3.299 mae: 0.994\n",
      "epoch:  55 loss: 1.16587675 r2: 0.550 rmse: 3.288 mae: 0.993\n",
      "epoch:  56 loss: 1.17696643 r2: 0.551 rmse: 3.286 mae: 0.992\n",
      "epoch:  57 loss: 1.17147398 r2: 0.550 rmse: 3.294 mae: 0.993\n",
      "epoch:  58 loss: 1.15976000 r2: 0.550 rmse: 3.295 mae: 0.993\n",
      "epoch:  59 loss: 1.15710664 r2: 0.551 rmse: 3.288 mae: 0.992\n",
      "epoch:  59 loss: 1.1571066380\n",
      "bet_r2:  0.5531640653233972\n"
     ]
    }
   ],
   "source": [
    "bptt = config['bptt']\n",
    "\n",
    "R2List = []\n",
    "residual_list = []\n",
    "\n",
    "for m in range(2,13):\n",
    "    month_index  = pd.to_datetime(dataset.Date).dt.month == m\n",
    "    \n",
    "    print('-------------------------------------------------')\n",
    "    print('-------------------------------------------------')\n",
    "    print(\"Month: \", m)\n",
    "\n",
    "\n",
    "    trainData = dataset[~month_index]\n",
    "    testData = dataset[month_index]\n",
    "\n",
    "    print(\"\\n train test split\")\n",
    "    print(trainData.shape)\n",
    "    print(testData.shape)\n",
    "\n",
    "\n",
    "    print(\"\\n \")\n",
    "    X_train = trainData[features_cols].values\n",
    "    X_train = torch.tensor(X_train).float().to(device)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    y_train = trainData[targetColumns].values\n",
    "    y_train = torch.tensor(y_train).float().to(device)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    X_test = testData[features_cols].values\n",
    "    X_test = torch.tensor(X_test).float().to(device)\n",
    "    print(X_test.shape)\n",
    "\n",
    "    y_test = testData[targetColumns].values\n",
    "    y_test = torch.tensor(y_test).float().to(device)\n",
    "    print(y_test.shape)\n",
    "\n",
    "\n",
    "    train_inout_seq = create_inout_sequences(X_train,y_train, bptt)\n",
    "    test_inout_seq = create_inout_sequences(X_test,y_test, bptt)\n",
    "    print(\"\\n sequences\")\n",
    "    print(train_inout_seq[0][0].shape,train_inout_seq[0][1].shape, train_inout_seq[0][2].shape)\n",
    "\n",
    "    at_mat = get_at_mat(targetColumns)\n",
    "    print(\"\\nattachment matrix\")\n",
    "    print(at_mat.shape)\n",
    "\n",
    "\n",
    "    layers = config['layers']\n",
    "    communities = 24\n",
    "    network_size = len(targetColumns)\n",
    "    feat_size = len(features_cols)\n",
    "    dropout = config['dropout']\n",
    "\n",
    "    model = LSTM(feat_size = feat_size, hidden_layer_size=communities,\n",
    "                 network_size=network_size, layers=layers,\n",
    "                communities=communities, dropout=dropout, at_mat=at_mat).to(device)\n",
    "    \n",
    "    \n",
    "\n",
    "    loss_function = nn.L1Loss()   \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config['step_size'], gamma=config['gamma'])\n",
    "    print(\"\\n model inititalized\")\n",
    "\n",
    "    model.load_state_dict(torch.load(pretrained_weights))\n",
    "    print(\"\\n model loaded\")\n",
    "    \n",
    "    residual, r2, rmse, mae = evaluate(model)\n",
    "    print(\"Pretraining R2: \",r2)\n",
    "\n",
    "    \n",
    "    best_r2 = r2\n",
    "    best_residual = residual\n",
    "    torch.save(model.state_dict(), 'data/'+'jfk_'+str(m)+'.pt')\n",
    "    np.save('data/'+'jfk_'+str(m)+'.npy', best_residual)\n",
    "\n",
    "    epochs = 60\n",
    "    \n",
    "    print(\"\\n Training model...\")\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        for feat,seq, labels in train_inout_seq:\n",
    "            optimizer.zero_grad()\n",
    "            model.hidden_cell = (torch.zeros(layers, 1, model.hidden_layer_size).to(device),\n",
    "                            torch.zeros(layers, 1, model.hidden_layer_size).to(device))\n",
    "\n",
    "            y_pred = model(seq, feat)\n",
    "\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "    #     if i%1 == 1:\n",
    "        residual, r2, rmse, mae = evaluate(model)\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f} r2: {r2:5.3f} rmse: {rmse:5.3f} mae: {mae:5.3f}')\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_residual = residual\n",
    "            torch.save(model.state_dict(), 'data/'+'jfk_'+str(m)+'.pt')\n",
    "            np.save('data/'+'jfk_'+str(m)+'.npy', best_residual)\n",
    "\n",
    "    print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "    print(\"bet_r2: \", best_r2)\n",
    "\n",
    "    R2List.append(best_r2)\n",
    "    residual_list.append(best_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5506309348043397,\n",
       " 0.5972399097890982,\n",
       " 0.6083282515195536,\n",
       " 0.6267548650792119,\n",
       " 0.5933234870001999,\n",
       " 0.5512411542818053,\n",
       " 0.5207402905815595,\n",
       " 0.6349401338057566,\n",
       " 0.6334234351737693,\n",
       " 0.5775233248991875,\n",
       " 0.5531640653233972]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.586119077477989"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(R2List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 8.3843988e-01,  3.4259129e+00, -2.6372004e-01, ...,\n",
       "          2.0339184e+00, -1.1042626e+00,  4.1553318e-05],\n",
       "        [ 8.1632847e-01,  6.0170403e+00,  7.1888571e+00, ...,\n",
       "         -5.9521580e+00, -1.5044886e-01,  1.9242421e-05],\n",
       "        [ 7.4045676e-01,  5.5556431e+00,  1.5694609e+00, ...,\n",
       "         -8.8799286e-01, -2.1621661e+00,  1.2608720e-05],\n",
       "        ...,\n",
       "        [-1.3746703e-01,  3.5684948e+00,  3.7512650e+00, ...,\n",
       "         -2.1051121e+00, -1.0627986e+00,  1.2192690e-05],\n",
       "        [ 6.9374144e-01,  1.7612743e+00,  1.6717048e+00, ...,\n",
       "         -2.0053911e+00, -1.9521338e-01,  5.5752657e-06],\n",
       "        [ 6.5782297e-01,  3.3353701e+00,  2.7849641e+00, ...,\n",
       "          2.4795446e+00,  8.3695906e-01,  3.1513514e-06]], dtype=float32),\n",
       " array([[ 1.0036776e+00,  4.8286819e-01,  4.2021017e+00, ...,\n",
       "          9.9893236e-01,  1.1398672e+00,  5.8864476e-05],\n",
       "        [ 9.6979946e-01, -5.4186373e+00,  2.3234272e+00, ...,\n",
       "         -1.7519712e+00,  1.2915612e+00,  5.1110939e-05],\n",
       "        [-8.2873583e-02, -5.0424852e+00, -8.5612488e-01, ...,\n",
       "         -3.9810314e+00,  2.8144324e-01,  4.3526925e-05],\n",
       "        ...,\n",
       "        [ 9.9326372e-01, -4.0608377e+00, -2.6506433e+00, ...,\n",
       "         -6.0360565e+00,  2.5360930e-01,  4.2301872e-05],\n",
       "        [-7.7829063e-02, -1.5067711e+00,  1.8777618e+00, ...,\n",
       "          3.2862020e+00,  8.4940553e-02,  3.1658034e-05],\n",
       "        [ 8.9764488e-01,  2.0884161e+00,  4.3468256e+00, ...,\n",
       "         -2.7084808e+00,  1.0402057e+00,  2.0826994e-05]], dtype=float32),\n",
       " array([[-8.3339274e-01,  5.2689095e+00,  4.6539531e+00, ...,\n",
       "         -4.6040030e+00,  1.3606508e+00,  6.2860476e-05],\n",
       "        [ 1.2160802e-01, -2.8757114e+00,  4.3608637e+00, ...,\n",
       "         -4.0780153e+00,  3.5092509e-01,  5.0152485e-05],\n",
       "        [ 1.6009116e-01,  7.3323154e-01,  5.0262127e+00, ...,\n",
       "          1.6370010e-01,  5.1974905e-01,  4.5415160e-05],\n",
       "        ...,\n",
       "        [ 7.5075889e-01, -5.6577806e+00, -8.3441887e+00, ...,\n",
       "         -3.4342246e+00,  1.1942079e+00,  7.7126919e-05],\n",
       "        [ 7.9707736e-01,  6.3647175e-01,  4.9510441e+00, ...,\n",
       "         -4.6341748e+00,  1.2378489e+00,  7.1640156e-05],\n",
       "        [ 7.9890794e-01, -6.7302637e+00,  1.4952183e-01, ...,\n",
       "         -1.2375431e+00,  1.1500883e+00,  5.4995966e-05]], dtype=float32),\n",
       " array([[-7.08171129e-02,  3.84425259e+00,  1.80086899e+00, ...,\n",
       "         -3.55195999e+00, -7.35271096e-01,  8.35751052e-05],\n",
       "        [-2.08723116e+00,  8.69137764e-01, -1.63270855e+00, ...,\n",
       "          1.59860516e+00,  2.71348834e-01,  7.27980369e-05],\n",
       "        [ 8.36776137e-01, -5.71501541e+00, -5.98078728e-01, ...,\n",
       "         -2.69432354e+00,  1.33248937e+00,  5.96892205e-05],\n",
       "        ...,\n",
       "        [-1.11844802e+00,  1.36608601e+00,  2.05810642e+00, ...,\n",
       "          1.52043629e+00,  1.20637488e+00,  3.84541854e-05],\n",
       "        [ 8.55695546e-01, -3.51023960e+00,  2.59877968e+00, ...,\n",
       "          2.05192280e+00,  1.23334062e+00,  3.14200151e-05],\n",
       "        [-1.16065907e+00, -1.25426035e+01,  4.70331860e+00, ...,\n",
       "          1.51370716e+00, -8.33547115e-01,  2.61534606e-05]], dtype=float32),\n",
       " array([[ 1.7052937e-01, -6.3121614e+00, -4.3494730e+00, ...,\n",
       "          6.6049910e-01,  1.3481077e+00,  5.0901966e-05],\n",
       "        [ 1.6235948e-02,  4.9032249e+00,  8.1064796e-01, ...,\n",
       "         -1.8456173e-01, -7.2679305e-01,  3.4549215e-05],\n",
       "        [-1.0317876e+00,  5.5887384e+00, -1.1993895e+00, ...,\n",
       "          1.8140092e+00,  1.2352115e+00,  2.9765293e-05],\n",
       "        ...,\n",
       "        [ 1.4471292e-01, -6.2586966e+00,  1.2131128e+00, ...,\n",
       "          3.0898304e+00, -1.6201380e+00,  4.3800377e-05],\n",
       "        [-3.0344815e+00, -9.2488003e-01,  2.1727791e+00, ...,\n",
       "         -2.2248678e+00, -1.9055958e+00,  3.1337731e-05],\n",
       "        [-8.7551236e-02,  5.7259817e+00, -1.8185959e+00, ...,\n",
       "          3.5376453e-01,  4.3637753e-02,  1.4551073e-05]], dtype=float32),\n",
       " array([[-9.4880962e-01, -2.3396091e+00,  7.3307705e-01, ...,\n",
       "          4.4174972e+00,  1.1337992e+00,  2.1049824e-05],\n",
       "        [ 1.2198179e+00,  5.6647301e-01,  3.6839228e+00, ...,\n",
       "         -3.5460529e+00, -1.6537039e+00,  2.3653107e-05],\n",
       "        [ 2.1273732e-02, -4.1593618e+00,  8.7265968e-02, ...,\n",
       "         -7.1167860e+00, -1.8178887e+00,  2.6013588e-05],\n",
       "        ...,\n",
       "        [-1.1037669e+00, -3.8531971e-01,  2.8861237e-01, ...,\n",
       "          2.6919308e+00,  3.2570577e-01,  4.7519643e-05],\n",
       "        [-1.8384838e-01, -3.0107012e+00,  1.0551987e+00, ...,\n",
       "         -4.2110062e-01,  1.2031335e+00,  4.0214203e-05],\n",
       "        [-1.4821110e+00, -2.4421148e+00,  2.8774214e-01, ...,\n",
       "          1.4601865e+00, -9.6304357e-02,  1.3329989e-05]], dtype=float32),\n",
       " array([[-7.2772002e-01,  5.5682583e+00, -1.5798283e-01, ...,\n",
       "         -1.1507392e+00, -2.6239972e+00,  6.2281979e-05],\n",
       "        [ 1.3878131e-01,  1.8353090e+00, -4.1883430e+00, ...,\n",
       "          3.0009174e+00, -6.7668688e-01,  4.7710404e-05],\n",
       "        [ 1.0752172e+00, -1.3452482e+00,  2.4348631e+00, ...,\n",
       "         -3.0471134e-01, -3.6493855e+00,  5.2498021e-05],\n",
       "        ...,\n",
       "        [-9.5419240e-01, -3.8247261e+00,  1.7744551e+00, ...,\n",
       "         -4.3572626e+00, -5.6226716e+00,  4.7140427e-05],\n",
       "        [-2.9415363e-01, -4.1270161e-01,  1.5215960e+00, ...,\n",
       "         -3.7001696e+00, -1.9950081e+00,  2.7457010e-05],\n",
       "        [-1.2675518e+00,  6.2161541e-01, -3.7488146e+00, ...,\n",
       "          2.3758044e+00,  1.2328817e+00,  2.7380267e-05]], dtype=float32),\n",
       " array([[ 1.4795160e-01, -9.9957657e-01,  3.4410963e+00, ...,\n",
       "         -8.8328838e-02, -8.0164778e-01,  4.3441301e-05],\n",
       "        [ 4.5349956e-02, -2.6665239e+00, -1.3063955e-01, ...,\n",
       "         -2.3223019e-01,  1.5828121e-01,  2.5780022e-05],\n",
       "        [ 1.0889611e+00,  4.5186872e+00,  4.6173096e-01, ...,\n",
       "          3.9158468e+00, -1.7730898e+00,  3.7697791e-05],\n",
       "        ...,\n",
       "        [-2.0761802e+00, -2.5089598e+00,  1.5695963e+00, ...,\n",
       "         -4.9245167e+00, -1.7828957e+00,  7.4519507e-05],\n",
       "        [-3.2307684e-02, -9.4760895e-02,  2.0654993e+00, ...,\n",
       "         -6.7144747e+00, -1.6638553e+00,  7.6082353e-05],\n",
       "        [-9.9202704e-01, -2.5858183e+00, -3.3239050e+00, ...,\n",
       "         -4.0678787e-01, -2.5676565e+00,  6.6042572e-05]], dtype=float32),\n",
       " array([[ 9.7494113e-01,  5.3670979e-01, -3.7047510e+00, ...,\n",
       "          3.0913877e-01, -1.8271786e+00,  1.0218705e-04],\n",
       "        [-1.3215381e-01, -3.3555031e-02, -2.5813255e+00, ...,\n",
       "          2.8709412e-02,  1.6101217e-01,  8.2651008e-05],\n",
       "        [ 7.9070288e-01, -3.8123388e+00, -3.5688105e+00, ...,\n",
       "          5.7722807e+00, -1.7074960e+00,  6.8938578e-05],\n",
       "        ...,\n",
       "        [ 9.1251093e-01,  5.9941149e+00, -7.8513145e-02, ...,\n",
       "          3.7628460e-01,  3.9358664e-01,  5.6542580e-05],\n",
       "        [ 6.4763117e-01,  7.0756054e-01, -2.8572121e+00, ...,\n",
       "         -1.9922338e+00,  2.2687793e-02,  4.5774577e-05],\n",
       "        [ 6.2129915e-01,  3.4961405e+00, -1.2553735e+00, ...,\n",
       "          1.3961029e-01,  1.1547881e+00,  3.6733771e-05]], dtype=float32),\n",
       " array([[ 1.0190724e+00,  1.3342581e+00, -3.0762501e+00, ...,\n",
       "          1.0744166e+00, -3.1397343e-03, -1.1565247e-05],\n",
       "        [-6.6199481e-02, -2.8797598e+00,  3.1974049e+00, ...,\n",
       "         -7.7644014e-01, -1.3344759e-01, -2.2955568e-05],\n",
       "        [-8.0468750e+00,  7.0284452e+00, -1.9297810e+00, ...,\n",
       "         -7.1510715e+00, -1.2065172e-01, -1.1586126e-05],\n",
       "        ...,\n",
       "        [-1.8669199e+00,  2.3504267e+00, -9.6783543e-01, ...,\n",
       "         -6.0053368e+00, -1.0336199e+00, -4.2169222e-06],\n",
       "        [ 8.0660528e-01,  9.6896267e-01,  1.2687874e-01, ...,\n",
       "          1.7959824e+00,  8.3901381e-01,  1.1027478e-05],\n",
       "        [ 6.5435594e-01,  1.6444149e+00,  2.0127034e+00, ...,\n",
       "         -3.0312538e-02, -2.8688896e-01,  7.7829627e-06]], dtype=float32),\n",
       " array([[-1.03708935e+00, -7.94781685e-01,  2.47299099e+00, ...,\n",
       "         -2.99050474e+00, -3.20938826e-02,  6.01049869e-05],\n",
       "        [-1.15287256e+00, -4.19786358e+00,  2.91712284e+00, ...,\n",
       "          2.28711128e+00, -2.10264015e+00,  1.03967905e-04],\n",
       "        [ 1.13200092e+00, -7.24513626e+00, -2.59594727e+00, ...,\n",
       "         -2.36416912e+00,  1.17408121e+00,  1.34929622e-04],\n",
       "        ...,\n",
       "        [ 6.08694434e-01,  4.32889271e+00, -2.84455776e-01, ...,\n",
       "         -1.37223053e+00, -2.39515185e-01,  1.79872713e-05],\n",
       "        [ 5.62800348e-01, -1.56986141e+00,  1.47312951e+00, ...,\n",
       "         -4.58676386e+00, -2.52556443e-01,  6.22073276e-06],\n",
       "        [-6.27280831e-01, -6.85845566e+00,  1.91983581e+00, ...,\n",
       "          2.07016850e+00, -1.61200118e+00, -3.02594308e-05]], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.530\n",
    "0.5506309348043397\n",
    "0.5972399097890982\n",
    "0.6083282515195536\n",
    "0.6267548650792119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  1\n",
      "\n",
      " test shape\n",
      "(722, 272)\n",
      "\n",
      " residual\n",
      "(722, 258)\n",
      "         Date  Hour         1        10       100       101       102  \\\n",
      "0  2018-01-01    22  1.073634 -3.288347  3.073706 -0.965400 -0.819268   \n",
      "1  2018-01-01    23  0.250151 -3.579380 -0.048968  1.032399  1.154741   \n",
      "2  2018-01-02     0 -1.733820  0.225997  0.682261 -1.058100 -0.916202   \n",
      "3  2018-01-02     1 -0.902148 -3.739099  1.196124 -0.850154  0.225197   \n",
      "4  2018-01-02     2 -0.000178 -0.117501  0.509563  0.001086 -0.989059   \n",
      "\n",
      "        106        107       108  ...        90        91        92        93  \\\n",
      "0 -1.754588  -2.832455 -0.884940  ... -3.921993  0.348744  3.898218 -0.940414   \n",
      "1 -0.800730 -15.068804  0.149996  ... -2.709341  0.696428 -5.188190  0.015907   \n",
      "2  1.089349  -1.090286  0.160533  ...  0.753269  1.398265  0.725189 -0.990966   \n",
      "3 -2.829922  -3.289001  0.039112  ... -1.984349  1.384919 -1.415277 -0.008739   \n",
      "4  0.006134  -3.215868  0.000318  ... -3.467297 -1.637727 -0.818416  0.001460   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0  0.002937  3.439336  0.000458  5.367305  0.714760  0.000010  \n",
      "1  0.001502  0.811171  0.000237  2.080948 -0.590178 -0.000009  \n",
      "2  0.001376  1.753793  0.000151  2.080092  1.233203  0.000004  \n",
      "3 -0.000585  0.979212  0.000056 -0.613677 -1.677565 -0.000002  \n",
      "4 -0.000240 -0.963300  0.000015 -1.661212  0.070346 -0.000005  \n",
      "\n",
      "[5 rows x 260 columns]\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  2\n",
      "\n",
      " test shape\n",
      "(653, 272)\n",
      "\n",
      " residual\n",
      "(653, 258)\n",
      "         Date  Hour         1        10       100       101       102  \\\n",
      "0  2018-02-01    19  0.838440  3.425913 -0.263720 -1.006994 -0.001927   \n",
      "1  2018-02-01    20  0.816328  6.017040  7.188857 -0.006934  0.010269   \n",
      "2  2018-02-01    21  0.740457  5.555643  1.569461  0.995317  1.014242   \n",
      "3  2018-02-01    22  0.602404  5.593664 -2.449355  1.002588  1.017070   \n",
      "4  2018-02-01    23  0.571662  0.416470 -2.258288  0.990632  0.999124   \n",
      "\n",
      "        106       107       108  ...        90        91        92        93  \\\n",
      "0  0.019421 -0.101795  0.018410  ... -1.160722 -1.063798  3.126017 -0.002452   \n",
      "1 -0.933175  1.884440 -0.975411  ... -3.781452  2.031068 -1.155316 -1.000607   \n",
      "2  0.061444  0.698396 -0.956544  ...  0.466969 -1.976646  2.755980 -3.000370   \n",
      "3  0.039287  2.204553 -0.941026  ... -0.890567 -2.838794 -1.359711 -0.000950   \n",
      "4 -0.983416  6.294840  0.058265  ... -5.341832  0.169102  3.617138 -0.001375   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0  0.000193 -2.805268 -0.000012  2.033918 -1.104263  0.000042  \n",
      "1  0.000094  2.154613 -0.000103 -5.952158 -0.150449  0.000019  \n",
      "2  0.000065  2.272879 -0.000051 -0.887993 -2.162166  0.000013  \n",
      "3  0.000056 -0.049788  0.000037  1.465561  0.778589  0.000009  \n",
      "4  0.000060  1.715100  0.000086  1.119757  0.690602  0.000009  \n",
      "\n",
      "[5 rows x 260 columns]\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  3\n",
      "\n",
      " test shape\n",
      "(725, 272)\n",
      "\n",
      " residual\n",
      "(725, 258)\n",
      "         Date  Hour         1        10       100       101       102  \\\n",
      "0  2018-03-01    19  1.003678  0.482868  4.202102  0.996986  0.097023   \n",
      "1  2018-03-01    20  0.969799 -5.418637  2.323427  0.017948  0.131336   \n",
      "2  2018-03-01    21 -0.082874 -5.042485 -0.856125 -0.994377  1.124194   \n",
      "3  2018-03-01    22  0.867877 -1.422482  2.064123  0.005666  1.119298   \n",
      "4  2018-03-01    23  0.741112  9.260858  1.793193 -0.984896  1.108892   \n",
      "\n",
      "        106       107       108  ...        90        91        92        93  \\\n",
      "0  0.040415 -1.080408  0.048779  ...  1.120762 -0.639506 -1.571972 -0.957812   \n",
      "1 -0.870311 -7.361962 -0.934822  ... -3.785593  1.450230 -3.034516  0.042124   \n",
      "2  1.149121 -1.025372  0.148359  ...  4.643553  1.881772 -6.855748  0.027682   \n",
      "3  0.149159  1.468188 -0.786988  ...  0.040088  3.341259 -0.393477 -1.991894   \n",
      "4  0.144242 -0.412807  0.217954  ...  1.148243 -2.531868  2.761431  0.001454   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0  0.001454 -1.124569  0.000266  0.998932  1.139867  0.000059  \n",
      "1 -0.997809  1.728251  0.000560 -1.751971  1.291561  0.000051  \n",
      "2 -0.997585  2.743613 -0.999670 -3.981031  0.281443  0.000044  \n",
      "3  0.001884 -1.893298 -0.000019  1.925293 -0.808833  0.000038  \n",
      "4 -0.998416 -0.634461 -0.000061  4.001769  1.095299  0.000018  \n",
      "\n",
      "[5 rows x 260 columns]\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  4\n",
      "\n",
      " test shape\n",
      "(701, 272)\n",
      "\n",
      " residual\n",
      "(701, 258)\n",
      "         Date  Hour         1        10       100       101       102  \\\n",
      "0  2018-04-01    19 -0.833393  5.268909  4.653953 -0.980447  1.180634   \n",
      "1  2018-04-01    20  0.121608 -2.875711  4.360864  1.032055 -0.813578   \n",
      "2  2018-04-01    21  0.160091  0.733232  5.026213  1.041399 -0.773689   \n",
      "3  2018-04-01    22  0.099119 -3.144914 -1.072381  1.039610 -0.759949   \n",
      "4  2018-04-01    23  0.553358  7.558944  1.316004  1.045673  1.225604   \n",
      "\n",
      "        106       107       108  ...        90        91        92        93  \\\n",
      "0  1.135814 -2.171263  0.084626  ... -5.342448  0.593887 -1.790968  0.063939   \n",
      "1  0.235926 -6.358538  0.098265  ... -8.142864  0.393665  2.907814 -0.926990   \n",
      "2  0.306776  2.588247  0.153896  ...  4.350441  1.720196 -0.964115 -0.930833   \n",
      "3 -1.663470 -0.560610 -0.795831  ... -7.092499  0.103024  1.326263 -0.950081   \n",
      "4  1.294796  2.406679 -0.783265  ...  8.990045  1.369623 -3.831668 -0.980361   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0  0.002944  3.207994  0.000468 -4.604003  1.360651  0.000063  \n",
      "1  0.003398  2.720716  0.000586 -4.078015  0.350925  0.000050  \n",
      "2  0.003252  3.948565 -0.999637  0.163700  0.519749  0.000045  \n",
      "3 -0.997414  5.373422  0.000069  1.226871 -0.464682  0.000044  \n",
      "4  0.001270  2.748670 -0.000118  7.041376  1.352150  0.000020  \n",
      "\n",
      "[5 rows x 260 columns]\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  5\n",
      "\n",
      " test shape\n",
      "(725, 272)\n",
      "\n",
      " residual\n",
      "(725, 258)\n",
      "         Date  Hour         1        10       100       101       102  \\\n",
      "0  2018-05-01    19 -0.070817  3.844253  1.800869 -0.991918  1.123829   \n",
      "1  2018-05-01    20 -2.087231  0.869138 -1.632709  1.018776 -0.874919   \n",
      "2  2018-05-01    21  0.836776 -5.715015 -0.598079  1.008994  1.131088   \n",
      "3  2018-05-01    22 -0.460891  0.596560 -1.234582  1.007093  0.106624   \n",
      "4  2018-05-01    23  0.683695 -6.816115  0.895201  1.009510  0.121942   \n",
      "\n",
      "        106       107       108  ...        90        91        92        93  \\\n",
      "0  1.051373  3.084388  0.061098  ... -1.684216  0.616523 -2.894273  0.055438   \n",
      "1 -0.867709 -3.102566 -2.936181  ...  3.605961 -0.475883 -0.132549  0.057531   \n",
      "2  0.137341  1.448491  0.103574  ...  2.903395 -2.253729 -1.057580 -0.953117   \n",
      "3  0.065761 -3.183091  0.135721  ... -4.488467  0.655006 -2.202945 -0.992437   \n",
      "4 -0.865921 -0.253556  0.198251  ...  0.947309 -3.624768 -3.414133 -1.001733   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0 -0.998450  1.637954  0.000072 -3.551960 -0.735271  0.000084  \n",
      "1 -0.997746  0.129424  0.000409  1.598605  0.271349  0.000073  \n",
      "2 -0.997645  2.857599 -0.999671 -2.694324  1.332489  0.000060  \n",
      "3  0.001040 -1.091014 -0.000057  3.738677 -0.101615  0.000041  \n",
      "4  0.000942  3.719570 -1.000121  1.066278  0.198836  0.000037  \n",
      "\n",
      "[5 rows x 260 columns]\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  6\n",
      "\n",
      " test shape\n",
      "(701, 272)\n",
      "\n",
      " residual\n",
      "(701, 258)\n",
      "         Date  Hour         1        10       100       101       102  \\\n",
      "0  2018-06-01    19  0.170529 -6.312161 -4.349473  1.013479  1.120163   \n",
      "1  2018-06-01    20  0.016236  4.903225  0.810648  0.019419  1.109930   \n",
      "2  2018-06-01    21 -1.031788  5.588738 -1.199389 -1.988571  1.102076   \n",
      "3  2018-06-01    22  0.923919 -1.151057 -0.494586  1.007629  1.093903   \n",
      "4  2018-06-01    23 -0.178232 -4.392438 -0.655596  0.005535  1.087105   \n",
      "\n",
      "        106       107       108  ...        90        91        92        93  \\\n",
      "0  1.057834 -2.376713  0.060849  ...  5.479934  1.460924  1.427294 -1.958045   \n",
      "1 -3.905791  1.070628  0.060015  ...  2.161342  0.326432  1.879896  0.033471   \n",
      "2  1.082880  3.343398  0.091259  ... -2.141758 -0.669426 -2.086072 -0.980745   \n",
      "3 -0.925502  1.599428  0.138873  ...  0.128578  2.647521 -1.720994 -0.001746   \n",
      "4 -1.907121 -0.407566 -0.812505  ... -2.645424  1.080239 -1.316566 -0.015996   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0  0.001677  2.555582  0.000282  0.660499  1.348108  0.000051  \n",
      "1 -1.998102 -1.663892  0.000525 -0.184562 -0.726793  0.000035  \n",
      "2  0.001752 -1.569954 -0.999580  1.814009  1.235211  0.000030  \n",
      "3  0.001086 -2.363297 -0.999870 -6.488707 -0.828744  0.000025  \n",
      "4  0.000767  0.428107 -0.000049 -2.133111  0.144754  0.000016  \n",
      "\n",
      "[5 rows x 260 columns]\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  7\n",
      "\n",
      " test shape\n",
      "(725, 272)\n",
      "\n",
      " residual\n",
      "(725, 258)\n",
      "         Date  Hour         1        10       100       101       102  \\\n",
      "0  2018-07-01    19 -0.948810 -2.339609  0.733077  0.013271  0.097738   \n",
      "1  2018-07-01    20  1.219818  0.566473  3.683923  0.014323 -0.890146   \n",
      "2  2018-07-01    21  0.021274 -4.159362  0.087266  0.007393  0.104397   \n",
      "3  2018-07-01    22  0.790805 -0.643907 -1.040540  1.000950  1.090992   \n",
      "4  2018-07-01    23 -1.563263  0.626781 -2.279691  0.004099 -0.932389   \n",
      "\n",
      "        106       107       108  ...        90        91        92        93  \\\n",
      "0  0.046821 -9.009774 -0.925359  ...  4.655285  0.295811 -2.308987 -0.975137   \n",
      "1 -1.867750  7.167894  0.116394  ... -3.455452 -1.331499 -1.659124 -0.966102   \n",
      "2  0.117615 -5.392005 -0.821245  ... -0.734842 -0.069317  4.550930 -0.994835   \n",
      "3 -0.864344  2.164298 -1.772629  ... -3.971037  2.364297 -0.236108 -0.009821   \n",
      "4  1.085626 -2.819379  0.212033  ...  1.638315  0.310093 -0.636749 -0.016971   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0  0.001738 -5.019055  0.000399  4.417497  1.133799  0.000021  \n",
      "1  0.002578 -3.097014  0.000496 -3.546053 -1.653704  0.000024  \n",
      "2 -0.998132 -2.218774  0.000148 -7.116786 -1.817889  0.000026  \n",
      "3  0.001352  1.369201 -0.000062 -2.958581  1.172720  0.000012  \n",
      "4 -0.999322  2.521918 -0.000206 -2.933923 -0.084328 -0.000010  \n",
      "\n",
      "[5 rows x 260 columns]\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  8\n",
      "\n",
      " test shape\n",
      "(725, 272)\n",
      "\n",
      " residual\n",
      "(725, 258)\n",
      "         Date  Hour         1        10       100       101       102  \\\n",
      "0  2018-08-01    19 -0.727720  5.568258 -0.157983  0.006172 -0.885961   \n",
      "1  2018-08-01    20  0.138781  1.835309 -4.188343  0.019553  0.125220   \n",
      "2  2018-08-01    21  1.075217 -1.345248  2.434863 -0.987491  1.128028   \n",
      "3  2018-08-01    22 -1.050809 -3.796567 -1.070895 -1.988688  1.115044   \n",
      "4  2018-08-01    23  0.896620  0.460484  2.877380  0.007811 -0.884169   \n",
      "\n",
      "        106       107       108  ...        90        91        92        93  \\\n",
      "0  1.040986  1.788538  0.047734  ...  0.239743  1.529681 -0.437802 -1.951096   \n",
      "1  0.074086 -1.670019  0.042714  ...  0.977132 -0.605053 -2.125299  0.040234   \n",
      "2  1.103900 -0.202579  0.097409  ...  2.536932 -0.408167 -0.834856  0.026023   \n",
      "3  0.071584 -0.323408  0.120206  ... -1.546185 -2.271052  4.379519 -0.991538   \n",
      "4  0.107147  0.836169 -1.824584  ... -2.123982 -2.759310  0.776515 -1.007934   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0  0.001366  2.976508  0.000147 -1.150739 -2.623997  0.000062  \n",
      "1  0.001630  4.818992 -0.999564  3.000917 -0.676687  0.000048  \n",
      "2  0.001687  4.574073  0.000305 -0.304711 -3.649385  0.000052  \n",
      "3  0.000929  0.259332  0.000064  2.969449  1.182691  0.000043  \n",
      "4  0.000524  1.331978 -0.000107  1.524301 -0.718016  0.000038  \n",
      "\n",
      "[5 rows x 260 columns]\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  9\n",
      "\n",
      " test shape\n",
      "(701, 272)\n",
      "\n",
      " residual\n",
      "(701, 258)\n",
      "         Date  Hour         1        10       100       101       102  \\\n",
      "0  2018-09-01    19  0.147952 -0.999577  3.441096  0.992452 -0.915408   \n",
      "1  2018-09-01    20  0.045350 -2.666524 -0.130640 -0.994024 -0.903637   \n",
      "2  2018-09-01    21  1.088961  4.518687  0.461731  0.009211  1.115836   \n",
      "3  2018-09-01    22  0.034709 -2.253844  3.394225 -0.990593 -0.882723   \n",
      "4  2018-09-01    23 -0.180312 -2.130651  0.575612  0.004290 -1.924559   \n",
      "\n",
      "        106       107       108  ...        90        91        92        93  \\\n",
      "0 -0.976001 -0.438653  0.047024  ...  0.261304 -2.853356  3.470620 -0.970248   \n",
      "1  0.055583 -1.065932  0.062074  ... -1.694806 -1.694228  3.035501 -2.981053   \n",
      "2  0.068686 -0.932518  0.115075  ...  4.170709 -1.373936  1.439019 -0.999923   \n",
      "3 -0.899052  5.424705 -0.827920  ...  2.744466  3.169801  1.936519 -0.014494   \n",
      "4  0.065062  1.567673 -0.851157  ... -0.455917 -1.147032  1.513822 -1.018118   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0  0.001096 -0.256413  0.000220 -0.088329 -0.801648  0.000043  \n",
      "1  0.001029 -0.326270  0.000311 -0.232230  0.158281  0.000026  \n",
      "2  0.000682 -3.743658  0.000101  3.915847 -1.773090  0.000038  \n",
      "3 -0.999596  0.368333 -0.000077 -4.502714  0.291677  0.000035  \n",
      "4 -0.000217 -3.103871 -0.000214  0.315620 -0.055962 -0.000002  \n",
      "\n",
      "[5 rows x 260 columns]\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  10\n",
      "\n",
      " test shape\n",
      "(725, 272)\n",
      "\n",
      " residual\n",
      "(725, 258)\n",
      "         Date  Hour         1        10       100       101       102  \\\n",
      "0  2018-10-01    19  0.974941  0.536710 -3.704751  1.014872 -3.815573   \n",
      "1  2018-10-01    20 -0.132154 -0.033555 -2.581326  1.023823 -0.833552   \n",
      "2  2018-10-01    21  0.790703 -3.812339 -3.568810  1.011583 -0.848434   \n",
      "3  2018-10-01    22 -0.142317  1.584746 -5.423977 -0.992059  0.141044   \n",
      "4  2018-10-01    23 -0.148479 -6.498555  0.801517  0.009034  1.137519   \n",
      "\n",
      "        106       107       108  ...        90        91        92        93  \\\n",
      "0  1.090513 -7.096410 -0.919216  ...  0.368993 -0.081772  1.162140  0.062736   \n",
      "1  0.153279  1.515308 -0.924629  ... -2.161753 -2.265869  2.919222  0.058675   \n",
      "2  0.164981 -5.981040 -1.879954  ...  3.006790  1.939430 -3.863770 -0.950300   \n",
      "3 -0.838966 -3.613145  0.203258  ... -5.418139  0.603376 -0.250363 -0.968417   \n",
      "4  1.172415  5.173696 -0.761759  ...  2.911963 -3.132793 -2.829175  0.011965   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0 -0.997851 -0.570233  0.000160  0.309139 -1.827179  0.000102  \n",
      "1  0.002492 -1.473436  0.000465  0.028709  0.161012  0.000083  \n",
      "2 -0.997396 -2.734591 -0.999647  5.772281 -1.707496  0.000069  \n",
      "3  0.001923 -6.253795 -0.000199  0.538064 -1.635689  0.000064  \n",
      "4 -1.998524 -1.546956 -1.000282  0.857923  0.238859  0.000050  \n",
      "\n",
      "[5 rows x 260 columns]\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  11\n",
      "\n",
      " test shape\n",
      "(701, 272)\n",
      "\n",
      " residual\n",
      "(701, 258)\n",
      "         Date  Hour         1        10       100       101       102  \\\n",
      "0  2018-11-01    19  1.019072  1.334258 -3.076250 -0.002788 -0.972684   \n",
      "1  2018-11-01    20 -0.066199 -2.879760  3.197405 -0.009857  1.019953   \n",
      "2  2018-11-01    21 -8.046875  7.028445 -1.929781  0.991423  1.025217   \n",
      "3  2018-11-01    22 -5.115577 -1.033954 -3.902410  0.990423 -0.983505   \n",
      "4  2018-11-01    23 -0.246697  2.050323 -1.151185 -1.009509  0.011080   \n",
      "\n",
      "        106       107       108  ...        90        91        92        93  \\\n",
      "0  1.022991  2.094192  0.044633  ...  0.822272 -1.803607 -0.877684 -1.010722   \n",
      "1 -1.947857  3.847200 -0.960825  ... -0.860712  2.070603  0.645879 -0.007587   \n",
      "2 -3.921778 -4.794701 -0.934017  ...  0.368880  0.166461 -1.049521 -0.006122   \n",
      "3 -1.880987  2.920095 -0.874954  ... -3.701853 -0.377360  1.355098 -0.005192   \n",
      "4  1.131967  1.598445 -0.825649  ...  0.440037 -2.063852  4.440412 -0.004737   \n",
      "\n",
      "         94        95        96        97        98            99  \n",
      "0  0.000169  3.588642  0.000083  1.074417 -0.003140 -1.156525e-05  \n",
      "1 -0.000090  0.672590 -0.000017 -0.776440 -0.133448 -2.295557e-05  \n",
      "2 -0.000204  0.289425 -0.000082 -7.151072 -0.120652 -1.158613e-05  \n",
      "3 -0.000196 -4.038959 -0.000014  6.357916  0.878171 -4.459798e-07  \n",
      "4 -0.000177  1.618338  0.000028 -0.273031 -0.315387  6.420671e-06  \n",
      "\n",
      "[5 rows x 260 columns]\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Month:  12\n",
      "\n",
      " test shape\n",
      "(725, 272)\n",
      "\n",
      " residual\n",
      "(725, 258)\n",
      "         Date  Hour         1        10       100       101       102  \\\n",
      "0  2018-12-01    19 -1.037089 -0.794782  2.472991 -0.037466 -1.000076   \n",
      "1  2018-12-01    20 -1.152873 -4.197864  2.917123 -2.020848 -0.008910   \n",
      "2  2018-12-01    21  1.132001 -7.245136 -2.595947  0.991743  0.961912   \n",
      "3  2018-12-01    22 -0.155162  3.032771  1.731452  0.997163  0.945838   \n",
      "4  2018-12-01    23 -0.532110  0.682151  1.456369 -1.088666 -3.208180   \n",
      "\n",
      "        106       107       108  ...        90        91        92        93  \\\n",
      "0  0.989644  3.949044  0.033493  ...  1.329187 -1.087409  2.473930 -1.004040   \n",
      "1 -0.988265 -1.103500  0.048182  ... -4.440741  1.855902 -3.212984 -1.005252   \n",
      "2 -0.913491 -2.949297  0.145324  ... -3.441288  1.489866 -0.328559 -0.000159   \n",
      "3 -0.972810 -6.367738 -1.858240  ...  2.125439  1.463168  2.408936 -0.006166   \n",
      "4 -0.131150 -7.378699  0.083520  ...  0.729717  1.305964  1.506292 -0.004073   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0  0.000270 -0.239604 -1.000089 -2.990505 -0.032094  0.000060  \n",
      "1  0.000182 -4.941377 -0.000142  2.287111 -2.102640  0.000104  \n",
      "2 -0.999882 -3.954603  0.000054 -2.364169  1.174081  0.000135  \n",
      "3  0.000206 -3.702996 -0.000189  0.666865 -0.101880  0.000083  \n",
      "4  0.000338  0.060810 -0.000069  1.088312 -0.500084  0.000044  \n",
      "\n",
      "[5 rows x 260 columns]\n"
     ]
    }
   ],
   "source": [
    "bptt = config['bptt']\n",
    "\n",
    "resdf_list = []\n",
    "\n",
    "for m in range(1,13):\n",
    "    month_index  = pd.to_datetime(dataset.Date).dt.month == m\n",
    "    \n",
    "    print('-------------------------------------------------')\n",
    "    print('-------------------------------------------------')\n",
    "    print(\"Month: \", m)\n",
    "\n",
    "    testData = dataset[month_index]\n",
    "    testData = testData[bptt:]\n",
    "    date = testData['Date']\n",
    "    hour = testData['Hour']\n",
    "    \n",
    "    print(\"\\n test shape\")\n",
    "    print(testData.shape)\n",
    "\n",
    "    residual = np.load('data/'+'jfk_'+str(m)+'.npy')\n",
    "    print(\"\\n residual\")\n",
    "    print(residual.shape)\n",
    "\n",
    "    res_df = pd.DataFrame(residual)\n",
    "    res_df.columns = targetColumns\n",
    "    res_df['Date'] = testData['Date'].values\n",
    "    res_df['Hour'] = testData['Hour'].values\n",
    "    res_df = res_df[['Date', 'Hour'] + targetColumns]\n",
    "    \n",
    "    print(res_df.head())\n",
    "    resdf_list.append(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>22</td>\n",
       "      <td>1.073634</td>\n",
       "      <td>-3.288347</td>\n",
       "      <td>3.073706</td>\n",
       "      <td>-0.965400</td>\n",
       "      <td>-0.819268</td>\n",
       "      <td>-1.754588</td>\n",
       "      <td>-2.832455</td>\n",
       "      <td>-0.884940</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.921993</td>\n",
       "      <td>0.348744</td>\n",
       "      <td>3.898218</td>\n",
       "      <td>-0.940414</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>3.439336</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>5.367305</td>\n",
       "      <td>0.714760</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>23</td>\n",
       "      <td>0.250151</td>\n",
       "      <td>-3.579380</td>\n",
       "      <td>-0.048968</td>\n",
       "      <td>1.032399</td>\n",
       "      <td>1.154741</td>\n",
       "      <td>-0.800730</td>\n",
       "      <td>-15.068804</td>\n",
       "      <td>0.149996</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.709341</td>\n",
       "      <td>0.696428</td>\n",
       "      <td>-5.188190</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.811171</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>2.080948</td>\n",
       "      <td>-0.590178</td>\n",
       "      <td>-0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.733820</td>\n",
       "      <td>0.225997</td>\n",
       "      <td>0.682261</td>\n",
       "      <td>-1.058100</td>\n",
       "      <td>-0.916202</td>\n",
       "      <td>1.089349</td>\n",
       "      <td>-1.090286</td>\n",
       "      <td>0.160533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753269</td>\n",
       "      <td>1.398265</td>\n",
       "      <td>0.725189</td>\n",
       "      <td>-0.990966</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>1.753793</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>2.080092</td>\n",
       "      <td>1.233203</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.902148</td>\n",
       "      <td>-3.739099</td>\n",
       "      <td>1.196124</td>\n",
       "      <td>-0.850154</td>\n",
       "      <td>0.225197</td>\n",
       "      <td>-2.829922</td>\n",
       "      <td>-3.289001</td>\n",
       "      <td>0.039112</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.984349</td>\n",
       "      <td>1.384919</td>\n",
       "      <td>-1.415277</td>\n",
       "      <td>-0.008739</td>\n",
       "      <td>-0.000585</td>\n",
       "      <td>0.979212</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-0.613677</td>\n",
       "      <td>-1.677565</td>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.117501</td>\n",
       "      <td>0.509563</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>-0.989059</td>\n",
       "      <td>0.006134</td>\n",
       "      <td>-3.215868</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.467297</td>\n",
       "      <td>-1.637727</td>\n",
       "      <td>-0.818416</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>-0.963300</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-1.661212</td>\n",
       "      <td>0.070346</td>\n",
       "      <td>-0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Hour         1        10       100       101       102  \\\n",
       "0  2018-01-01    22  1.073634 -3.288347  3.073706 -0.965400 -0.819268   \n",
       "1  2018-01-01    23  0.250151 -3.579380 -0.048968  1.032399  1.154741   \n",
       "2  2018-01-02     0 -1.733820  0.225997  0.682261 -1.058100 -0.916202   \n",
       "3  2018-01-02     1 -0.902148 -3.739099  1.196124 -0.850154  0.225197   \n",
       "4  2018-01-02     2 -0.000178 -0.117501  0.509563  0.001086 -0.989059   \n",
       "\n",
       "        106        107       108  ...        90        91        92        93  \\\n",
       "0 -1.754588  -2.832455 -0.884940  ... -3.921993  0.348744  3.898218 -0.940414   \n",
       "1 -0.800730 -15.068804  0.149996  ... -2.709341  0.696428 -5.188190  0.015907   \n",
       "2  1.089349  -1.090286  0.160533  ...  0.753269  1.398265  0.725189 -0.990966   \n",
       "3 -2.829922  -3.289001  0.039112  ... -1.984349  1.384919 -1.415277 -0.008739   \n",
       "4  0.006134  -3.215868  0.000318  ... -3.467297 -1.637727 -0.818416  0.001460   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.002937  3.439336  0.000458  5.367305  0.714760  0.000010  \n",
       "1  0.001502  0.811171  0.000237  2.080948 -0.590178 -0.000009  \n",
       "2  0.001376  1.753793  0.000151  2.080092  1.233203  0.000004  \n",
       "3 -0.000585  0.979212  0.000056 -0.613677 -1.677565 -0.000002  \n",
       "4 -0.000240 -0.963300  0.000015 -1.661212  0.070346 -0.000005  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_res_df = pd.concat(resdf_list)\n",
    "all_res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_df.to_csv('data/residual_jfk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attachment = torch.argmax(F.softmax(model.attachment_matrix, dim=1), dim=1).detach().cpu().numpy()\n",
    "# community_assignment = dict(zip(targetColumns, attachment))\n",
    "# community_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 comm\n",
    "# 0.505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 comm\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
