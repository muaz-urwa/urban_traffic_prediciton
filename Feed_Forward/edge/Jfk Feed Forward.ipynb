{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/urwa/Documents/side_projects/urban/data/featureData/jfk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, scheduler, criterion,epochs = 500):\n",
    "    losses = []\n",
    "    # Main optimization loop\n",
    "    for t in range(epochs):\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        y_predicted = model(X_train)\n",
    "\n",
    "        current_loss = criterion(y_predicted, y_train)\n",
    "\n",
    "        current_loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        print(f\"t = {t}, loss = {current_loss}\")\n",
    "\n",
    "        losses.append(current_loss)\n",
    "\n",
    "        scheduler.step()    \n",
    "    return losses,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8757, 1049)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>...</th>\n",
       "      <th>91_lag_3</th>\n",
       "      <th>92_lag_3</th>\n",
       "      <th>93_lag_3</th>\n",
       "      <th>94_lag_3</th>\n",
       "      <th>95_lag_3</th>\n",
       "      <th>96_lag_3</th>\n",
       "      <th>97_lag_3</th>\n",
       "      <th>98_lag_3</th>\n",
       "      <th>99_lag_3</th>\n",
       "      <th>arrival_lag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 1049 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Hour  1  10  100  101  102  106  107  108  ...  91_lag_3  \\\n",
       "0  2018-01-01     3  0   0    0    0    0    0    0    0  ...       1.0   \n",
       "1  2018-01-01     4  0   3    0    0    1    0    0    1  ...       4.0   \n",
       "2  2018-01-01     5  0   4    0    0    1    2    3    1  ...       0.0   \n",
       "\n",
       "   92_lag_3  93_lag_3  94_lag_3  95_lag_3  96_lag_3  97_lag_3  98_lag_3  \\\n",
       "0       1.0       0.0       1.0       6.0       0.0       1.0       0.0   \n",
       "1       1.0       0.0       0.0       2.0       0.0       0.0       0.0   \n",
       "2       0.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "\n",
       "   99_lag_3  arrival_lag_3  \n",
       "0       0.0            6.0  \n",
       "1       0.0            6.0  \n",
       "2       0.0            2.0  \n",
       "\n",
       "[3 rows x 1049 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Net(nn.Module):\n",
    "    def __init__(self, in_features,out_features):\n",
    "        super(Linear_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=out_features, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Net(nn.Module):\n",
    "    def __init__(self, in_features,out_features):\n",
    "        super(Simple_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=1000, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=1000, out_features=500, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=500, out_features=out_features, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = self.fc3(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag_columns = [c for c in dataset.columns if 'lag' in c]\n",
    "len(lag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DateColumns = ['Date']\n",
    "\n",
    "ext_columns = ['Dow', 'arrival','maxtemp', 'mintemp', 'avgtemp', 'departure', 'hdd',\n",
    "       'cdd', 'participation', 'newsnow', 'snowdepth', 'ifSnow']\n",
    "\n",
    "targetColumns = [c for c in dataset.columns if c not in ext_columns and \\\n",
    "                c not in DateColumns and c not in lag_columns and c != 'Hour']\n",
    "len(targetColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_cols = [c for c in dataset.columns if c not in targetColumns and c not in DateColumns]\n",
    "len(features_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset[features_cols].values\n",
    "y = dataset[targetColumns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_x = StandardScaler()\n",
    "# scaler_y = StandardScaler()\n",
    "\n",
    "# scaler_x.fit(x)\n",
    "# scaler_y.fit(y)\n",
    "\n",
    "# x = scaler_x.transform(x)\n",
    "# y = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8757, 790])\n",
      "torch.Size([8757, 258])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(x).float().to(device)\n",
    "print(x.shape)\n",
    "y = torch.tensor(y).float().to(device)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 3.945075511932373\n",
      "t = 1, loss = 3.5066444873809814\n",
      "t = 2, loss = 3.5782277584075928\n",
      "t = 3, loss = 3.760641574859619\n",
      "t = 4, loss = 3.6900789737701416\n",
      "t = 5, loss = 3.8508338928222656\n",
      "t = 6, loss = 3.707540512084961\n",
      "t = 7, loss = 3.8576674461364746\n",
      "t = 8, loss = 3.7046079635620117\n",
      "t = 9, loss = 3.8610286712646484\n",
      "t = 10, loss = 3.7039833068847656\n",
      "t = 11, loss = 3.866161823272705\n",
      "t = 12, loss = 3.7032883167266846\n",
      "t = 13, loss = 3.8670191764831543\n",
      "t = 14, loss = 3.701852560043335\n",
      "t = 15, loss = 3.8673946857452393\n",
      "t = 16, loss = 3.7023205757141113\n",
      "t = 17, loss = 3.869685411453247\n",
      "t = 18, loss = 3.701101779937744\n",
      "t = 19, loss = 3.870021343231201\n",
      "t = 20, loss = 3.6984243392944336\n",
      "t = 21, loss = 3.869065761566162\n",
      "t = 22, loss = 3.69588041305542\n",
      "t = 23, loss = 3.8680527210235596\n",
      "t = 24, loss = 3.6939237117767334\n",
      "t = 25, loss = 3.8673267364501953\n",
      "t = 26, loss = 3.6928598880767822\n",
      "t = 27, loss = 3.867093563079834\n",
      "t = 28, loss = 3.692352533340454\n",
      "t = 29, loss = 3.867011308670044\n",
      "t = 30, loss = 3.6925439834594727\n",
      "t = 31, loss = 3.8675549030303955\n",
      "t = 32, loss = 3.693270444869995\n",
      "t = 33, loss = 3.8690133094787598\n",
      "t = 34, loss = 3.6949222087860107\n",
      "t = 35, loss = 3.8702549934387207\n",
      "t = 36, loss = 3.697006940841675\n",
      "t = 37, loss = 3.871816635131836\n",
      "t = 38, loss = 3.698399782180786\n",
      "t = 39, loss = 3.8734779357910156\n",
      "t = 40, loss = 3.6998751163482666\n",
      "t = 41, loss = 3.874638080596924\n",
      "t = 42, loss = 3.7011678218841553\n",
      "t = 43, loss = 3.8760430812835693\n",
      "t = 44, loss = 3.7023003101348877\n",
      "t = 45, loss = 3.877056121826172\n",
      "t = 46, loss = 3.7038989067077637\n",
      "t = 47, loss = 3.87947416305542\n",
      "t = 48, loss = 3.7053639888763428\n",
      "t = 49, loss = 3.88181209564209\n",
      "t = 50, loss = 3.706674814224243\n",
      "t = 51, loss = 3.8843069076538086\n",
      "t = 52, loss = 3.7078895568847656\n",
      "t = 53, loss = 3.886214256286621\n",
      "t = 54, loss = 3.7083425521850586\n",
      "t = 55, loss = 3.887587308883667\n",
      "t = 56, loss = 3.7084245681762695\n",
      "t = 57, loss = 3.8886587619781494\n",
      "t = 58, loss = 3.708817481994629\n",
      "t = 59, loss = 3.8894472122192383\n",
      "t = 60, loss = 3.709106922149658\n",
      "t = 61, loss = 3.8902852535247803\n",
      "t = 62, loss = 3.7100017070770264\n",
      "t = 63, loss = 3.891420364379883\n",
      "t = 64, loss = 3.7111918926239014\n",
      "t = 65, loss = 3.892911672592163\n",
      "t = 66, loss = 3.711812734603882\n",
      "t = 67, loss = 3.893763303756714\n",
      "t = 68, loss = 3.7126991748809814\n",
      "t = 69, loss = 3.8947808742523193\n",
      "t = 70, loss = 3.7132232189178467\n",
      "t = 71, loss = 3.895728826522827\n",
      "t = 72, loss = 3.7137322425842285\n",
      "t = 73, loss = 3.8966243267059326\n",
      "t = 74, loss = 3.7140517234802246\n",
      "t = 75, loss = 3.897306442260742\n",
      "t = 76, loss = 3.7143449783325195\n",
      "t = 77, loss = 3.898257255554199\n",
      "t = 78, loss = 3.7147834300994873\n",
      "t = 79, loss = 3.899142026901245\n",
      "t = 80, loss = 3.715233564376831\n",
      "t = 81, loss = 3.9000463485717773\n",
      "t = 82, loss = 3.715622901916504\n",
      "t = 83, loss = 3.9007601737976074\n",
      "t = 84, loss = 3.716115713119507\n",
      "t = 85, loss = 3.9015886783599854\n",
      "t = 86, loss = 3.7165486812591553\n",
      "t = 87, loss = 3.9022557735443115\n",
      "t = 88, loss = 3.7171146869659424\n",
      "t = 89, loss = 3.9026942253112793\n",
      "t = 90, loss = 3.7174527645111084\n",
      "t = 91, loss = 3.903409719467163\n",
      "t = 92, loss = 3.7176737785339355\n",
      "t = 93, loss = 3.903806447982788\n",
      "t = 94, loss = 3.7180871963500977\n",
      "t = 95, loss = 3.904785394668579\n",
      "t = 96, loss = 3.7180819511413574\n",
      "t = 97, loss = 3.9052579402923584\n",
      "t = 98, loss = 3.7184252738952637\n",
      "t = 99, loss = 3.9061381816864014\n",
      "t = 100, loss = 3.718510150909424\n",
      "t = 101, loss = 3.0791914463043213\n",
      "t = 102, loss = 2.5179660320281982\n",
      "t = 103, loss = 2.077589511871338\n",
      "t = 104, loss = 1.7850615978240967\n",
      "t = 105, loss = 1.6237856149673462\n",
      "t = 106, loss = 1.5491878986358643\n",
      "t = 107, loss = 1.5188277959823608\n",
      "t = 108, loss = 1.5064178705215454\n",
      "t = 109, loss = 1.5004793405532837\n",
      "t = 110, loss = 1.4969444274902344\n",
      "t = 111, loss = 1.494404911994934\n",
      "t = 112, loss = 1.4923115968704224\n",
      "t = 113, loss = 1.490458369255066\n",
      "t = 114, loss = 1.4887526035308838\n",
      "t = 115, loss = 1.4871550798416138\n",
      "t = 116, loss = 1.4856452941894531\n",
      "t = 117, loss = 1.4842064380645752\n",
      "t = 118, loss = 1.4828300476074219\n",
      "t = 119, loss = 1.4815070629119873\n",
      "t = 120, loss = 1.4802345037460327\n",
      "t = 121, loss = 1.4790107011795044\n",
      "t = 122, loss = 1.4778302907943726\n",
      "t = 123, loss = 1.4766913652420044\n",
      "t = 124, loss = 1.4755918979644775\n",
      "t = 125, loss = 1.4745287895202637\n",
      "t = 126, loss = 1.4734992980957031\n",
      "t = 127, loss = 1.4725021123886108\n",
      "t = 128, loss = 1.4715354442596436\n",
      "t = 129, loss = 1.4705966711044312\n",
      "t = 130, loss = 1.4696837663650513\n",
      "t = 131, loss = 1.468797206878662\n",
      "t = 132, loss = 1.4679347276687622\n",
      "t = 133, loss = 1.4670953750610352\n",
      "t = 134, loss = 1.4662777185440063\n",
      "t = 135, loss = 1.4654803276062012\n",
      "t = 136, loss = 1.4647023677825928\n",
      "t = 137, loss = 1.4639430046081543\n",
      "t = 138, loss = 1.4632014036178589\n",
      "t = 139, loss = 1.4624760150909424\n",
      "t = 140, loss = 1.4617670774459839\n",
      "t = 141, loss = 1.461073637008667\n",
      "t = 142, loss = 1.4603936672210693\n",
      "t = 143, loss = 1.4597268104553223\n",
      "t = 144, loss = 1.4590731859207153\n",
      "t = 145, loss = 1.4584331512451172\n",
      "t = 146, loss = 1.4578046798706055\n",
      "t = 147, loss = 1.4571877717971802\n",
      "t = 148, loss = 1.4565812349319458\n",
      "t = 149, loss = 1.4559847116470337\n",
      "t = 150, loss = 1.4553985595703125\n",
      "t = 151, loss = 1.4548224210739136\n",
      "t = 152, loss = 1.45425546169281\n",
      "t = 153, loss = 1.4536972045898438\n",
      "t = 154, loss = 1.453147530555725\n",
      "t = 155, loss = 1.4526067972183228\n",
      "t = 156, loss = 1.4520740509033203\n",
      "t = 157, loss = 1.451548457145691\n",
      "t = 158, loss = 1.4510306119918823\n",
      "t = 159, loss = 1.4505205154418945\n",
      "t = 160, loss = 1.4500174522399902\n",
      "t = 161, loss = 1.4495210647583008\n",
      "t = 162, loss = 1.4490312337875366\n",
      "t = 163, loss = 1.4485479593276978\n",
      "t = 164, loss = 1.4480708837509155\n",
      "t = 165, loss = 1.4476001262664795\n",
      "t = 166, loss = 1.4471349716186523\n",
      "t = 167, loss = 1.4466760158538818\n",
      "t = 168, loss = 1.4462225437164307\n",
      "t = 169, loss = 1.4457751512527466\n",
      "t = 170, loss = 1.4453336000442505\n",
      "t = 171, loss = 1.4448972940444946\n",
      "t = 172, loss = 1.4444659948349\n",
      "t = 173, loss = 1.444039225578308\n",
      "t = 174, loss = 1.4436174631118774\n",
      "t = 175, loss = 1.4432004690170288\n",
      "t = 176, loss = 1.4427878856658936\n",
      "t = 177, loss = 1.442379355430603\n",
      "t = 178, loss = 1.4419753551483154\n",
      "t = 179, loss = 1.441575288772583\n",
      "t = 180, loss = 1.441179633140564\n",
      "t = 181, loss = 1.4407880306243896\n",
      "t = 182, loss = 1.4404008388519287\n",
      "t = 183, loss = 1.4400173425674438\n",
      "t = 184, loss = 1.439637541770935\n",
      "t = 185, loss = 1.4392614364624023\n",
      "t = 186, loss = 1.4388890266418457\n",
      "t = 187, loss = 1.4385201930999756\n",
      "t = 188, loss = 1.438154935836792\n",
      "t = 189, loss = 1.4377928972244263\n",
      "t = 190, loss = 1.437434434890747\n",
      "t = 191, loss = 1.4370794296264648\n",
      "t = 192, loss = 1.4367274045944214\n",
      "t = 193, loss = 1.4363783597946167\n",
      "t = 194, loss = 1.4360326528549194\n",
      "t = 195, loss = 1.4356895685195923\n",
      "t = 196, loss = 1.4353495836257935\n",
      "t = 197, loss = 1.435012698173523\n",
      "t = 198, loss = 1.4346784353256226\n",
      "t = 199, loss = 1.434347152709961\n",
      "t = 200, loss = 1.4340183734893799\n",
      "t = 201, loss = 1.4339858293533325\n",
      "t = 202, loss = 1.4339531660079956\n",
      "t = 203, loss = 1.4339205026626587\n",
      "t = 204, loss = 1.4338879585266113\n",
      "t = 205, loss = 1.433855414390564\n",
      "t = 206, loss = 1.4338229894638062\n",
      "t = 207, loss = 1.4337904453277588\n",
      "t = 208, loss = 1.4337579011917114\n",
      "t = 209, loss = 1.4337255954742432\n",
      "t = 210, loss = 1.4336930513381958\n",
      "t = 211, loss = 1.4336607456207275\n",
      "t = 212, loss = 1.4336282014846802\n",
      "t = 213, loss = 1.4335960149765015\n",
      "t = 214, loss = 1.4335637092590332\n",
      "t = 215, loss = 1.433531403541565\n",
      "t = 216, loss = 1.4334992170333862\n",
      "t = 217, loss = 1.433466911315918\n",
      "t = 218, loss = 1.4334347248077393\n",
      "t = 219, loss = 1.43340265750885\n",
      "t = 220, loss = 1.4333704710006714\n",
      "t = 221, loss = 1.4333384037017822\n",
      "t = 222, loss = 1.433306336402893\n",
      "t = 223, loss = 1.433274269104004\n",
      "t = 224, loss = 1.4332420825958252\n",
      "t = 225, loss = 1.4332102537155151\n",
      "t = 226, loss = 1.4331780672073364\n",
      "t = 227, loss = 1.433146357536316\n",
      "t = 228, loss = 1.4331144094467163\n",
      "t = 229, loss = 1.4330823421478271\n",
      "t = 230, loss = 1.433050513267517\n",
      "t = 231, loss = 1.4330185651779175\n",
      "t = 232, loss = 1.432986855506897\n",
      "t = 233, loss = 1.4329549074172974\n",
      "t = 234, loss = 1.4329231977462769\n",
      "t = 235, loss = 1.4328914880752563\n",
      "t = 236, loss = 1.4328596591949463\n",
      "t = 237, loss = 1.4328280687332153\n",
      "t = 238, loss = 1.4327963590621948\n",
      "t = 239, loss = 1.4327646493911743\n",
      "t = 240, loss = 1.4327329397201538\n",
      "t = 241, loss = 1.4327012300491333\n",
      "t = 242, loss = 1.4326698780059814\n",
      "t = 243, loss = 1.432638168334961\n",
      "t = 244, loss = 1.43260657787323\n",
      "t = 245, loss = 1.432574987411499\n",
      "t = 246, loss = 1.4325436353683472\n",
      "t = 247, loss = 1.4325120449066162\n",
      "t = 248, loss = 1.4324806928634644\n",
      "t = 249, loss = 1.4324491024017334\n",
      "t = 250, loss = 1.4324177503585815\n",
      "t = 251, loss = 1.4323861598968506\n",
      "t = 252, loss = 1.4323550462722778\n",
      "t = 253, loss = 1.4323235750198364\n",
      "t = 254, loss = 1.4322922229766846\n",
      "t = 255, loss = 1.4322611093521118\n",
      "t = 256, loss = 1.4322298765182495\n",
      "t = 257, loss = 1.4321985244750977\n",
      "t = 258, loss = 1.432167410850525\n",
      "t = 259, loss = 1.432136058807373\n",
      "t = 260, loss = 1.4321048259735107\n",
      "t = 261, loss = 1.4320738315582275\n",
      "t = 262, loss = 1.4320424795150757\n",
      "t = 263, loss = 1.4320114850997925\n",
      "t = 264, loss = 1.4319803714752197\n",
      "t = 265, loss = 1.431949257850647\n",
      "t = 266, loss = 1.4319182634353638\n",
      "t = 267, loss = 1.4318872690200806\n",
      "t = 268, loss = 1.4318562746047974\n",
      "t = 269, loss = 1.4318252801895142\n",
      "t = 270, loss = 1.431794285774231\n",
      "t = 271, loss = 1.4317632913589478\n",
      "t = 272, loss = 1.4317326545715332\n",
      "t = 273, loss = 1.43170166015625\n",
      "t = 274, loss = 1.4316707849502563\n",
      "t = 275, loss = 1.4316399097442627\n",
      "t = 276, loss = 1.4316091537475586\n",
      "t = 277, loss = 1.431578278541565\n",
      "t = 278, loss = 1.4315476417541504\n",
      "t = 279, loss = 1.4315166473388672\n",
      "t = 280, loss = 1.4314860105514526\n",
      "t = 281, loss = 1.4314552545547485\n",
      "t = 282, loss = 1.431424617767334\n",
      "t = 283, loss = 1.4313939809799194\n",
      "t = 284, loss = 1.4313633441925049\n",
      "t = 285, loss = 1.4313328266143799\n",
      "t = 286, loss = 1.4313020706176758\n",
      "t = 287, loss = 1.4312715530395508\n",
      "t = 288, loss = 1.4312410354614258\n",
      "t = 289, loss = 1.4312105178833008\n",
      "t = 290, loss = 1.4311798810958862\n",
      "t = 291, loss = 1.4311493635177612\n",
      "t = 292, loss = 1.4311188459396362\n",
      "t = 293, loss = 1.4310885667800903\n",
      "t = 294, loss = 1.4310581684112549\n",
      "t = 295, loss = 1.4310277700424194\n",
      "t = 296, loss = 1.4309974908828735\n",
      "t = 297, loss = 1.430967092514038\n",
      "t = 298, loss = 1.430936574935913\n",
      "t = 299, loss = 1.4309064149856567\n",
      "t = 300, loss = 1.4308760166168213\n",
      "t = 301, loss = 1.4308730363845825\n",
      "t = 302, loss = 1.4308700561523438\n",
      "t = 303, loss = 1.4308669567108154\n",
      "t = 304, loss = 1.4308639764785767\n",
      "t = 305, loss = 1.4308608770370483\n",
      "t = 306, loss = 1.4308580160140991\n",
      "t = 307, loss = 1.4308547973632812\n",
      "t = 308, loss = 1.4308518171310425\n",
      "t = 309, loss = 1.4308488368988037\n",
      "t = 310, loss = 1.430845856666565\n",
      "t = 311, loss = 1.4308428764343262\n",
      "t = 312, loss = 1.4308397769927979\n",
      "t = 313, loss = 1.430836796760559\n",
      "t = 314, loss = 1.4308336973190308\n",
      "t = 315, loss = 1.4308308362960815\n",
      "t = 316, loss = 1.4308276176452637\n",
      "t = 317, loss = 1.430824637413025\n",
      "t = 318, loss = 1.4308216571807861\n",
      "t = 319, loss = 1.4308184385299683\n",
      "t = 320, loss = 1.430815577507019\n",
      "t = 321, loss = 1.4308125972747803\n",
      "t = 322, loss = 1.430809497833252\n",
      "t = 323, loss = 1.4308065176010132\n",
      "t = 324, loss = 1.430803656578064\n",
      "t = 325, loss = 1.430800437927246\n",
      "t = 326, loss = 1.4307973384857178\n",
      "t = 327, loss = 1.4307944774627686\n",
      "t = 328, loss = 1.4307914972305298\n",
      "t = 329, loss = 1.4307883977890015\n",
      "t = 330, loss = 1.4307854175567627\n",
      "t = 331, loss = 1.4307823181152344\n",
      "t = 332, loss = 1.430779218673706\n",
      "t = 333, loss = 1.4307764768600464\n",
      "t = 334, loss = 1.4307732582092285\n",
      "t = 335, loss = 1.4307703971862793\n",
      "t = 336, loss = 1.430767297744751\n",
      "t = 337, loss = 1.4307641983032227\n",
      "t = 338, loss = 1.4307612180709839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 339, loss = 1.4307582378387451\n",
      "t = 340, loss = 1.4307552576065063\n",
      "t = 341, loss = 1.4307522773742676\n",
      "t = 342, loss = 1.4307490587234497\n",
      "t = 343, loss = 1.4307461977005005\n",
      "t = 344, loss = 1.4307432174682617\n",
      "t = 345, loss = 1.4307399988174438\n",
      "t = 346, loss = 1.4307372570037842\n",
      "t = 347, loss = 1.4307340383529663\n",
      "t = 348, loss = 1.430731177330017\n",
      "t = 349, loss = 1.4307280778884888\n",
      "t = 350, loss = 1.4307249784469604\n",
      "t = 351, loss = 1.4307219982147217\n",
      "t = 352, loss = 1.4307188987731934\n",
      "t = 353, loss = 1.4307160377502441\n",
      "t = 354, loss = 1.4307130575180054\n",
      "t = 355, loss = 1.4307100772857666\n",
      "t = 356, loss = 1.4307068586349487\n",
      "t = 357, loss = 1.4307039976119995\n",
      "t = 358, loss = 1.4307008981704712\n",
      "t = 359, loss = 1.430698037147522\n",
      "t = 360, loss = 1.430694818496704\n",
      "t = 361, loss = 1.4306918382644653\n",
      "t = 362, loss = 1.4306888580322266\n",
      "t = 363, loss = 1.4306858777999878\n",
      "t = 364, loss = 1.4306827783584595\n",
      "t = 365, loss = 1.4306797981262207\n",
      "t = 366, loss = 1.430676817893982\n",
      "t = 367, loss = 1.4306738376617432\n",
      "t = 368, loss = 1.4306708574295044\n",
      "t = 369, loss = 1.4306676387786865\n",
      "t = 370, loss = 1.4306647777557373\n",
      "t = 371, loss = 1.4306617975234985\n",
      "t = 372, loss = 1.4306588172912598\n",
      "t = 373, loss = 1.430655837059021\n",
      "t = 374, loss = 1.4306527376174927\n",
      "t = 375, loss = 1.430649757385254\n",
      "t = 376, loss = 1.4306467771530151\n",
      "t = 377, loss = 1.4306435585021973\n",
      "t = 378, loss = 1.4306408166885376\n",
      "t = 379, loss = 1.4306375980377197\n",
      "t = 380, loss = 1.4306347370147705\n",
      "t = 381, loss = 1.4306316375732422\n",
      "t = 382, loss = 1.430628776550293\n",
      "t = 383, loss = 1.430625557899475\n",
      "t = 384, loss = 1.4306226968765259\n",
      "t = 385, loss = 1.4306195974349976\n",
      "t = 386, loss = 1.4306167364120483\n",
      "t = 387, loss = 1.4306137561798096\n",
      "t = 388, loss = 1.4306107759475708\n",
      "t = 389, loss = 1.430607557296753\n",
      "t = 390, loss = 1.4306046962738037\n",
      "t = 391, loss = 1.4306015968322754\n",
      "t = 392, loss = 1.4305987358093262\n",
      "t = 393, loss = 1.4305955171585083\n",
      "t = 394, loss = 1.4305925369262695\n",
      "t = 395, loss = 1.4305895566940308\n",
      "t = 396, loss = 1.430586576461792\n",
      "t = 397, loss = 1.4305835962295532\n",
      "t = 398, loss = 1.430580496788025\n",
      "t = 399, loss = 1.4305775165557861\n",
      "t = 400, loss = 1.430574655532837\n",
      "t = 401, loss = 1.4305744171142578\n",
      "t = 402, loss = 1.4305740594863892\n",
      "t = 403, loss = 1.4305737018585205\n",
      "t = 404, loss = 1.4305734634399414\n",
      "t = 405, loss = 1.4305731058120728\n",
      "t = 406, loss = 1.430572748184204\n",
      "t = 407, loss = 1.430572509765625\n",
      "t = 408, loss = 1.4305721521377563\n",
      "t = 409, loss = 1.4305717945098877\n",
      "t = 410, loss = 1.4305715560913086\n",
      "t = 411, loss = 1.43057119846344\n",
      "t = 412, loss = 1.4305708408355713\n",
      "t = 413, loss = 1.4305706024169922\n",
      "t = 414, loss = 1.4305702447891235\n",
      "t = 415, loss = 1.430570125579834\n",
      "t = 416, loss = 1.4305696487426758\n",
      "t = 417, loss = 1.4305692911148071\n",
      "t = 418, loss = 1.4305691719055176\n",
      "t = 419, loss = 1.430569052696228\n",
      "t = 420, loss = 1.4305686950683594\n",
      "t = 421, loss = 1.4305682182312012\n",
      "t = 422, loss = 1.4305680990219116\n",
      "t = 423, loss = 1.430567741394043\n",
      "t = 424, loss = 1.4305673837661743\n",
      "t = 425, loss = 1.4305671453475952\n",
      "t = 426, loss = 1.4305667877197266\n",
      "t = 427, loss = 1.430566430091858\n",
      "t = 428, loss = 1.4305663108825684\n",
      "t = 429, loss = 1.4305659532546997\n",
      "t = 430, loss = 1.4305657148361206\n",
      "t = 431, loss = 1.430565357208252\n",
      "t = 432, loss = 1.4305649995803833\n",
      "t = 433, loss = 1.4305647611618042\n",
      "t = 434, loss = 1.4305644035339355\n",
      "t = 435, loss = 1.430564045906067\n",
      "t = 436, loss = 1.4305638074874878\n",
      "t = 437, loss = 1.4305634498596191\n",
      "t = 438, loss = 1.4305630922317505\n",
      "t = 439, loss = 1.4305628538131714\n",
      "t = 440, loss = 1.4305624961853027\n",
      "t = 441, loss = 1.430562138557434\n",
      "t = 442, loss = 1.430561900138855\n",
      "t = 443, loss = 1.4305616617202759\n",
      "t = 444, loss = 1.4305614233016968\n",
      "t = 445, loss = 1.4305613040924072\n",
      "t = 446, loss = 1.4305609464645386\n",
      "t = 447, loss = 1.43056058883667\n",
      "t = 448, loss = 1.4305603504180908\n",
      "t = 449, loss = 1.4305599927902222\n",
      "t = 450, loss = 1.4305596351623535\n",
      "t = 451, loss = 1.4305593967437744\n",
      "t = 452, loss = 1.4305590391159058\n",
      "t = 453, loss = 1.430558681488037\n",
      "t = 454, loss = 1.430558443069458\n",
      "t = 455, loss = 1.4305580854415894\n",
      "t = 456, loss = 1.4305577278137207\n",
      "t = 457, loss = 1.4305574893951416\n",
      "t = 458, loss = 1.430557131767273\n",
      "t = 459, loss = 1.4305570125579834\n",
      "t = 460, loss = 1.4305566549301147\n",
      "t = 461, loss = 1.430556297302246\n",
      "t = 462, loss = 1.430556058883667\n",
      "t = 463, loss = 1.4305557012557983\n",
      "t = 464, loss = 1.4305553436279297\n",
      "t = 465, loss = 1.4305551052093506\n",
      "t = 466, loss = 1.430554747581482\n",
      "t = 467, loss = 1.4305543899536133\n",
      "t = 468, loss = 1.4305541515350342\n",
      "t = 469, loss = 1.4305537939071655\n",
      "t = 470, loss = 1.430553674697876\n",
      "t = 471, loss = 1.4305531978607178\n",
      "t = 472, loss = 1.4305531978607178\n",
      "t = 473, loss = 1.4305528402328491\n",
      "t = 474, loss = 1.43055260181427\n",
      "t = 475, loss = 1.4305522441864014\n",
      "t = 476, loss = 1.4305518865585327\n",
      "t = 477, loss = 1.4305516481399536\n",
      "t = 478, loss = 1.430551290512085\n",
      "t = 479, loss = 1.4305509328842163\n",
      "t = 480, loss = 1.4305506944656372\n",
      "t = 481, loss = 1.4305503368377686\n",
      "t = 482, loss = 1.4305499792099\n",
      "t = 483, loss = 1.4305497407913208\n",
      "t = 484, loss = 1.4305493831634521\n",
      "t = 485, loss = 1.4305490255355835\n",
      "t = 486, loss = 1.4305487871170044\n",
      "t = 487, loss = 1.4305484294891357\n",
      "t = 488, loss = 1.430548071861267\n",
      "t = 489, loss = 1.430547833442688\n",
      "t = 490, loss = 1.4305474758148193\n",
      "t = 491, loss = 1.4305471181869507\n",
      "t = 492, loss = 1.4305468797683716\n",
      "t = 493, loss = 1.430546522140503\n",
      "t = 494, loss = 1.430546522140503\n",
      "t = 495, loss = 1.4305460453033447\n",
      "t = 496, loss = 1.4305459260940552\n",
      "t = 497, loss = 1.4305455684661865\n",
      "t = 498, loss = 1.4305453300476074\n",
      "t = 499, loss = 1.4305449724197388\n",
      "Training R2:  0.395613137984783\n",
      "Test R2:  0.3895255834209832\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_feat = len(features_cols)\n",
    "out_feat = len(targetColumns)\n",
    "model = Linear_Net(in_features=in_feat ,out_features=out_feat).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=100, gamma=0.1)\n",
    "\n",
    "losses,model = train(model, optimiser, scheduler, criterion, epochs = 500)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "    y_pred_train = model(X_train)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "\n",
    "train_r2 = r2_score(y_train.detach().cpu().numpy(), y_pred_train.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "test_r2=r2_score(y_test.detach().cpu().numpy(), y_pred_test.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "\n",
    "print(\"Training R2: \",train_r2)\n",
    "print(\"Test R2: \",test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 2.506361246109009\n",
      "t = 1, loss = 2.2773966789245605\n",
      "t = 2, loss = 2.177927255630493\n",
      "t = 3, loss = 2.065777540206909\n",
      "t = 4, loss = 1.9325838088989258\n",
      "t = 5, loss = 1.81338369846344\n",
      "t = 6, loss = 1.7266521453857422\n",
      "t = 7, loss = 1.7068370580673218\n",
      "t = 8, loss = 1.660282015800476\n",
      "t = 9, loss = 1.6279807090759277\n",
      "t = 10, loss = 1.5599104166030884\n",
      "t = 11, loss = 1.53435218334198\n",
      "t = 12, loss = 1.5048505067825317\n",
      "t = 13, loss = 1.4758268594741821\n",
      "t = 14, loss = 1.4561454057693481\n",
      "t = 15, loss = 1.4291974306106567\n",
      "t = 16, loss = 1.4079010486602783\n",
      "t = 17, loss = 1.3852083683013916\n",
      "t = 18, loss = 1.3751728534698486\n",
      "t = 19, loss = 1.3541477918624878\n",
      "t = 20, loss = 1.3452671766281128\n",
      "t = 21, loss = 1.3349553346633911\n",
      "t = 22, loss = 1.3331881761550903\n",
      "t = 23, loss = 1.3166348934173584\n",
      "t = 24, loss = 1.3130065202713013\n",
      "t = 25, loss = 1.3096239566802979\n",
      "t = 26, loss = 1.2907520532608032\n",
      "t = 27, loss = 1.2901866436004639\n",
      "t = 28, loss = 1.2729181051254272\n",
      "t = 29, loss = 1.2856454849243164\n",
      "t = 30, loss = 1.2609769105911255\n",
      "t = 31, loss = 1.278525710105896\n",
      "t = 32, loss = 1.2620480060577393\n",
      "t = 33, loss = 1.279552936553955\n",
      "t = 34, loss = 1.2576335668563843\n",
      "t = 35, loss = 1.2814620733261108\n",
      "t = 36, loss = 1.259401559829712\n",
      "t = 37, loss = 1.3112268447875977\n",
      "t = 38, loss = 1.3131728172302246\n",
      "t = 39, loss = 1.464671015739441\n",
      "t = 40, loss = 1.3661912679672241\n",
      "t = 41, loss = 1.5653175115585327\n",
      "t = 42, loss = 1.2704139947891235\n",
      "t = 43, loss = 1.3689121007919312\n",
      "t = 44, loss = 1.2544283866882324\n",
      "t = 45, loss = 1.3331189155578613\n",
      "t = 46, loss = 1.274454951286316\n",
      "t = 47, loss = 1.354280948638916\n",
      "t = 48, loss = 1.2502585649490356\n",
      "t = 49, loss = 1.3208507299423218\n",
      "t = 50, loss = 1.2621791362762451\n",
      "t = 51, loss = 1.3330222368240356\n",
      "t = 52, loss = 1.2585686445236206\n",
      "t = 53, loss = 1.3465945720672607\n",
      "t = 54, loss = 1.2533951997756958\n",
      "t = 55, loss = 1.330756664276123\n",
      "t = 56, loss = 1.2501236200332642\n",
      "t = 57, loss = 1.3230352401733398\n",
      "t = 58, loss = 1.2504528760910034\n",
      "t = 59, loss = 1.3321993350982666\n",
      "t = 60, loss = 1.243302583694458\n",
      "t = 61, loss = 1.3118979930877686\n",
      "t = 62, loss = 1.250523328781128\n",
      "t = 63, loss = 1.3348877429962158\n",
      "t = 64, loss = 1.2314940690994263\n",
      "t = 65, loss = 1.2999560832977295\n",
      "t = 66, loss = 1.2432852983474731\n",
      "t = 67, loss = 1.3145796060562134\n",
      "t = 68, loss = 1.2365056276321411\n",
      "t = 69, loss = 1.308257818222046\n",
      "t = 70, loss = 1.2306814193725586\n",
      "t = 71, loss = 1.2992417812347412\n",
      "t = 72, loss = 1.2352536916732788\n",
      "t = 73, loss = 1.3008090257644653\n",
      "t = 74, loss = 1.2289338111877441\n",
      "t = 75, loss = 1.296736478805542\n",
      "t = 76, loss = 1.229303002357483\n",
      "t = 77, loss = 1.3027079105377197\n",
      "t = 78, loss = 1.2229626178741455\n",
      "t = 79, loss = 1.2861778736114502\n",
      "t = 80, loss = 1.219919204711914\n",
      "t = 81, loss = 1.2869038581848145\n",
      "t = 82, loss = 1.229636788368225\n",
      "t = 83, loss = 1.294255256652832\n",
      "t = 84, loss = 1.2246719598770142\n",
      "t = 85, loss = 1.2990742921829224\n",
      "t = 86, loss = 1.2219964265823364\n",
      "t = 87, loss = 1.2936795949935913\n",
      "t = 88, loss = 1.2177684307098389\n",
      "t = 89, loss = 1.2804768085479736\n",
      "t = 90, loss = 1.2193080186843872\n",
      "t = 91, loss = 1.2856731414794922\n",
      "t = 92, loss = 1.2130550146102905\n",
      "t = 93, loss = 1.2746471166610718\n",
      "t = 94, loss = 1.22312331199646\n",
      "t = 95, loss = 1.2909008264541626\n",
      "t = 96, loss = 1.2214865684509277\n",
      "t = 97, loss = 1.2919098138809204\n",
      "t = 98, loss = 1.2254282236099243\n",
      "t = 99, loss = 1.2913570404052734\n",
      "t = 100, loss = 1.2251315116882324\n",
      "t = 101, loss = 1.1933000087738037\n",
      "t = 102, loss = 1.1753814220428467\n",
      "t = 103, loss = 1.1651506423950195\n",
      "t = 104, loss = 1.1593022346496582\n",
      "t = 105, loss = 1.1559538841247559\n",
      "t = 106, loss = 1.1539132595062256\n",
      "t = 107, loss = 1.1525616645812988\n",
      "t = 108, loss = 1.1516335010528564\n",
      "t = 109, loss = 1.1509960889816284\n",
      "t = 110, loss = 1.1505473852157593\n",
      "t = 111, loss = 1.150214672088623\n",
      "t = 112, loss = 1.1499581336975098\n",
      "t = 113, loss = 1.1497565507888794\n",
      "t = 114, loss = 1.149593710899353\n",
      "t = 115, loss = 1.1494578123092651\n",
      "t = 116, loss = 1.1493405103683472\n",
      "t = 117, loss = 1.1492356061935425\n",
      "t = 118, loss = 1.149139404296875\n",
      "t = 119, loss = 1.1490492820739746\n",
      "t = 120, loss = 1.1489635705947876\n",
      "t = 121, loss = 1.1488804817199707\n",
      "t = 122, loss = 1.1487993001937866\n",
      "t = 123, loss = 1.1487197875976562\n",
      "t = 124, loss = 1.1486412286758423\n",
      "t = 125, loss = 1.1485639810562134\n",
      "t = 126, loss = 1.1484874486923218\n",
      "t = 127, loss = 1.1484119892120361\n",
      "t = 128, loss = 1.1483371257781982\n",
      "t = 129, loss = 1.1482629776000977\n",
      "t = 130, loss = 1.1481895446777344\n",
      "t = 131, loss = 1.1481167078018188\n",
      "t = 132, loss = 1.148044466972351\n",
      "t = 133, loss = 1.147972822189331\n",
      "t = 134, loss = 1.1479015350341797\n",
      "t = 135, loss = 1.147830843925476\n",
      "t = 136, loss = 1.1477605104446411\n",
      "t = 137, loss = 1.147690773010254\n",
      "t = 138, loss = 1.1476213932037354\n",
      "t = 139, loss = 1.147552251815796\n",
      "t = 140, loss = 1.1474837064743042\n",
      "t = 141, loss = 1.1474155187606812\n",
      "t = 142, loss = 1.1473478078842163\n",
      "t = 143, loss = 1.1472803354263306\n",
      "t = 144, loss = 1.147213101387024\n",
      "t = 145, loss = 1.147146224975586\n",
      "t = 146, loss = 1.1470797061920166\n",
      "t = 147, loss = 1.1470134258270264\n",
      "t = 148, loss = 1.1469475030899048\n",
      "t = 149, loss = 1.1468820571899414\n",
      "t = 150, loss = 1.1468168497085571\n",
      "t = 151, loss = 1.1467517614364624\n",
      "t = 152, loss = 1.1466872692108154\n",
      "t = 153, loss = 1.146622896194458\n",
      "t = 154, loss = 1.1465588808059692\n",
      "t = 155, loss = 1.1464951038360596\n",
      "t = 156, loss = 1.146431565284729\n",
      "t = 157, loss = 1.1463682651519775\n",
      "t = 158, loss = 1.1463052034378052\n",
      "t = 159, loss = 1.146242380142212\n",
      "t = 160, loss = 1.1461797952651978\n",
      "t = 161, loss = 1.1461175680160522\n",
      "t = 162, loss = 1.1460553407669067\n",
      "t = 163, loss = 1.1459933519363403\n",
      "t = 164, loss = 1.145931601524353\n",
      "t = 165, loss = 1.1458699703216553\n",
      "t = 166, loss = 1.1458086967468262\n",
      "t = 167, loss = 1.145747423171997\n",
      "t = 168, loss = 1.145686388015747\n",
      "t = 169, loss = 1.1456255912780762\n",
      "t = 170, loss = 1.1455649137496948\n",
      "t = 171, loss = 1.1455045938491821\n",
      "t = 172, loss = 1.1454442739486694\n",
      "t = 173, loss = 1.1453841924667358\n",
      "t = 174, loss = 1.1453243494033813\n",
      "t = 175, loss = 1.1452643871307373\n",
      "t = 176, loss = 1.145204782485962\n",
      "t = 177, loss = 1.1451451778411865\n",
      "t = 178, loss = 1.1450859308242798\n",
      "t = 179, loss = 1.145026683807373\n",
      "t = 180, loss = 1.1449675559997559\n",
      "t = 181, loss = 1.1449084281921387\n",
      "t = 182, loss = 1.1448495388031006\n",
      "t = 183, loss = 1.144790768623352\n",
      "t = 184, loss = 1.144732117652893\n",
      "t = 185, loss = 1.1446737051010132\n",
      "t = 186, loss = 1.1446154117584229\n",
      "t = 187, loss = 1.144557237625122\n",
      "t = 188, loss = 1.1444991827011108\n",
      "t = 189, loss = 1.1444411277770996\n",
      "t = 190, loss = 1.144383430480957\n",
      "t = 191, loss = 1.144325613975525\n",
      "t = 192, loss = 1.1442679166793823\n",
      "t = 193, loss = 1.1442103385925293\n",
      "t = 194, loss = 1.1441529989242554\n",
      "t = 195, loss = 1.1440956592559814\n",
      "t = 196, loss = 1.1440383195877075\n",
      "t = 197, loss = 1.1439810991287231\n",
      "t = 198, loss = 1.1439239978790283\n",
      "t = 199, loss = 1.1438671350479126\n",
      "t = 200, loss = 1.1438101530075073\n",
      "t = 201, loss = 1.1438045501708984\n",
      "t = 202, loss = 1.143798828125\n",
      "t = 203, loss = 1.1437931060791016\n",
      "t = 204, loss = 1.1437875032424927\n",
      "t = 205, loss = 1.1437817811965942\n",
      "t = 206, loss = 1.1437760591506958\n",
      "t = 207, loss = 1.143770456314087\n",
      "t = 208, loss = 1.1437647342681885\n",
      "t = 209, loss = 1.1437591314315796\n",
      "t = 210, loss = 1.1437534093856812\n",
      "t = 211, loss = 1.1437478065490723\n",
      "t = 212, loss = 1.1437420845031738\n",
      "t = 213, loss = 1.1437363624572754\n",
      "t = 214, loss = 1.1437307596206665\n",
      "t = 215, loss = 1.143725037574768\n",
      "t = 216, loss = 1.1437194347381592\n",
      "t = 217, loss = 1.1437137126922607\n",
      "t = 218, loss = 1.1437081098556519\n",
      "t = 219, loss = 1.1437023878097534\n",
      "t = 220, loss = 1.1436967849731445\n",
      "t = 221, loss = 1.143691062927246\n",
      "t = 222, loss = 1.1436853408813477\n",
      "t = 223, loss = 1.1436797380447388\n",
      "t = 224, loss = 1.1436741352081299\n",
      "t = 225, loss = 1.1436684131622314\n",
      "t = 226, loss = 1.1436628103256226\n",
      "t = 227, loss = 1.1436572074890137\n",
      "t = 228, loss = 1.1436514854431152\n",
      "t = 229, loss = 1.1436457633972168\n",
      "t = 230, loss = 1.1436400413513184\n",
      "t = 231, loss = 1.1436344385147095\n",
      "t = 232, loss = 1.1436288356781006\n",
      "t = 233, loss = 1.1436232328414917\n",
      "t = 234, loss = 1.1436175107955933\n",
      "t = 235, loss = 1.1436119079589844\n",
      "t = 236, loss = 1.1436063051223755\n",
      "t = 237, loss = 1.143600583076477\n",
      "t = 238, loss = 1.1435949802398682\n",
      "t = 239, loss = 1.1435893774032593\n",
      "t = 240, loss = 1.1435836553573608\n",
      "t = 241, loss = 1.143578052520752\n",
      "t = 242, loss = 1.143572449684143\n",
      "t = 243, loss = 1.1435667276382446\n",
      "t = 244, loss = 1.1435610055923462\n",
      "t = 245, loss = 1.1435555219650269\n",
      "t = 246, loss = 1.1435497999191284\n",
      "t = 247, loss = 1.14354407787323\n",
      "t = 248, loss = 1.143538475036621\n",
      "t = 249, loss = 1.1435328722000122\n",
      "t = 250, loss = 1.1435272693634033\n",
      "t = 251, loss = 1.1435216665267944\n",
      "t = 252, loss = 1.143515944480896\n",
      "t = 253, loss = 1.143510341644287\n",
      "t = 254, loss = 1.1435047388076782\n",
      "t = 255, loss = 1.1434990167617798\n",
      "t = 256, loss = 1.143493413925171\n",
      "t = 257, loss = 1.143487811088562\n",
      "t = 258, loss = 1.1434822082519531\n",
      "t = 259, loss = 1.1434766054153442\n",
      "t = 260, loss = 1.1434708833694458\n",
      "t = 261, loss = 1.143465280532837\n",
      "t = 262, loss = 1.143459677696228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 263, loss = 1.1434539556503296\n",
      "t = 264, loss = 1.1434483528137207\n",
      "t = 265, loss = 1.1434427499771118\n",
      "t = 266, loss = 1.143437147140503\n",
      "t = 267, loss = 1.143431544303894\n",
      "t = 268, loss = 1.1434258222579956\n",
      "t = 269, loss = 1.1434203386306763\n",
      "t = 270, loss = 1.1434146165847778\n",
      "t = 271, loss = 1.1434091329574585\n",
      "t = 272, loss = 1.14340341091156\n",
      "t = 273, loss = 1.1433979272842407\n",
      "t = 274, loss = 1.1433922052383423\n",
      "t = 275, loss = 1.143386721611023\n",
      "t = 276, loss = 1.1433809995651245\n",
      "t = 277, loss = 1.1433753967285156\n",
      "t = 278, loss = 1.1433697938919067\n",
      "t = 279, loss = 1.1433641910552979\n",
      "t = 280, loss = 1.1433584690093994\n",
      "t = 281, loss = 1.1433528661727905\n",
      "t = 282, loss = 1.1433472633361816\n",
      "t = 283, loss = 1.1433417797088623\n",
      "t = 284, loss = 1.1433361768722534\n",
      "t = 285, loss = 1.1433305740356445\n",
      "t = 286, loss = 1.143324851989746\n",
      "t = 287, loss = 1.1433193683624268\n",
      "t = 288, loss = 1.1433136463165283\n",
      "t = 289, loss = 1.1433080434799194\n",
      "t = 290, loss = 1.1433024406433105\n",
      "t = 291, loss = 1.1432968378067017\n",
      "t = 292, loss = 1.1432912349700928\n",
      "t = 293, loss = 1.1432857513427734\n",
      "t = 294, loss = 1.1432801485061646\n",
      "t = 295, loss = 1.1432745456695557\n",
      "t = 296, loss = 1.1432688236236572\n",
      "t = 297, loss = 1.143263339996338\n",
      "t = 298, loss = 1.1432576179504395\n",
      "t = 299, loss = 1.1432521343231201\n",
      "t = 300, loss = 1.1432465314865112\n",
      "t = 301, loss = 1.1432459354400635\n",
      "t = 302, loss = 1.1432454586029053\n",
      "t = 303, loss = 1.1432448625564575\n",
      "t = 304, loss = 1.1432443857192993\n",
      "t = 305, loss = 1.1432437896728516\n",
      "t = 306, loss = 1.1432431936264038\n",
      "t = 307, loss = 1.143242597579956\n",
      "t = 308, loss = 1.1432421207427979\n",
      "t = 309, loss = 1.14324152469635\n",
      "t = 310, loss = 1.143241047859192\n",
      "t = 311, loss = 1.1432404518127441\n",
      "t = 312, loss = 1.1432397365570068\n",
      "t = 313, loss = 1.1432392597198486\n",
      "t = 314, loss = 1.1432386636734009\n",
      "t = 315, loss = 1.1432381868362427\n",
      "t = 316, loss = 1.143237590789795\n",
      "t = 317, loss = 1.1432369947433472\n",
      "t = 318, loss = 1.143236517906189\n",
      "t = 319, loss = 1.1432359218597412\n",
      "t = 320, loss = 1.1432353258132935\n",
      "t = 321, loss = 1.1432347297668457\n",
      "t = 322, loss = 1.1432342529296875\n",
      "t = 323, loss = 1.1432337760925293\n",
      "t = 324, loss = 1.1432331800460815\n",
      "t = 325, loss = 1.1432325839996338\n",
      "t = 326, loss = 1.143231987953186\n",
      "t = 327, loss = 1.1432315111160278\n",
      "t = 328, loss = 1.14323091506958\n",
      "t = 329, loss = 1.1432303190231323\n",
      "t = 330, loss = 1.1432298421859741\n",
      "t = 331, loss = 1.1432292461395264\n",
      "t = 332, loss = 1.1432287693023682\n",
      "t = 333, loss = 1.1432280540466309\n",
      "t = 334, loss = 1.1432275772094727\n",
      "t = 335, loss = 1.143226981163025\n",
      "t = 336, loss = 1.1432263851165771\n",
      "t = 337, loss = 1.143225908279419\n",
      "t = 338, loss = 1.1432251930236816\n",
      "t = 339, loss = 1.1432247161865234\n",
      "t = 340, loss = 1.1432242393493652\n",
      "t = 341, loss = 1.1432236433029175\n",
      "t = 342, loss = 1.1432230472564697\n",
      "t = 343, loss = 1.1432225704193115\n",
      "t = 344, loss = 1.1432219743728638\n",
      "t = 345, loss = 1.1432214975357056\n",
      "t = 346, loss = 1.1432207822799683\n",
      "t = 347, loss = 1.14322030544281\n",
      "t = 348, loss = 1.1432197093963623\n",
      "t = 349, loss = 1.1432191133499146\n",
      "t = 350, loss = 1.1432186365127563\n",
      "t = 351, loss = 1.1432181596755981\n",
      "t = 352, loss = 1.1432174444198608\n",
      "t = 353, loss = 1.1432169675827026\n",
      "t = 354, loss = 1.1432163715362549\n",
      "t = 355, loss = 1.1432158946990967\n",
      "t = 356, loss = 1.143215298652649\n",
      "t = 357, loss = 1.1432147026062012\n",
      "t = 358, loss = 1.143214225769043\n",
      "t = 359, loss = 1.1432135105133057\n",
      "t = 360, loss = 1.1432130336761475\n",
      "t = 361, loss = 1.1432125568389893\n",
      "t = 362, loss = 1.1432119607925415\n",
      "t = 363, loss = 1.1432113647460938\n",
      "t = 364, loss = 1.1432108879089355\n",
      "t = 365, loss = 1.1432101726531982\n",
      "t = 366, loss = 1.14320969581604\n",
      "t = 367, loss = 1.1432090997695923\n",
      "t = 368, loss = 1.143208622932434\n",
      "t = 369, loss = 1.1432080268859863\n",
      "t = 370, loss = 1.1432075500488281\n",
      "t = 371, loss = 1.1432069540023804\n",
      "t = 372, loss = 1.1432063579559326\n",
      "t = 373, loss = 1.1432057619094849\n",
      "t = 374, loss = 1.143205165863037\n",
      "t = 375, loss = 1.143204689025879\n",
      "t = 376, loss = 1.1432040929794312\n",
      "t = 377, loss = 1.143203616142273\n",
      "t = 378, loss = 1.1432029008865356\n",
      "t = 379, loss = 1.1432024240493774\n",
      "t = 380, loss = 1.1432018280029297\n",
      "t = 381, loss = 1.1432013511657715\n",
      "t = 382, loss = 1.1432007551193237\n",
      "t = 383, loss = 1.1432002782821655\n",
      "t = 384, loss = 1.1431996822357178\n",
      "t = 385, loss = 1.14319908618927\n",
      "t = 386, loss = 1.1431984901428223\n",
      "t = 387, loss = 1.1431978940963745\n",
      "t = 388, loss = 1.1431974172592163\n",
      "t = 389, loss = 1.1431968212127686\n",
      "t = 390, loss = 1.1431963443756104\n",
      "t = 391, loss = 1.1431957483291626\n",
      "t = 392, loss = 1.1431951522827148\n",
      "t = 393, loss = 1.1431946754455566\n",
      "t = 394, loss = 1.1431940793991089\n",
      "t = 395, loss = 1.1431936025619507\n",
      "t = 396, loss = 1.143193006515503\n",
      "t = 397, loss = 1.1431924104690552\n",
      "t = 398, loss = 1.143191933631897\n",
      "t = 399, loss = 1.1431912183761597\n",
      "t = 400, loss = 1.1431907415390015\n",
      "t = 401, loss = 1.1431907415390015\n",
      "t = 402, loss = 1.143190622329712\n",
      "t = 403, loss = 1.143190622329712\n",
      "t = 404, loss = 1.1431905031204224\n",
      "t = 405, loss = 1.1431905031204224\n",
      "t = 406, loss = 1.1431905031204224\n",
      "t = 407, loss = 1.1431903839111328\n",
      "t = 408, loss = 1.1431902647018433\n",
      "t = 409, loss = 1.1431902647018433\n",
      "t = 410, loss = 1.1431901454925537\n",
      "t = 411, loss = 1.1431901454925537\n",
      "t = 412, loss = 1.1431901454925537\n",
      "t = 413, loss = 1.1431900262832642\n",
      "t = 414, loss = 1.1431900262832642\n",
      "t = 415, loss = 1.1431900262832642\n",
      "t = 416, loss = 1.1431899070739746\n",
      "t = 417, loss = 1.143189787864685\n",
      "t = 418, loss = 1.143189787864685\n",
      "t = 419, loss = 1.143189787864685\n",
      "t = 420, loss = 1.1431896686553955\n",
      "t = 421, loss = 1.1431896686553955\n",
      "t = 422, loss = 1.143189549446106\n",
      "t = 423, loss = 1.143189549446106\n",
      "t = 424, loss = 1.143189549446106\n",
      "t = 425, loss = 1.1431894302368164\n",
      "t = 426, loss = 1.1431893110275269\n",
      "t = 427, loss = 1.1431893110275269\n",
      "t = 428, loss = 1.1431893110275269\n",
      "t = 429, loss = 1.1431891918182373\n",
      "t = 430, loss = 1.1431891918182373\n",
      "t = 431, loss = 1.1431890726089478\n",
      "t = 432, loss = 1.1431890726089478\n",
      "t = 433, loss = 1.1431890726089478\n",
      "t = 434, loss = 1.1431889533996582\n",
      "t = 435, loss = 1.1431888341903687\n",
      "t = 436, loss = 1.1431888341903687\n",
      "t = 437, loss = 1.1431888341903687\n",
      "t = 438, loss = 1.143188714981079\n",
      "t = 439, loss = 1.1431885957717896\n",
      "t = 440, loss = 1.1431885957717896\n",
      "t = 441, loss = 1.1431885957717896\n",
      "t = 442, loss = 1.1431884765625\n",
      "t = 443, loss = 1.1431884765625\n",
      "t = 444, loss = 1.1431883573532104\n",
      "t = 445, loss = 1.1431883573532104\n",
      "t = 446, loss = 1.143188238143921\n",
      "t = 447, loss = 1.143188238143921\n",
      "t = 448, loss = 1.1431881189346313\n",
      "t = 449, loss = 1.1431881189346313\n",
      "t = 450, loss = 1.1431881189346313\n",
      "t = 451, loss = 1.1431879997253418\n",
      "t = 452, loss = 1.1431878805160522\n",
      "t = 453, loss = 1.1431878805160522\n",
      "t = 454, loss = 1.1431878805160522\n",
      "t = 455, loss = 1.1431877613067627\n",
      "t = 456, loss = 1.1431877613067627\n",
      "t = 457, loss = 1.1431876420974731\n",
      "t = 458, loss = 1.1431876420974731\n",
      "t = 459, loss = 1.1431876420974731\n",
      "t = 460, loss = 1.1431875228881836\n",
      "t = 461, loss = 1.143187403678894\n",
      "t = 462, loss = 1.143187403678894\n",
      "t = 463, loss = 1.143187403678894\n",
      "t = 464, loss = 1.1431872844696045\n",
      "t = 465, loss = 1.143187165260315\n",
      "t = 466, loss = 1.143187165260315\n",
      "t = 467, loss = 1.143187165260315\n",
      "t = 468, loss = 1.143187165260315\n",
      "t = 469, loss = 1.1431870460510254\n",
      "t = 470, loss = 1.1431869268417358\n",
      "t = 471, loss = 1.1431869268417358\n",
      "t = 472, loss = 1.1431868076324463\n",
      "t = 473, loss = 1.1431868076324463\n",
      "t = 474, loss = 1.1431868076324463\n",
      "t = 475, loss = 1.1431866884231567\n",
      "t = 476, loss = 1.1431866884231567\n",
      "t = 477, loss = 1.1431866884231567\n",
      "t = 478, loss = 1.1431865692138672\n",
      "t = 479, loss = 1.1431864500045776\n",
      "t = 480, loss = 1.1431864500045776\n",
      "t = 481, loss = 1.1431864500045776\n",
      "t = 482, loss = 1.143186330795288\n",
      "t = 483, loss = 1.143186330795288\n",
      "t = 484, loss = 1.143186330795288\n",
      "t = 485, loss = 1.1431862115859985\n",
      "t = 486, loss = 1.1431862115859985\n",
      "t = 487, loss = 1.143186092376709\n",
      "t = 488, loss = 1.143186092376709\n",
      "t = 489, loss = 1.143186092376709\n",
      "t = 490, loss = 1.1431858539581299\n",
      "t = 491, loss = 1.1431858539581299\n",
      "t = 492, loss = 1.1431858539581299\n",
      "t = 493, loss = 1.1431858539581299\n",
      "t = 494, loss = 1.1431857347488403\n",
      "t = 495, loss = 1.1431857347488403\n",
      "t = 496, loss = 1.1431856155395508\n",
      "t = 497, loss = 1.1431856155395508\n",
      "t = 498, loss = 1.1431856155395508\n",
      "t = 499, loss = 1.1431856155395508\n",
      "Training R2:  0.43815950580591473\n",
      "Test R2:  0.43776949664893167\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_feat = len(features_cols)\n",
    "out_feat = len(targetColumns)\n",
    "model = Simple_Net(in_features=in_feat ,out_features=out_feat).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=100, gamma=0.1)\n",
    "\n",
    "losses,model = train(model, optimiser, scheduler, criterion, epochs = 500)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "    y_pred_train = model(X_train)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "\n",
    "train_r2 = r2_score(y_train.detach().cpu().numpy(), y_pred_train.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "test_r2=r2_score(y_test.detach().cpu().numpy(), y_pred_test.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "\n",
    "print(\"Training R2: \",train_r2)\n",
    "print(\"Test R2: \",test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
