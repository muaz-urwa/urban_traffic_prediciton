{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/urwa/Documents/side_projects/urban/data/featureData/com_jfk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, scheduler, criterion,epochs = 500):\n",
    "    losses = []\n",
    "    # Main optimization loop\n",
    "    for t in range(epochs):\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        y_predicted = model(X_train)\n",
    "\n",
    "        current_loss = criterion(y_predicted, y_train)\n",
    "\n",
    "        current_loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        print(f\"t = {t}, loss = {current_loss}\")\n",
    "\n",
    "        losses.append(current_loss)\n",
    "\n",
    "        scheduler.step()    \n",
    "    return losses,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8757, 113)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>1.0</th>\n",
       "      <th>1.1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>1.3</th>\n",
       "      <th>2.0</th>\n",
       "      <th>...</th>\n",
       "      <th>4.1_lag_3</th>\n",
       "      <th>4.2_lag_3</th>\n",
       "      <th>4.3_lag_3</th>\n",
       "      <th>4.4_lag_3</th>\n",
       "      <th>4.5_lag_3</th>\n",
       "      <th>5.0_lag_3</th>\n",
       "      <th>5.1_lag_3</th>\n",
       "      <th>5.2_lag_3</th>\n",
       "      <th>5.3_lag_3</th>\n",
       "      <th>arrival_lag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Hour  0.0  0.1  0.2  1.0  1.1  1.2  1.3  2.0  ...  4.1_lag_3  \\\n",
       "0  2018-01-01     3    2    0    0    5    1    2    4    1  ...       15.0   \n",
       "1  2018-01-01     4   11    3    4    7    0    4    1    2  ...       11.0   \n",
       "2  2018-01-01     5   51    6   28   27   36   23   34    2  ...        3.0   \n",
       "\n",
       "   4.2_lag_3  4.3_lag_3  4.4_lag_3  4.5_lag_3  5.0_lag_3  5.1_lag_3  \\\n",
       "0       47.0       20.0        7.0       21.0        1.0        0.0   \n",
       "1       41.0        2.0        2.0        5.0        2.0        0.0   \n",
       "2       15.0        1.0        3.0        2.0        0.0        0.0   \n",
       "\n",
       "   5.2_lag_3  5.3_lag_3  arrival_lag_3  \n",
       "0        0.0        0.0            6.0  \n",
       "1        0.0        0.0            6.0  \n",
       "2        0.0        0.0            2.0  \n",
       "\n",
       "[3 rows x 113 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Net(nn.Module):\n",
    "    def __init__(self, in_features,out_features):\n",
    "        super(Linear_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=out_features, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Net(nn.Module):\n",
    "    def __init__(self, in_features,out_features):\n",
    "        super(Simple_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=1000, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=1000, out_features=500, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=500, out_features=out_features, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = self.fc3(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag_columns = [c for c in dataset.columns if 'lag' in c]\n",
    "len(lag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DateColumns = ['Date']\n",
    "\n",
    "ext_columns = ['Dow', 'arrival','maxtemp', 'mintemp', 'avgtemp', 'departure', 'hdd',\n",
    "       'cdd', 'participation', 'newsnow', 'snowdepth', 'ifSnow']\n",
    "\n",
    "targetColumns = [c for c in dataset.columns if c not in ext_columns and \\\n",
    "                c not in DateColumns and c not in lag_columns and c != 'Hour']\n",
    "len(targetColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_cols = [c for c in dataset.columns if c not in targetColumns and c not in DateColumns]\n",
    "len(features_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset[features_cols].values\n",
    "y = dataset[targetColumns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_x = StandardScaler()\n",
    "# scaler_y = StandardScaler()\n",
    "\n",
    "# scaler_x.fit(x)\n",
    "# scaler_y.fit(y)\n",
    "\n",
    "# x = scaler_x.transform(x)\n",
    "# y = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8757, 88])\n",
      "torch.Size([8757, 24])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(x).float().to(device)\n",
    "print(x.shape)\n",
    "y = torch.tensor(y).float().to(device)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 32.632076263427734\n",
      "t = 1, loss = 361.24200439453125\n",
      "t = 2, loss = 146.27928161621094\n",
      "t = 3, loss = 361.4561462402344\n",
      "t = 4, loss = 146.11961364746094\n",
      "t = 5, loss = 361.6450500488281\n",
      "t = 6, loss = 145.97999572753906\n",
      "t = 7, loss = 361.7682189941406\n",
      "t = 8, loss = 145.8929901123047\n",
      "t = 9, loss = 361.8885803222656\n",
      "t = 10, loss = 145.8054962158203\n",
      "t = 11, loss = 361.9612121582031\n",
      "t = 12, loss = 145.7490692138672\n",
      "t = 13, loss = 362.0513916015625\n",
      "t = 14, loss = 145.66201782226562\n",
      "t = 15, loss = 362.1164855957031\n",
      "t = 16, loss = 145.6094512939453\n",
      "t = 17, loss = 362.2076416015625\n",
      "t = 18, loss = 145.5324249267578\n",
      "t = 19, loss = 362.2710266113281\n",
      "t = 20, loss = 145.482177734375\n",
      "t = 21, loss = 362.3204345703125\n",
      "t = 22, loss = 145.44215393066406\n",
      "t = 23, loss = 362.3868103027344\n",
      "t = 24, loss = 145.38040161132812\n",
      "t = 25, loss = 362.4272766113281\n",
      "t = 26, loss = 145.34938049316406\n",
      "t = 27, loss = 362.46759033203125\n",
      "t = 28, loss = 145.3118438720703\n",
      "t = 29, loss = 362.50372314453125\n",
      "t = 30, loss = 145.27902221679688\n",
      "t = 31, loss = 362.54547119140625\n",
      "t = 32, loss = 145.2388458251953\n",
      "t = 33, loss = 362.58160400390625\n",
      "t = 34, loss = 145.20445251464844\n",
      "t = 35, loss = 362.62445068359375\n",
      "t = 36, loss = 145.16746520996094\n",
      "t = 37, loss = 362.6658935546875\n",
      "t = 38, loss = 145.1311492919922\n",
      "t = 39, loss = 362.7092590332031\n",
      "t = 40, loss = 145.09500122070312\n",
      "t = 41, loss = 362.7454833984375\n",
      "t = 42, loss = 145.06036376953125\n",
      "t = 43, loss = 362.7795715332031\n",
      "t = 44, loss = 145.02781677246094\n",
      "t = 45, loss = 362.8169860839844\n",
      "t = 46, loss = 144.9923553466797\n",
      "t = 47, loss = 362.8539123535156\n",
      "t = 48, loss = 144.95802307128906\n",
      "t = 49, loss = 362.87982177734375\n",
      "t = 50, loss = 144.93533325195312\n",
      "t = 51, loss = 362.9194641113281\n",
      "t = 52, loss = 144.89816284179688\n",
      "t = 53, loss = 362.9531555175781\n",
      "t = 54, loss = 144.86558532714844\n",
      "t = 55, loss = 362.9852600097656\n",
      "t = 56, loss = 144.83824157714844\n",
      "t = 57, loss = 363.0048828125\n",
      "t = 58, loss = 144.81961059570312\n",
      "t = 59, loss = 363.03045654296875\n",
      "t = 60, loss = 144.79837036132812\n",
      "t = 61, loss = 363.05218505859375\n",
      "t = 62, loss = 144.77932739257812\n",
      "t = 63, loss = 363.07342529296875\n",
      "t = 64, loss = 144.7588348388672\n",
      "t = 65, loss = 363.0935363769531\n",
      "t = 66, loss = 144.74021911621094\n",
      "t = 67, loss = 363.1141662597656\n",
      "t = 68, loss = 144.72100830078125\n",
      "t = 69, loss = 363.13525390625\n",
      "t = 70, loss = 144.70111083984375\n",
      "t = 71, loss = 363.1534729003906\n",
      "t = 72, loss = 144.6825714111328\n",
      "t = 73, loss = 363.17449951171875\n",
      "t = 74, loss = 144.66415405273438\n",
      "t = 75, loss = 363.1926574707031\n",
      "t = 76, loss = 144.6463623046875\n",
      "t = 77, loss = 363.2102966308594\n",
      "t = 78, loss = 144.62953186035156\n",
      "t = 79, loss = 363.2317810058594\n",
      "t = 80, loss = 144.60888671875\n",
      "t = 81, loss = 363.252197265625\n",
      "t = 82, loss = 144.5908660888672\n",
      "t = 83, loss = 363.2699279785156\n",
      "t = 84, loss = 144.5762176513672\n",
      "t = 85, loss = 363.2870788574219\n",
      "t = 86, loss = 144.56072998046875\n",
      "t = 87, loss = 363.3009948730469\n",
      "t = 88, loss = 144.5465850830078\n",
      "t = 89, loss = 363.3149108886719\n",
      "t = 90, loss = 144.53688049316406\n",
      "t = 91, loss = 363.32440185546875\n",
      "t = 92, loss = 144.52716064453125\n",
      "t = 93, loss = 363.3338317871094\n",
      "t = 94, loss = 144.51744079589844\n",
      "t = 95, loss = 363.3491516113281\n",
      "t = 96, loss = 144.5050811767578\n",
      "t = 97, loss = 363.3613586425781\n",
      "t = 98, loss = 144.49436950683594\n",
      "t = 99, loss = 363.3741149902344\n",
      "t = 100, loss = 144.48138427734375\n",
      "t = 101, loss = 103.04727172851562\n",
      "t = 102, loss = 85.4256591796875\n",
      "t = 103, loss = 65.70402526855469\n",
      "t = 104, loss = 53.295902252197266\n",
      "t = 105, loss = 39.06120681762695\n",
      "t = 106, loss = 31.289152145385742\n",
      "t = 107, loss = 24.06744956970215\n",
      "t = 108, loss = 25.395626068115234\n",
      "t = 109, loss = 24.081750869750977\n",
      "t = 110, loss = 25.597558975219727\n",
      "t = 111, loss = 24.52326202392578\n",
      "t = 112, loss = 25.91293716430664\n",
      "t = 113, loss = 24.685304641723633\n",
      "t = 114, loss = 25.79104232788086\n",
      "t = 115, loss = 24.681499481201172\n",
      "t = 116, loss = 25.745203018188477\n",
      "t = 117, loss = 24.68022918701172\n",
      "t = 118, loss = 25.709754943847656\n",
      "t = 119, loss = 24.658742904663086\n",
      "t = 120, loss = 25.678892135620117\n",
      "t = 121, loss = 24.62698745727539\n",
      "t = 122, loss = 25.660490036010742\n",
      "t = 123, loss = 24.587278366088867\n",
      "t = 124, loss = 25.640174865722656\n",
      "t = 125, loss = 24.58279037475586\n",
      "t = 126, loss = 25.635059356689453\n",
      "t = 127, loss = 24.559255599975586\n",
      "t = 128, loss = 25.61834716796875\n",
      "t = 129, loss = 24.540721893310547\n",
      "t = 130, loss = 25.60114097595215\n",
      "t = 131, loss = 24.52770233154297\n",
      "t = 132, loss = 25.596223831176758\n",
      "t = 133, loss = 24.52057456970215\n",
      "t = 134, loss = 25.59039306640625\n",
      "t = 135, loss = 24.513671875\n",
      "t = 136, loss = 25.588518142700195\n",
      "t = 137, loss = 24.50890350341797\n",
      "t = 138, loss = 25.573835372924805\n",
      "t = 139, loss = 24.502887725830078\n",
      "t = 140, loss = 25.567163467407227\n",
      "t = 141, loss = 24.501115798950195\n",
      "t = 142, loss = 25.55292320251465\n",
      "t = 143, loss = 24.49527931213379\n",
      "t = 144, loss = 25.566551208496094\n",
      "t = 145, loss = 24.4849796295166\n",
      "t = 146, loss = 25.56829833984375\n",
      "t = 147, loss = 24.4802303314209\n",
      "t = 148, loss = 25.571012496948242\n",
      "t = 149, loss = 24.474937438964844\n",
      "t = 150, loss = 25.568801879882812\n",
      "t = 151, loss = 24.471755981445312\n",
      "t = 152, loss = 25.571741104125977\n",
      "t = 153, loss = 24.469871520996094\n",
      "t = 154, loss = 25.57602882385254\n",
      "t = 155, loss = 24.460987091064453\n",
      "t = 156, loss = 25.57516860961914\n",
      "t = 157, loss = 24.449787139892578\n",
      "t = 158, loss = 25.5859317779541\n",
      "t = 159, loss = 24.44416618347168\n",
      "t = 160, loss = 25.599048614501953\n",
      "t = 161, loss = 24.437847137451172\n",
      "t = 162, loss = 25.599855422973633\n",
      "t = 163, loss = 24.434904098510742\n",
      "t = 164, loss = 25.603561401367188\n",
      "t = 165, loss = 24.430225372314453\n",
      "t = 166, loss = 25.610206604003906\n",
      "t = 167, loss = 24.425809860229492\n",
      "t = 168, loss = 25.61656379699707\n",
      "t = 169, loss = 24.423776626586914\n",
      "t = 170, loss = 25.61731719970703\n",
      "t = 171, loss = 24.421602249145508\n",
      "t = 172, loss = 25.620689392089844\n",
      "t = 173, loss = 24.41705894470215\n",
      "t = 174, loss = 25.618892669677734\n",
      "t = 175, loss = 24.416919708251953\n",
      "t = 176, loss = 25.62171745300293\n",
      "t = 177, loss = 24.416866302490234\n",
      "t = 178, loss = 25.625635147094727\n",
      "t = 179, loss = 24.405881881713867\n",
      "t = 180, loss = 25.625688552856445\n",
      "t = 181, loss = 24.40734100341797\n",
      "t = 182, loss = 25.632160186767578\n",
      "t = 183, loss = 24.398555755615234\n",
      "t = 184, loss = 25.633480072021484\n",
      "t = 185, loss = 24.393943786621094\n",
      "t = 186, loss = 25.638277053833008\n",
      "t = 187, loss = 24.386201858520508\n",
      "t = 188, loss = 25.63614845275879\n",
      "t = 189, loss = 24.379817962646484\n",
      "t = 190, loss = 25.64371681213379\n",
      "t = 191, loss = 24.37466049194336\n",
      "t = 192, loss = 25.646339416503906\n",
      "t = 193, loss = 24.37090492248535\n",
      "t = 194, loss = 25.6491641998291\n",
      "t = 195, loss = 24.366294860839844\n",
      "t = 196, loss = 25.65482521057129\n",
      "t = 197, loss = 24.36229705810547\n",
      "t = 198, loss = 25.6610107421875\n",
      "t = 199, loss = 24.353649139404297\n",
      "t = 200, loss = 25.66501235961914\n",
      "t = 201, loss = 21.410308837890625\n",
      "t = 202, loss = 17.45871925354004\n",
      "t = 203, loss = 14.00268840789795\n",
      "t = 204, loss = 11.298307418823242\n",
      "t = 205, loss = 9.580071449279785\n",
      "t = 206, loss = 8.837459564208984\n",
      "t = 207, loss = 8.601308822631836\n",
      "t = 208, loss = 8.517754554748535\n",
      "t = 209, loss = 8.47062873840332\n",
      "t = 210, loss = 8.436609268188477\n",
      "t = 211, loss = 8.408429145812988\n",
      "t = 212, loss = 8.383560180664062\n",
      "t = 213, loss = 8.360621452331543\n",
      "t = 214, loss = 8.338885307312012\n",
      "t = 215, loss = 8.318055152893066\n",
      "t = 216, loss = 8.298050880432129\n",
      "t = 217, loss = 8.278766632080078\n",
      "t = 218, loss = 8.260215759277344\n",
      "t = 219, loss = 8.242355346679688\n",
      "t = 220, loss = 8.225164413452148\n",
      "t = 221, loss = 8.20859432220459\n",
      "t = 222, loss = 8.192621231079102\n",
      "t = 223, loss = 8.177244186401367\n",
      "t = 224, loss = 8.162440299987793\n",
      "t = 225, loss = 8.148232460021973\n",
      "t = 226, loss = 8.13467025756836\n",
      "t = 227, loss = 8.121660232543945\n",
      "t = 228, loss = 8.109160423278809\n",
      "t = 229, loss = 8.097139358520508\n",
      "t = 230, loss = 8.085591316223145\n",
      "t = 231, loss = 8.074545860290527\n",
      "t = 232, loss = 8.063973426818848\n",
      "t = 233, loss = 8.053851127624512\n",
      "t = 234, loss = 8.0441255569458\n",
      "t = 235, loss = 8.034791946411133\n",
      "t = 236, loss = 8.025835037231445\n",
      "t = 237, loss = 8.017207145690918\n",
      "t = 238, loss = 8.00893497467041\n",
      "t = 239, loss = 8.000954627990723\n",
      "t = 240, loss = 7.993249416351318\n",
      "t = 241, loss = 7.985803604125977\n",
      "t = 242, loss = 7.978604793548584\n",
      "t = 243, loss = 7.971622467041016\n",
      "t = 244, loss = 7.964845180511475\n",
      "t = 245, loss = 7.958273887634277\n",
      "t = 246, loss = 7.951900482177734\n",
      "t = 247, loss = 7.9456987380981445\n",
      "t = 248, loss = 7.939680576324463\n",
      "t = 249, loss = 7.933847904205322\n",
      "t = 250, loss = 7.928171157836914\n",
      "t = 251, loss = 7.922618389129639\n",
      "t = 252, loss = 7.91720724105835\n",
      "t = 253, loss = 7.9119181632995605\n",
      "t = 254, loss = 7.906753063201904\n",
      "t = 255, loss = 7.901704788208008\n",
      "t = 256, loss = 7.896775722503662\n",
      "t = 257, loss = 7.891950607299805\n",
      "t = 258, loss = 7.8872222900390625\n",
      "t = 259, loss = 7.882569789886475\n",
      "t = 260, loss = 7.877991199493408\n",
      "t = 261, loss = 7.873495101928711\n",
      "t = 262, loss = 7.86908483505249\n",
      "t = 263, loss = 7.864751815795898\n",
      "t = 264, loss = 7.860481262207031\n",
      "t = 265, loss = 7.856276035308838\n",
      "t = 266, loss = 7.8521409034729\n",
      "t = 267, loss = 7.848062038421631\n",
      "t = 268, loss = 7.8440399169921875\n",
      "t = 269, loss = 7.840081691741943\n",
      "t = 270, loss = 7.836180210113525\n",
      "t = 271, loss = 7.832334041595459\n",
      "t = 272, loss = 7.828547477722168\n",
      "t = 273, loss = 7.824813365936279\n",
      "t = 274, loss = 7.82112979888916\n",
      "t = 275, loss = 7.817488193511963\n",
      "t = 276, loss = 7.81388521194458\n",
      "t = 277, loss = 7.810328006744385\n",
      "t = 278, loss = 7.806812286376953\n",
      "t = 279, loss = 7.803341865539551\n",
      "t = 280, loss = 7.79990816116333\n",
      "t = 281, loss = 7.796510696411133\n",
      "t = 282, loss = 7.793150424957275\n",
      "t = 283, loss = 7.789829254150391\n",
      "t = 284, loss = 7.7865424156188965\n",
      "t = 285, loss = 7.783293724060059\n",
      "t = 286, loss = 7.780086040496826\n",
      "t = 287, loss = 7.776912689208984\n",
      "t = 288, loss = 7.773775577545166\n",
      "t = 289, loss = 7.770670413970947\n",
      "t = 290, loss = 7.7675957679748535\n",
      "t = 291, loss = 7.764547348022461\n",
      "t = 292, loss = 7.761524677276611\n",
      "t = 293, loss = 7.758537769317627\n",
      "t = 294, loss = 7.7555832862854\n",
      "t = 295, loss = 7.752654075622559\n",
      "t = 296, loss = 7.749753475189209\n",
      "t = 297, loss = 7.746883869171143\n",
      "t = 298, loss = 7.74404239654541\n",
      "t = 299, loss = 7.741227626800537\n",
      "t = 300, loss = 7.73844051361084\n",
      "t = 301, loss = 7.738162517547607\n",
      "t = 302, loss = 7.737884044647217\n",
      "t = 303, loss = 7.737607002258301\n",
      "t = 304, loss = 7.737329959869385\n",
      "t = 305, loss = 7.737052917480469\n",
      "t = 306, loss = 7.736777305603027\n",
      "t = 307, loss = 7.736501216888428\n",
      "t = 308, loss = 7.7362260818481445\n",
      "t = 309, loss = 7.735950469970703\n",
      "t = 310, loss = 7.73567533493042\n",
      "t = 311, loss = 7.7353997230529785\n",
      "t = 312, loss = 7.735125541687012\n",
      "t = 313, loss = 7.734850883483887\n",
      "t = 314, loss = 7.734576225280762\n",
      "t = 315, loss = 7.734302043914795\n",
      "t = 316, loss = 7.734028339385986\n",
      "t = 317, loss = 7.733754634857178\n",
      "t = 318, loss = 7.733480930328369\n",
      "t = 319, loss = 7.733208179473877\n",
      "t = 320, loss = 7.732935428619385\n",
      "t = 321, loss = 7.732663631439209\n",
      "t = 322, loss = 7.732391834259033\n",
      "t = 323, loss = 7.732119560241699\n",
      "t = 324, loss = 7.73184871673584\n",
      "t = 325, loss = 7.731577396392822\n",
      "t = 326, loss = 7.731306552886963\n",
      "t = 327, loss = 7.731036186218262\n",
      "t = 328, loss = 7.730765342712402\n",
      "t = 329, loss = 7.730495929718018\n",
      "t = 330, loss = 7.730225563049316\n",
      "t = 331, loss = 7.729956150054932\n",
      "t = 332, loss = 7.729686737060547\n",
      "t = 333, loss = 7.7294182777404785\n",
      "t = 334, loss = 7.72914981842041\n",
      "t = 335, loss = 7.728881359100342\n",
      "t = 336, loss = 7.728612899780273\n",
      "t = 337, loss = 7.728344917297363\n",
      "t = 338, loss = 7.728077411651611\n",
      "t = 339, loss = 7.727808952331543\n",
      "t = 340, loss = 7.727541923522949\n",
      "t = 341, loss = 7.727274417877197\n",
      "t = 342, loss = 7.7270073890686035\n",
      "t = 343, loss = 7.726739883422852\n",
      "t = 344, loss = 7.726472854614258\n",
      "t = 345, loss = 7.726206302642822\n",
      "t = 346, loss = 7.725940227508545\n",
      "t = 347, loss = 7.725673198699951\n",
      "t = 348, loss = 7.725407123565674\n",
      "t = 349, loss = 7.725140571594238\n",
      "t = 350, loss = 7.724875450134277\n",
      "t = 351, loss = 7.724609375\n",
      "t = 352, loss = 7.724344253540039\n",
      "t = 353, loss = 7.724078178405762\n",
      "t = 354, loss = 7.723813056945801\n",
      "t = 355, loss = 7.723548889160156\n",
      "t = 356, loss = 7.723283767700195\n",
      "t = 357, loss = 7.723019599914551\n",
      "t = 358, loss = 7.722754955291748\n",
      "t = 359, loss = 7.72249174118042\n",
      "t = 360, loss = 7.722227573394775\n",
      "t = 361, loss = 7.721963882446289\n",
      "t = 362, loss = 7.721700668334961\n",
      "t = 363, loss = 7.721437931060791\n",
      "t = 364, loss = 7.721174716949463\n",
      "t = 365, loss = 7.720911979675293\n",
      "t = 366, loss = 7.720649242401123\n",
      "t = 367, loss = 7.720386981964111\n",
      "t = 368, loss = 7.720125198364258\n",
      "t = 369, loss = 7.719863414764404\n",
      "t = 370, loss = 7.719600677490234\n",
      "t = 371, loss = 7.719339847564697\n",
      "t = 372, loss = 7.719078063964844\n",
      "t = 373, loss = 7.718817234039307\n",
      "t = 374, loss = 7.7185564041137695\n",
      "t = 375, loss = 7.718294620513916\n",
      "t = 376, loss = 7.7180352210998535\n",
      "t = 377, loss = 7.717774391174316\n",
      "t = 378, loss = 7.717514514923096\n",
      "t = 379, loss = 7.717254638671875\n",
      "t = 380, loss = 7.716994285583496\n",
      "t = 381, loss = 7.716734409332275\n",
      "t = 382, loss = 7.716475009918213\n",
      "t = 383, loss = 7.716216087341309\n",
      "t = 384, loss = 7.715956687927246\n",
      "t = 385, loss = 7.715698719024658\n",
      "t = 386, loss = 7.715439319610596\n",
      "t = 387, loss = 7.71518087387085\n",
      "t = 388, loss = 7.71492338180542\n",
      "t = 389, loss = 7.714664936065674\n",
      "t = 390, loss = 7.714406967163086\n",
      "t = 391, loss = 7.7141499519348145\n",
      "t = 392, loss = 7.713891983032227\n",
      "t = 393, loss = 7.713634967803955\n",
      "t = 394, loss = 7.713377952575684\n",
      "t = 395, loss = 7.713120937347412\n",
      "t = 396, loss = 7.712865352630615\n",
      "t = 397, loss = 7.712608814239502\n",
      "t = 398, loss = 7.712352752685547\n",
      "t = 399, loss = 7.712096214294434\n",
      "t = 400, loss = 7.711841106414795\n",
      "t = 401, loss = 7.71181583404541\n",
      "t = 402, loss = 7.711790084838867\n",
      "t = 403, loss = 7.711764812469482\n",
      "t = 404, loss = 7.711739540100098\n",
      "t = 405, loss = 7.711713790893555\n",
      "t = 406, loss = 7.71168851852417\n",
      "t = 407, loss = 7.711662769317627\n",
      "t = 408, loss = 7.711637496948242\n",
      "t = 409, loss = 7.711611747741699\n",
      "t = 410, loss = 7.7115864753723145\n",
      "t = 411, loss = 7.7115607261657715\n",
      "t = 412, loss = 7.711535453796387\n",
      "t = 413, loss = 7.711509704589844\n",
      "t = 414, loss = 7.711484432220459\n",
      "t = 415, loss = 7.711458683013916\n",
      "t = 416, loss = 7.711433410644531\n",
      "t = 417, loss = 7.7114081382751465\n",
      "t = 418, loss = 7.7113823890686035\n",
      "t = 419, loss = 7.711357116699219\n",
      "t = 420, loss = 7.711331367492676\n",
      "t = 421, loss = 7.711306095123291\n",
      "t = 422, loss = 7.7112812995910645\n",
      "t = 423, loss = 7.711255073547363\n",
      "t = 424, loss = 7.711230278015137\n",
      "t = 425, loss = 7.7112040519714355\n",
      "t = 426, loss = 7.711179256439209\n",
      "t = 427, loss = 7.711153984069824\n",
      "t = 428, loss = 7.711128234863281\n",
      "t = 429, loss = 7.7111029624938965\n",
      "t = 430, loss = 7.711076736450195\n",
      "t = 431, loss = 7.711051940917969\n",
      "t = 432, loss = 7.711026191711426\n",
      "t = 433, loss = 7.711000919342041\n",
      "t = 434, loss = 7.710975170135498\n",
      "t = 435, loss = 7.710949897766113\n",
      "t = 436, loss = 7.7109246253967285\n",
      "t = 437, loss = 7.7108988761901855\n",
      "t = 438, loss = 7.710873603820801\n",
      "t = 439, loss = 7.710848808288574\n",
      "t = 440, loss = 7.710822582244873\n",
      "t = 441, loss = 7.7107977867126465\n",
      "t = 442, loss = 7.710772514343262\n",
      "t = 443, loss = 7.710746765136719\n",
      "t = 444, loss = 7.710721492767334\n",
      "t = 445, loss = 7.710695743560791\n",
      "t = 446, loss = 7.710670471191406\n",
      "t = 447, loss = 7.71064567565918\n",
      "t = 448, loss = 7.710620403289795\n",
      "t = 449, loss = 7.710594654083252\n",
      "t = 450, loss = 7.710569381713867\n",
      "t = 451, loss = 7.710544586181641\n",
      "t = 452, loss = 7.7105183601379395\n",
      "t = 453, loss = 7.7104926109313965\n",
      "t = 454, loss = 7.710467338562012\n",
      "t = 455, loss = 7.710442543029785\n",
      "t = 456, loss = 7.710416316986084\n",
      "t = 457, loss = 7.710391521453857\n",
      "t = 458, loss = 7.710366249084473\n",
      "t = 459, loss = 7.71034049987793\n",
      "t = 460, loss = 7.710315227508545\n",
      "t = 461, loss = 7.710289478302002\n",
      "t = 462, loss = 7.710265159606934\n",
      "t = 463, loss = 7.710239410400391\n",
      "t = 464, loss = 7.710214614868164\n",
      "t = 465, loss = 7.710188388824463\n",
      "t = 466, loss = 7.710163116455078\n",
      "t = 467, loss = 7.710138320922852\n",
      "t = 468, loss = 7.710113048553467\n",
      "t = 469, loss = 7.710087299346924\n",
      "t = 470, loss = 7.710062026977539\n",
      "t = 471, loss = 7.710036277770996\n",
      "t = 472, loss = 7.7100114822387695\n",
      "t = 473, loss = 7.709985256195068\n",
      "t = 474, loss = 7.7099609375\n",
      "t = 475, loss = 7.709936141967773\n",
      "t = 476, loss = 7.709909915924072\n",
      "t = 477, loss = 7.709885120391846\n",
      "t = 478, loss = 7.7098588943481445\n",
      "t = 479, loss = 7.709834098815918\n",
      "t = 480, loss = 7.709808826446533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 481, loss = 7.70978307723999\n",
      "t = 482, loss = 7.7097578048706055\n",
      "t = 483, loss = 7.7097320556640625\n",
      "t = 484, loss = 7.709706783294678\n",
      "t = 485, loss = 7.709681987762451\n",
      "t = 486, loss = 7.709657192230225\n",
      "t = 487, loss = 7.709630966186523\n",
      "t = 488, loss = 7.709606170654297\n",
      "t = 489, loss = 7.709580898284912\n",
      "t = 490, loss = 7.709555149078369\n",
      "t = 491, loss = 7.709529876708984\n",
      "t = 492, loss = 7.7095046043396\n",
      "t = 493, loss = 7.709478855133057\n",
      "t = 494, loss = 7.70945405960083\n",
      "t = 495, loss = 7.709428787231445\n",
      "t = 496, loss = 7.709403038024902\n",
      "t = 497, loss = 7.709378719329834\n",
      "t = 498, loss = 7.709352493286133\n",
      "t = 499, loss = 7.709327697753906\n",
      "Training R2:  0.6797266831894033\n",
      "Test R2:  0.678432117401737\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_feat = len(features_cols)\n",
    "out_feat = len(targetColumns)\n",
    "model = Linear_Net(in_features=in_feat ,out_features=out_feat).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=100, gamma=0.1)\n",
    "\n",
    "losses,model = train(model, optimiser, scheduler, criterion, epochs = 500)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "    y_pred_train = model(X_train)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "\n",
    "train_r2 = r2_score(y_train.detach().cpu().numpy(), y_pred_train.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "test_r2=r2_score(y_test.detach().cpu().numpy(), y_pred_test.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "\n",
    "print(\"Training R2: \",train_r2)\n",
    "print(\"Test R2: \",test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 24.843494415283203\n",
      "t = 1, loss = 381.8148498535156\n",
      "t = 2, loss = 24.66884994506836\n",
      "t = 3, loss = 206.57867431640625\n",
      "t = 4, loss = 66.3736343383789\n",
      "t = 5, loss = 200.25140380859375\n",
      "t = 6, loss = 660034.5625\n",
      "t = 7, loss = 104.52891540527344\n",
      "t = 8, loss = 672189.875\n",
      "t = 9, loss = 82854674432.0\n",
      "t = 10, loss = 15987012608.0\n",
      "t = 11, loss = 77677551616.0\n",
      "t = 12, loss = 5.068299865864274e+17\n",
      "t = 13, loss = 3156991551733760.0\n",
      "t = 14, loss = 504930861056.0\n",
      "t = 15, loss = 80374.484375\n",
      "t = 16, loss = 23.107608795166016\n",
      "t = 17, loss = 24.5872802734375\n",
      "t = 18, loss = 24.583728790283203\n",
      "t = 19, loss = 24.58017921447754\n",
      "t = 20, loss = 24.57662582397461\n",
      "t = 21, loss = 24.57308006286621\n",
      "t = 22, loss = 24.56960105895996\n",
      "t = 23, loss = 24.566333770751953\n",
      "t = 24, loss = 24.56305503845215\n",
      "t = 25, loss = 24.559906005859375\n",
      "t = 26, loss = 24.556928634643555\n",
      "t = 27, loss = 24.5538272857666\n",
      "t = 28, loss = 24.551000595092773\n",
      "t = 29, loss = 24.547887802124023\n",
      "t = 30, loss = 24.545085906982422\n",
      "t = 31, loss = 24.54218292236328\n",
      "t = 32, loss = 24.539379119873047\n",
      "t = 33, loss = 24.53636360168457\n",
      "t = 34, loss = 24.533681869506836\n",
      "t = 35, loss = 24.530710220336914\n",
      "t = 36, loss = 24.52787208557129\n",
      "t = 37, loss = 24.524940490722656\n",
      "t = 38, loss = 24.522220611572266\n",
      "t = 39, loss = 24.519224166870117\n",
      "t = 40, loss = 24.516407012939453\n",
      "t = 41, loss = 24.51358985900879\n",
      "t = 42, loss = 24.510738372802734\n",
      "t = 43, loss = 24.507720947265625\n",
      "t = 44, loss = 24.505006790161133\n",
      "t = 45, loss = 24.502090454101562\n",
      "t = 46, loss = 24.499242782592773\n",
      "t = 47, loss = 24.496301651000977\n",
      "t = 48, loss = 24.493562698364258\n",
      "t = 49, loss = 24.490558624267578\n",
      "t = 50, loss = 24.487733840942383\n",
      "t = 51, loss = 24.4848575592041\n",
      "t = 52, loss = 24.482053756713867\n",
      "t = 53, loss = 24.47905921936035\n",
      "t = 54, loss = 24.476274490356445\n",
      "t = 55, loss = 24.473337173461914\n",
      "t = 56, loss = 24.470470428466797\n",
      "t = 57, loss = 24.467605590820312\n",
      "t = 58, loss = 24.46474838256836\n",
      "t = 59, loss = 24.461807250976562\n",
      "t = 60, loss = 24.4589900970459\n",
      "t = 61, loss = 24.456012725830078\n",
      "t = 62, loss = 24.453170776367188\n",
      "t = 63, loss = 24.450281143188477\n",
      "t = 64, loss = 24.447437286376953\n",
      "t = 65, loss = 24.444435119628906\n",
      "t = 66, loss = 24.441621780395508\n",
      "t = 67, loss = 24.438705444335938\n",
      "t = 68, loss = 24.43577766418457\n",
      "t = 69, loss = 24.43285369873047\n",
      "t = 70, loss = 24.430072784423828\n",
      "t = 71, loss = 24.42706871032715\n",
      "t = 72, loss = 24.42412757873535\n",
      "t = 73, loss = 24.421293258666992\n",
      "t = 74, loss = 24.418426513671875\n",
      "t = 75, loss = 24.415447235107422\n",
      "t = 76, loss = 24.412567138671875\n",
      "t = 77, loss = 24.409677505493164\n",
      "t = 78, loss = 24.406726837158203\n",
      "t = 79, loss = 24.40374755859375\n",
      "t = 80, loss = 24.400959014892578\n",
      "t = 81, loss = 24.397991180419922\n",
      "t = 82, loss = 24.395008087158203\n",
      "t = 83, loss = 24.39208984375\n",
      "t = 84, loss = 24.389245986938477\n",
      "t = 85, loss = 24.38620376586914\n",
      "t = 86, loss = 24.383258819580078\n",
      "t = 87, loss = 24.3803768157959\n",
      "t = 88, loss = 24.377399444580078\n",
      "t = 89, loss = 24.374418258666992\n",
      "t = 90, loss = 24.371501922607422\n",
      "t = 91, loss = 24.368532180786133\n",
      "t = 92, loss = 24.365497589111328\n",
      "t = 93, loss = 24.362571716308594\n",
      "t = 94, loss = 24.359630584716797\n",
      "t = 95, loss = 24.35662078857422\n",
      "t = 96, loss = 24.353628158569336\n",
      "t = 97, loss = 24.35059356689453\n",
      "t = 98, loss = 24.347606658935547\n",
      "t = 99, loss = 24.34461784362793\n",
      "t = 100, loss = 24.34165382385254\n",
      "t = 101, loss = 24.341327667236328\n",
      "t = 102, loss = 24.340999603271484\n",
      "t = 103, loss = 24.34067153930664\n",
      "t = 104, loss = 24.34034538269043\n",
      "t = 105, loss = 24.340015411376953\n",
      "t = 106, loss = 24.339717864990234\n",
      "t = 107, loss = 24.339397430419922\n",
      "t = 108, loss = 24.339107513427734\n",
      "t = 109, loss = 24.33878517150879\n",
      "t = 110, loss = 24.33848762512207\n",
      "t = 111, loss = 24.338165283203125\n",
      "t = 112, loss = 24.33787727355957\n",
      "t = 113, loss = 24.337553024291992\n",
      "t = 114, loss = 24.337255477905273\n",
      "t = 115, loss = 24.336936950683594\n",
      "t = 116, loss = 24.336645126342773\n",
      "t = 117, loss = 24.336320877075195\n",
      "t = 118, loss = 24.336021423339844\n",
      "t = 119, loss = 24.335708618164062\n",
      "t = 120, loss = 24.335411071777344\n",
      "t = 121, loss = 24.335084915161133\n",
      "t = 122, loss = 24.334800720214844\n",
      "t = 123, loss = 24.33448600769043\n",
      "t = 124, loss = 24.33418846130371\n",
      "t = 125, loss = 24.3338680267334\n",
      "t = 126, loss = 24.33358383178711\n",
      "t = 127, loss = 24.333261489868164\n",
      "t = 128, loss = 24.332963943481445\n",
      "t = 129, loss = 24.332651138305664\n",
      "t = 130, loss = 24.33236312866211\n",
      "t = 131, loss = 24.3320369720459\n",
      "t = 132, loss = 24.33174705505371\n",
      "t = 133, loss = 24.331430435180664\n",
      "t = 134, loss = 24.331138610839844\n",
      "t = 135, loss = 24.330814361572266\n",
      "t = 136, loss = 24.330524444580078\n",
      "t = 137, loss = 24.330211639404297\n",
      "t = 138, loss = 24.32991600036621\n",
      "t = 139, loss = 24.329601287841797\n",
      "t = 140, loss = 24.32930564880371\n",
      "t = 141, loss = 24.328983306884766\n",
      "t = 142, loss = 24.32870101928711\n",
      "t = 143, loss = 24.32837677001953\n",
      "t = 144, loss = 24.328086853027344\n",
      "t = 145, loss = 24.327768325805664\n",
      "t = 146, loss = 24.327478408813477\n",
      "t = 147, loss = 24.3271541595459\n",
      "t = 148, loss = 24.326868057250977\n",
      "t = 149, loss = 24.32655143737793\n",
      "t = 150, loss = 24.32625389099121\n",
      "t = 151, loss = 24.3259334564209\n",
      "t = 152, loss = 24.325647354125977\n",
      "t = 153, loss = 24.325328826904297\n",
      "t = 154, loss = 24.32503318786621\n",
      "t = 155, loss = 24.324710845947266\n",
      "t = 156, loss = 24.324424743652344\n",
      "t = 157, loss = 24.3241024017334\n",
      "t = 158, loss = 24.323810577392578\n",
      "t = 159, loss = 24.323497772216797\n",
      "t = 160, loss = 24.32320213317871\n",
      "t = 161, loss = 24.3228702545166\n",
      "t = 162, loss = 24.322587966918945\n",
      "t = 163, loss = 24.32227325439453\n",
      "t = 164, loss = 24.321979522705078\n",
      "t = 165, loss = 24.3216495513916\n",
      "t = 166, loss = 24.32137107849121\n",
      "t = 167, loss = 24.3210391998291\n",
      "t = 168, loss = 24.320743560791016\n",
      "t = 169, loss = 24.3204288482666\n",
      "t = 170, loss = 24.32014274597168\n",
      "t = 171, loss = 24.31981086730957\n",
      "t = 172, loss = 24.31951904296875\n",
      "t = 173, loss = 24.319202423095703\n",
      "t = 174, loss = 24.31890869140625\n",
      "t = 175, loss = 24.318580627441406\n",
      "t = 176, loss = 24.31829833984375\n",
      "t = 177, loss = 24.317974090576172\n",
      "t = 178, loss = 24.31767463684082\n",
      "t = 179, loss = 24.31735610961914\n",
      "t = 180, loss = 24.317066192626953\n",
      "t = 181, loss = 24.31673812866211\n",
      "t = 182, loss = 24.31644630432129\n",
      "t = 183, loss = 24.31612205505371\n",
      "t = 184, loss = 24.315837860107422\n",
      "t = 185, loss = 24.315505981445312\n",
      "t = 186, loss = 24.31521987915039\n",
      "t = 187, loss = 24.314889907836914\n",
      "t = 188, loss = 24.314599990844727\n",
      "t = 189, loss = 24.31427574157715\n",
      "t = 190, loss = 24.313983917236328\n",
      "t = 191, loss = 24.313657760620117\n",
      "t = 192, loss = 24.313364028930664\n",
      "t = 193, loss = 24.313039779663086\n",
      "t = 194, loss = 24.312744140625\n",
      "t = 195, loss = 24.31242561340332\n",
      "t = 196, loss = 24.312135696411133\n",
      "t = 197, loss = 24.311803817749023\n",
      "t = 198, loss = 24.311508178710938\n",
      "t = 199, loss = 24.31118392944336\n",
      "t = 200, loss = 24.310897827148438\n",
      "t = 201, loss = 24.31085968017578\n",
      "t = 202, loss = 24.310829162597656\n",
      "t = 203, loss = 24.310794830322266\n",
      "t = 204, loss = 24.310760498046875\n",
      "t = 205, loss = 24.310726165771484\n",
      "t = 206, loss = 24.31069564819336\n",
      "t = 207, loss = 24.31066131591797\n",
      "t = 208, loss = 24.310626983642578\n",
      "t = 209, loss = 24.310592651367188\n",
      "t = 210, loss = 24.310562133789062\n",
      "t = 211, loss = 24.310529708862305\n",
      "t = 212, loss = 24.310501098632812\n",
      "t = 213, loss = 24.310466766357422\n",
      "t = 214, loss = 24.310436248779297\n",
      "t = 215, loss = 24.310405731201172\n",
      "t = 216, loss = 24.310375213623047\n",
      "t = 217, loss = 24.310340881347656\n",
      "t = 218, loss = 24.31031036376953\n",
      "t = 219, loss = 24.310279846191406\n",
      "t = 220, loss = 24.31024932861328\n",
      "t = 221, loss = 24.310218811035156\n",
      "t = 222, loss = 24.31018829345703\n",
      "t = 223, loss = 24.31015396118164\n",
      "t = 224, loss = 24.31012535095215\n",
      "t = 225, loss = 24.31009292602539\n",
      "t = 226, loss = 24.310062408447266\n",
      "t = 227, loss = 24.31003189086914\n",
      "t = 228, loss = 24.310001373291016\n",
      "t = 229, loss = 24.309967041015625\n",
      "t = 230, loss = 24.309938430786133\n",
      "t = 231, loss = 24.309906005859375\n",
      "t = 232, loss = 24.30987548828125\n",
      "t = 233, loss = 24.309844970703125\n",
      "t = 234, loss = 24.309814453125\n",
      "t = 235, loss = 24.309782028198242\n",
      "t = 236, loss = 24.309749603271484\n",
      "t = 237, loss = 24.309722900390625\n",
      "t = 238, loss = 24.309690475463867\n",
      "t = 239, loss = 24.30965805053711\n",
      "t = 240, loss = 24.309627532958984\n",
      "t = 241, loss = 24.30959701538086\n",
      "t = 242, loss = 24.309566497802734\n",
      "t = 243, loss = 24.309534072875977\n",
      "t = 244, loss = 24.309505462646484\n",
      "t = 245, loss = 24.309473037719727\n",
      "t = 246, loss = 24.3094425201416\n",
      "t = 247, loss = 24.309412002563477\n",
      "t = 248, loss = 24.30938148498535\n",
      "t = 249, loss = 24.309349060058594\n",
      "t = 250, loss = 24.30931854248047\n",
      "t = 251, loss = 24.309288024902344\n",
      "t = 252, loss = 24.30925750732422\n",
      "t = 253, loss = 24.309223175048828\n",
      "t = 254, loss = 24.309194564819336\n",
      "t = 255, loss = 24.309162139892578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 256, loss = 24.309131622314453\n",
      "t = 257, loss = 24.309101104736328\n",
      "t = 258, loss = 24.309070587158203\n",
      "t = 259, loss = 24.309040069580078\n",
      "t = 260, loss = 24.309009552001953\n",
      "t = 261, loss = 24.308975219726562\n",
      "t = 262, loss = 24.308948516845703\n",
      "t = 263, loss = 24.308914184570312\n",
      "t = 264, loss = 24.30888557434082\n",
      "t = 265, loss = 24.308853149414062\n",
      "t = 266, loss = 24.308822631835938\n",
      "t = 267, loss = 24.308792114257812\n",
      "t = 268, loss = 24.308761596679688\n",
      "t = 269, loss = 24.308727264404297\n",
      "t = 270, loss = 24.308700561523438\n",
      "t = 271, loss = 24.308666229248047\n",
      "t = 272, loss = 24.308635711669922\n",
      "t = 273, loss = 24.308605194091797\n",
      "t = 274, loss = 24.308576583862305\n",
      "t = 275, loss = 24.308542251586914\n",
      "t = 276, loss = 24.308513641357422\n",
      "t = 277, loss = 24.308481216430664\n",
      "t = 278, loss = 24.308452606201172\n",
      "t = 279, loss = 24.30841827392578\n",
      "t = 280, loss = 24.308387756347656\n",
      "t = 281, loss = 24.30835723876953\n",
      "t = 282, loss = 24.308326721191406\n",
      "t = 283, loss = 24.30829429626465\n",
      "t = 284, loss = 24.308265686035156\n",
      "t = 285, loss = 24.3082332611084\n",
      "t = 286, loss = 24.30820083618164\n",
      "t = 287, loss = 24.308170318603516\n",
      "t = 288, loss = 24.308141708374023\n",
      "t = 289, loss = 24.308109283447266\n",
      "t = 290, loss = 24.30807876586914\n",
      "t = 291, loss = 24.308048248291016\n",
      "t = 292, loss = 24.308015823364258\n",
      "t = 293, loss = 24.307985305786133\n",
      "t = 294, loss = 24.307952880859375\n",
      "t = 295, loss = 24.30792236328125\n",
      "t = 296, loss = 24.307891845703125\n",
      "t = 297, loss = 24.307859420776367\n",
      "t = 298, loss = 24.307828903198242\n",
      "t = 299, loss = 24.307798385620117\n",
      "t = 300, loss = 24.30776596069336\n",
      "t = 301, loss = 24.30776596069336\n",
      "t = 302, loss = 24.307758331298828\n",
      "t = 303, loss = 24.307756423950195\n",
      "t = 304, loss = 24.30775260925293\n",
      "t = 305, loss = 24.307750701904297\n",
      "t = 306, loss = 24.307748794555664\n",
      "t = 307, loss = 24.307743072509766\n",
      "t = 308, loss = 24.3077392578125\n",
      "t = 309, loss = 24.307737350463867\n",
      "t = 310, loss = 24.307735443115234\n",
      "t = 311, loss = 24.30773162841797\n",
      "t = 312, loss = 24.307727813720703\n",
      "t = 313, loss = 24.307723999023438\n",
      "t = 314, loss = 24.307720184326172\n",
      "t = 315, loss = 24.307720184326172\n",
      "t = 316, loss = 24.307714462280273\n",
      "t = 317, loss = 24.30771255493164\n",
      "t = 318, loss = 24.307708740234375\n",
      "t = 319, loss = 24.30770492553711\n",
      "t = 320, loss = 24.30770492553711\n",
      "t = 321, loss = 24.307701110839844\n",
      "t = 322, loss = 24.307697296142578\n",
      "t = 323, loss = 24.307693481445312\n",
      "t = 324, loss = 24.307689666748047\n",
      "t = 325, loss = 24.307687759399414\n",
      "t = 326, loss = 24.30768585205078\n",
      "t = 327, loss = 24.307680130004883\n",
      "t = 328, loss = 24.30767822265625\n",
      "t = 329, loss = 24.307674407958984\n",
      "t = 330, loss = 24.30767250061035\n",
      "t = 331, loss = 24.30767059326172\n",
      "t = 332, loss = 24.307666778564453\n",
      "t = 333, loss = 24.307661056518555\n",
      "t = 334, loss = 24.307659149169922\n",
      "t = 335, loss = 24.30765724182129\n",
      "t = 336, loss = 24.307653427124023\n",
      "t = 337, loss = 24.307649612426758\n",
      "t = 338, loss = 24.307647705078125\n",
      "t = 339, loss = 24.30764389038086\n",
      "t = 340, loss = 24.307641983032227\n",
      "t = 341, loss = 24.307640075683594\n",
      "t = 342, loss = 24.307634353637695\n",
      "t = 343, loss = 24.30763053894043\n",
      "t = 344, loss = 24.307628631591797\n",
      "t = 345, loss = 24.307626724243164\n",
      "t = 346, loss = 24.3076229095459\n",
      "t = 347, loss = 24.307619094848633\n",
      "t = 348, loss = 24.3076171875\n",
      "t = 349, loss = 24.307613372802734\n",
      "t = 350, loss = 24.3076114654541\n",
      "t = 351, loss = 24.307605743408203\n",
      "t = 352, loss = 24.30760383605957\n",
      "t = 353, loss = 24.307600021362305\n",
      "t = 354, loss = 24.30759620666504\n",
      "t = 355, loss = 24.307594299316406\n",
      "t = 356, loss = 24.307592391967773\n",
      "t = 357, loss = 24.307588577270508\n",
      "t = 358, loss = 24.30758285522461\n",
      "t = 359, loss = 24.30758285522461\n",
      "t = 360, loss = 24.307579040527344\n",
      "t = 361, loss = 24.307575225830078\n",
      "t = 362, loss = 24.307571411132812\n",
      "t = 363, loss = 24.307567596435547\n",
      "t = 364, loss = 24.307565689086914\n",
      "t = 365, loss = 24.30756378173828\n",
      "t = 366, loss = 24.30756187438965\n",
      "t = 367, loss = 24.30755615234375\n",
      "t = 368, loss = 24.307552337646484\n",
      "t = 369, loss = 24.30755043029785\n",
      "t = 370, loss = 24.30754852294922\n",
      "t = 371, loss = 24.307544708251953\n",
      "t = 372, loss = 24.307540893554688\n",
      "t = 373, loss = 24.307537078857422\n",
      "t = 374, loss = 24.307533264160156\n",
      "t = 375, loss = 24.307533264160156\n",
      "t = 376, loss = 24.30752944946289\n",
      "t = 377, loss = 24.307525634765625\n",
      "t = 378, loss = 24.30752182006836\n",
      "t = 379, loss = 24.307519912719727\n",
      "t = 380, loss = 24.30751609802246\n",
      "t = 381, loss = 24.307514190673828\n",
      "t = 382, loss = 24.307510375976562\n",
      "t = 383, loss = 24.307506561279297\n",
      "t = 384, loss = 24.30750274658203\n",
      "t = 385, loss = 24.3075008392334\n",
      "t = 386, loss = 24.307497024536133\n",
      "t = 387, loss = 24.3074951171875\n",
      "t = 388, loss = 24.307491302490234\n",
      "t = 389, loss = 24.30748748779297\n",
      "t = 390, loss = 24.307485580444336\n",
      "t = 391, loss = 24.307483673095703\n",
      "t = 392, loss = 24.307479858398438\n",
      "t = 393, loss = 24.307476043701172\n",
      "t = 394, loss = 24.30747413635254\n",
      "t = 395, loss = 24.307470321655273\n",
      "t = 396, loss = 24.307466506958008\n",
      "t = 397, loss = 24.307464599609375\n",
      "t = 398, loss = 24.30746078491211\n",
      "t = 399, loss = 24.307456970214844\n",
      "t = 400, loss = 24.30745506286621\n",
      "t = 401, loss = 24.30745506286621\n",
      "t = 402, loss = 24.30745506286621\n",
      "t = 403, loss = 24.30745506286621\n",
      "t = 404, loss = 24.30745506286621\n",
      "t = 405, loss = 24.30745506286621\n",
      "t = 406, loss = 24.307453155517578\n",
      "t = 407, loss = 24.307453155517578\n",
      "t = 408, loss = 24.307453155517578\n",
      "t = 409, loss = 24.307449340820312\n",
      "t = 410, loss = 24.307449340820312\n",
      "t = 411, loss = 24.307449340820312\n",
      "t = 412, loss = 24.307449340820312\n",
      "t = 413, loss = 24.307449340820312\n",
      "t = 414, loss = 24.307449340820312\n",
      "t = 415, loss = 24.307449340820312\n",
      "t = 416, loss = 24.307449340820312\n",
      "t = 417, loss = 24.307449340820312\n",
      "t = 418, loss = 24.307449340820312\n",
      "t = 419, loss = 24.307449340820312\n",
      "t = 420, loss = 24.307449340820312\n",
      "t = 421, loss = 24.307449340820312\n",
      "t = 422, loss = 24.307445526123047\n",
      "t = 423, loss = 24.307445526123047\n",
      "t = 424, loss = 24.307445526123047\n",
      "t = 425, loss = 24.307445526123047\n",
      "t = 426, loss = 24.307445526123047\n",
      "t = 427, loss = 24.307445526123047\n",
      "t = 428, loss = 24.307445526123047\n",
      "t = 429, loss = 24.307445526123047\n",
      "t = 430, loss = 24.307445526123047\n",
      "t = 431, loss = 24.307445526123047\n",
      "t = 432, loss = 24.307445526123047\n",
      "t = 433, loss = 24.307445526123047\n",
      "t = 434, loss = 24.307443618774414\n",
      "t = 435, loss = 24.307443618774414\n",
      "t = 436, loss = 24.30744171142578\n",
      "t = 437, loss = 24.30744171142578\n",
      "t = 438, loss = 24.30744171142578\n",
      "t = 439, loss = 24.30744171142578\n",
      "t = 440, loss = 24.30744171142578\n",
      "t = 441, loss = 24.30744171142578\n",
      "t = 442, loss = 24.30744171142578\n",
      "t = 443, loss = 24.30744171142578\n",
      "t = 444, loss = 24.30743980407715\n",
      "t = 445, loss = 24.30743980407715\n",
      "t = 446, loss = 24.30743980407715\n",
      "t = 447, loss = 24.30743980407715\n",
      "t = 448, loss = 24.30743980407715\n",
      "t = 449, loss = 24.30743980407715\n",
      "t = 450, loss = 24.30743980407715\n",
      "t = 451, loss = 24.30743980407715\n",
      "t = 452, loss = 24.30743980407715\n",
      "t = 453, loss = 24.30743980407715\n",
      "t = 454, loss = 24.30743980407715\n",
      "t = 455, loss = 24.30743980407715\n",
      "t = 456, loss = 24.30743980407715\n",
      "t = 457, loss = 24.30743980407715\n",
      "t = 458, loss = 24.307435989379883\n",
      "t = 459, loss = 24.307435989379883\n",
      "t = 460, loss = 24.307435989379883\n",
      "t = 461, loss = 24.307435989379883\n",
      "t = 462, loss = 24.307435989379883\n",
      "t = 463, loss = 24.307435989379883\n",
      "t = 464, loss = 24.307435989379883\n",
      "t = 465, loss = 24.307435989379883\n",
      "t = 466, loss = 24.307435989379883\n",
      "t = 467, loss = 24.307435989379883\n",
      "t = 468, loss = 24.30743408203125\n",
      "t = 469, loss = 24.30743408203125\n",
      "t = 470, loss = 24.30743408203125\n",
      "t = 471, loss = 24.307432174682617\n",
      "t = 472, loss = 24.307430267333984\n",
      "t = 473, loss = 24.307430267333984\n",
      "t = 474, loss = 24.307430267333984\n",
      "t = 475, loss = 24.307430267333984\n",
      "t = 476, loss = 24.307430267333984\n",
      "t = 477, loss = 24.307430267333984\n",
      "t = 478, loss = 24.307430267333984\n",
      "t = 479, loss = 24.307430267333984\n",
      "t = 480, loss = 24.307430267333984\n",
      "t = 481, loss = 24.30742835998535\n",
      "t = 482, loss = 24.30742835998535\n",
      "t = 483, loss = 24.30742835998535\n",
      "t = 484, loss = 24.30742645263672\n",
      "t = 485, loss = 24.30742645263672\n",
      "t = 486, loss = 24.30742645263672\n",
      "t = 487, loss = 24.30742645263672\n",
      "t = 488, loss = 24.30742645263672\n",
      "t = 489, loss = 24.30742645263672\n",
      "t = 490, loss = 24.30742645263672\n",
      "t = 491, loss = 24.30742645263672\n",
      "t = 492, loss = 24.30742645263672\n",
      "t = 493, loss = 24.30742645263672\n",
      "t = 494, loss = 24.30742645263672\n",
      "t = 495, loss = 24.307424545288086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 496, loss = 24.307424545288086\n",
      "t = 497, loss = 24.307424545288086\n",
      "t = 498, loss = 24.307422637939453\n",
      "t = 499, loss = 24.307422637939453\n",
      "Training R2:  -2.6549522600067688\n",
      "Test R2:  -462.92573581986795\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_feat = len(features_cols)\n",
    "out_feat = len(targetColumns)\n",
    "model = Simple_Net(in_features=in_feat ,out_features=out_feat).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=100, gamma=0.1)\n",
    "\n",
    "losses,model = train(model, optimiser, scheduler, criterion, epochs = 500)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "    y_pred_train = model(X_train)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "\n",
    "train_r2 = r2_score(y_train.detach().cpu().numpy(), y_pred_train.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "test_r2=r2_score(y_test.detach().cpu().numpy(), y_pred_test.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "\n",
    "print(\"Training R2: \",train_r2)\n",
    "print(\"Test R2: \",test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
