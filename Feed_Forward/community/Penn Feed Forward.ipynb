{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/urwa/Documents/side_projects/urban/data/featureData/penn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, scheduler, criterion,epochs = 500):\n",
    "    losses = []\n",
    "    # Main optimization loop\n",
    "    for t in range(epochs):\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        y_predicted = model(X_train)\n",
    "\n",
    "        current_loss = criterion(y_predicted, y_train)\n",
    "\n",
    "        current_loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        print(f\"t = {t}, loss = {current_loss}\")\n",
    "\n",
    "        losses.append(current_loss)\n",
    "\n",
    "        scheduler.step()    \n",
    "    return losses,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8757, 1045)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>...</th>\n",
       "      <th>91_lag_3</th>\n",
       "      <th>92_lag_3</th>\n",
       "      <th>93_lag_3</th>\n",
       "      <th>94_lag_3</th>\n",
       "      <th>95_lag_3</th>\n",
       "      <th>96_lag_3</th>\n",
       "      <th>97_lag_3</th>\n",
       "      <th>98_lag_3</th>\n",
       "      <th>99_lag_3</th>\n",
       "      <th>arrival_lag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 1045 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Hour  1  10  100  101  102  106  107  108  ...  91_lag_3  \\\n",
       "0  2018-01-01     3  5   0    3    0    0    0   26    0  ...       1.0   \n",
       "1  2018-01-01     4  3   0    7    0    0    0    8    0  ...       0.0   \n",
       "2  2018-01-01     5  6   0    1    1    0    2    1    0  ...       0.0   \n",
       "\n",
       "   92_lag_3  93_lag_3  94_lag_3  95_lag_3  96_lag_3  97_lag_3  98_lag_3  \\\n",
       "0       0.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "1       0.0       1.0       0.0       3.0       0.0       2.0       0.0   \n",
       "2       0.0       0.0       0.0       1.0       0.0       1.0       1.0   \n",
       "\n",
       "   99_lag_3  arrival_lag_3  \n",
       "0       0.0            0.0  \n",
       "1       0.0            1.0  \n",
       "2       0.0            1.0  \n",
       "\n",
       "[3 rows x 1045 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Net(nn.Module):\n",
    "    def __init__(self, in_features,out_features):\n",
    "        super(Linear_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=out_features, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Net(nn.Module):\n",
    "    def __init__(self, in_features,out_features):\n",
    "        super(Simple_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=1000, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=1000, out_features=500, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=500, out_features=out_features, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = self.fc3(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag_columns = [c for c in dataset.columns if 'lag' in c]\n",
    "len(lag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DateColumns = ['Date']\n",
    "\n",
    "ext_columns = ['Dow', 'arrival','maxtemp', 'mintemp', 'avgtemp', 'departure', 'hdd',\n",
    "       'cdd', 'participation', 'newsnow', 'snowdepth', 'ifSnow']\n",
    "\n",
    "targetColumns = [c for c in dataset.columns if c not in ext_columns and \\\n",
    "                c not in DateColumns and c not in lag_columns and c != 'Hour']\n",
    "len(targetColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "787"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_cols = [c for c in dataset.columns if c not in targetColumns and c not in DateColumns]\n",
    "len(features_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset[features_cols].values\n",
    "y = dataset[targetColumns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_x = StandardScaler()\n",
    "# scaler_y = StandardScaler()\n",
    "\n",
    "# scaler_x.fit(x)\n",
    "# scaler_y.fit(y)\n",
    "\n",
    "# x = scaler_x.transform(x)\n",
    "# y = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8757, 787])\n",
      "torch.Size([8757, 257])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(x).float().to(device)\n",
    "print(x.shape)\n",
    "y = torch.tensor(y).float().to(device)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 5.113589763641357\n",
      "t = 1, loss = 6.196808338165283\n",
      "t = 2, loss = 5.886536598205566\n",
      "t = 3, loss = 6.481637001037598\n",
      "t = 4, loss = 5.989182472229004\n",
      "t = 5, loss = 6.538891792297363\n",
      "t = 6, loss = 6.008185863494873\n",
      "t = 7, loss = 6.518050670623779\n",
      "t = 8, loss = 6.0126729011535645\n",
      "t = 9, loss = 6.5072245597839355\n",
      "t = 10, loss = 6.020452499389648\n",
      "t = 11, loss = 6.504340171813965\n",
      "t = 12, loss = 6.030454158782959\n",
      "t = 13, loss = 6.499886512756348\n",
      "t = 14, loss = 6.034731864929199\n",
      "t = 15, loss = 6.4941887855529785\n",
      "t = 16, loss = 6.036081790924072\n",
      "t = 17, loss = 6.486876964569092\n",
      "t = 18, loss = 6.042609214782715\n",
      "t = 19, loss = 6.484394073486328\n",
      "t = 20, loss = 6.049984455108643\n",
      "t = 21, loss = 6.483949661254883\n",
      "t = 22, loss = 6.054821968078613\n",
      "t = 23, loss = 6.479799270629883\n",
      "t = 24, loss = 6.055909633636475\n",
      "t = 25, loss = 6.473097324371338\n",
      "t = 26, loss = 6.058071613311768\n",
      "t = 27, loss = 6.468724727630615\n",
      "t = 28, loss = 6.06025505065918\n",
      "t = 29, loss = 6.463531017303467\n",
      "t = 30, loss = 6.06248664855957\n",
      "t = 31, loss = 6.458299160003662\n",
      "t = 32, loss = 6.065258979797363\n",
      "t = 33, loss = 6.453888416290283\n",
      "t = 34, loss = 6.068238258361816\n",
      "t = 35, loss = 6.449916362762451\n",
      "t = 36, loss = 6.071938514709473\n",
      "t = 37, loss = 6.445581912994385\n",
      "t = 38, loss = 6.074821949005127\n",
      "t = 39, loss = 6.4420270919799805\n",
      "t = 40, loss = 6.077967643737793\n",
      "t = 41, loss = 6.438843727111816\n",
      "t = 42, loss = 6.081272125244141\n",
      "t = 43, loss = 6.436078071594238\n",
      "t = 44, loss = 6.084050178527832\n",
      "t = 45, loss = 6.432229042053223\n",
      "t = 46, loss = 6.086853981018066\n",
      "t = 47, loss = 6.429081439971924\n",
      "t = 48, loss = 6.089710712432861\n",
      "t = 49, loss = 6.426244258880615\n",
      "t = 50, loss = 6.092440128326416\n",
      "t = 51, loss = 6.4230499267578125\n",
      "t = 52, loss = 6.094980716705322\n",
      "t = 53, loss = 6.4203996658325195\n",
      "t = 54, loss = 6.097314834594727\n",
      "t = 55, loss = 6.417422294616699\n",
      "t = 56, loss = 6.100015640258789\n",
      "t = 57, loss = 6.41432523727417\n",
      "t = 58, loss = 6.102486610412598\n",
      "t = 59, loss = 6.41179895401001\n",
      "t = 60, loss = 6.104836940765381\n",
      "t = 61, loss = 6.409276962280273\n",
      "t = 62, loss = 6.107118606567383\n",
      "t = 63, loss = 6.406803131103516\n",
      "t = 64, loss = 6.109231472015381\n",
      "t = 65, loss = 6.4043474197387695\n",
      "t = 66, loss = 6.111417770385742\n",
      "t = 67, loss = 6.402007102966309\n",
      "t = 68, loss = 6.113487243652344\n",
      "t = 69, loss = 6.399920463562012\n",
      "t = 70, loss = 6.115760326385498\n",
      "t = 71, loss = 6.398096561431885\n",
      "t = 72, loss = 6.117581367492676\n",
      "t = 73, loss = 6.395936012268066\n",
      "t = 74, loss = 6.119597434997559\n",
      "t = 75, loss = 6.393575191497803\n",
      "t = 76, loss = 6.12184476852417\n",
      "t = 77, loss = 6.391331672668457\n",
      "t = 78, loss = 6.124033451080322\n",
      "t = 79, loss = 6.38959264755249\n",
      "t = 80, loss = 6.126645565032959\n",
      "t = 81, loss = 6.3879828453063965\n",
      "t = 82, loss = 6.129167079925537\n",
      "t = 83, loss = 6.385979652404785\n",
      "t = 84, loss = 6.131978511810303\n",
      "t = 85, loss = 6.38482666015625\n",
      "t = 86, loss = 6.1345696449279785\n",
      "t = 87, loss = 6.383174896240234\n",
      "t = 88, loss = 6.136966228485107\n",
      "t = 89, loss = 6.381943702697754\n",
      "t = 90, loss = 6.138674736022949\n",
      "t = 91, loss = 6.380111217498779\n",
      "t = 92, loss = 6.14058256149292\n",
      "t = 93, loss = 6.3787922859191895\n",
      "t = 94, loss = 6.142486572265625\n",
      "t = 95, loss = 6.377492904663086\n",
      "t = 96, loss = 6.143956184387207\n",
      "t = 97, loss = 6.376092433929443\n",
      "t = 98, loss = 6.1455793380737305\n",
      "t = 99, loss = 6.375161170959473\n",
      "t = 100, loss = 6.1467976570129395\n",
      "t = 101, loss = 4.9886274337768555\n",
      "t = 102, loss = 3.9023635387420654\n",
      "t = 103, loss = 2.9821629524230957\n",
      "t = 104, loss = 2.3245885372161865\n",
      "t = 105, loss = 1.9448341131210327\n",
      "t = 106, loss = 1.7749770879745483\n",
      "t = 107, loss = 1.7129437923431396\n",
      "t = 108, loss = 1.6866081953048706\n",
      "t = 109, loss = 1.6696938276290894\n",
      "t = 110, loss = 1.6559089422225952\n",
      "t = 111, loss = 1.6435999870300293\n",
      "t = 112, loss = 1.6322377920150757\n",
      "t = 113, loss = 1.6216411590576172\n",
      "t = 114, loss = 1.6117316484451294\n",
      "t = 115, loss = 1.6024478673934937\n",
      "t = 116, loss = 1.5937461853027344\n",
      "t = 117, loss = 1.5855729579925537\n",
      "t = 118, loss = 1.5778886079788208\n",
      "t = 119, loss = 1.5706733465194702\n",
      "t = 120, loss = 1.5638922452926636\n",
      "t = 121, loss = 1.5575147867202759\n",
      "t = 122, loss = 1.5515064001083374\n",
      "t = 123, loss = 1.5458354949951172\n",
      "t = 124, loss = 1.5404776334762573\n",
      "t = 125, loss = 1.5354057550430298\n",
      "t = 126, loss = 1.5305956602096558\n",
      "t = 127, loss = 1.5260238647460938\n",
      "t = 128, loss = 1.5216673612594604\n",
      "t = 129, loss = 1.517507791519165\n",
      "t = 130, loss = 1.5135257244110107\n",
      "t = 131, loss = 1.5097063779830933\n",
      "t = 132, loss = 1.5060410499572754\n",
      "t = 133, loss = 1.5025161504745483\n",
      "t = 134, loss = 1.499121069908142\n",
      "t = 135, loss = 1.495845913887024\n",
      "t = 136, loss = 1.492680549621582\n",
      "t = 137, loss = 1.4896162748336792\n",
      "t = 138, loss = 1.4866454601287842\n",
      "t = 139, loss = 1.4837623834609985\n",
      "t = 140, loss = 1.4809610843658447\n",
      "t = 141, loss = 1.4782373905181885\n",
      "t = 142, loss = 1.475584626197815\n",
      "t = 143, loss = 1.472998857498169\n",
      "t = 144, loss = 1.4704746007919312\n",
      "t = 145, loss = 1.4680079221725464\n",
      "t = 146, loss = 1.4655951261520386\n",
      "t = 147, loss = 1.4632353782653809\n",
      "t = 148, loss = 1.460923671722412\n",
      "t = 149, loss = 1.4586583375930786\n",
      "t = 150, loss = 1.4564357995986938\n",
      "t = 151, loss = 1.454254150390625\n",
      "t = 152, loss = 1.4521127939224243\n",
      "t = 153, loss = 1.4500102996826172\n",
      "t = 154, loss = 1.447945237159729\n",
      "t = 155, loss = 1.4459168910980225\n",
      "t = 156, loss = 1.4439221620559692\n",
      "t = 157, loss = 1.4419586658477783\n",
      "t = 158, loss = 1.4400256872177124\n",
      "t = 159, loss = 1.4381215572357178\n",
      "t = 160, loss = 1.4362465143203735\n",
      "t = 161, loss = 1.4344003200531006\n",
      "t = 162, loss = 1.432580828666687\n",
      "t = 163, loss = 1.4307876825332642\n",
      "t = 164, loss = 1.429019570350647\n",
      "t = 165, loss = 1.4272747039794922\n",
      "t = 166, loss = 1.4255526065826416\n",
      "t = 167, loss = 1.4238550662994385\n",
      "t = 168, loss = 1.4221802949905396\n",
      "t = 169, loss = 1.4205267429351807\n",
      "t = 170, loss = 1.4188945293426514\n",
      "t = 171, loss = 1.4172829389572144\n",
      "t = 172, loss = 1.4156917333602905\n",
      "t = 173, loss = 1.4141204357147217\n",
      "t = 174, loss = 1.412568211555481\n",
      "t = 175, loss = 1.4110352993011475\n",
      "t = 176, loss = 1.4095209836959839\n",
      "t = 177, loss = 1.4080241918563843\n",
      "t = 178, loss = 1.4065446853637695\n",
      "t = 179, loss = 1.4050816297531128\n",
      "t = 180, loss = 1.4036344289779663\n",
      "t = 181, loss = 1.4022034406661987\n",
      "t = 182, loss = 1.4007889032363892\n",
      "t = 183, loss = 1.3993891477584839\n",
      "t = 184, loss = 1.3980048894882202\n",
      "t = 185, loss = 1.396634817123413\n",
      "t = 186, loss = 1.3952794075012207\n",
      "t = 187, loss = 1.3939385414123535\n",
      "t = 188, loss = 1.3926113843917847\n",
      "t = 189, loss = 1.3912981748580933\n",
      "t = 190, loss = 1.3899983167648315\n",
      "t = 191, loss = 1.3887120485305786\n",
      "t = 192, loss = 1.3874391317367554\n",
      "t = 193, loss = 1.3861780166625977\n",
      "t = 194, loss = 1.3849289417266846\n",
      "t = 195, loss = 1.3836919069290161\n",
      "t = 196, loss = 1.3824670314788818\n",
      "t = 197, loss = 1.381253957748413\n",
      "t = 198, loss = 1.3800513744354248\n",
      "t = 199, loss = 1.378859281539917\n",
      "t = 200, loss = 1.3776769638061523\n",
      "t = 201, loss = 1.3775596618652344\n",
      "t = 202, loss = 1.3774423599243164\n",
      "t = 203, loss = 1.3773249387741089\n",
      "t = 204, loss = 1.3772081136703491\n",
      "t = 205, loss = 1.3770909309387207\n",
      "t = 206, loss = 1.3769742250442505\n",
      "t = 207, loss = 1.3768573999404907\n",
      "t = 208, loss = 1.376740574836731\n",
      "t = 209, loss = 1.3766241073608398\n",
      "t = 210, loss = 1.3765077590942383\n",
      "t = 211, loss = 1.3763914108276367\n",
      "t = 212, loss = 1.3762751817703247\n",
      "t = 213, loss = 1.3761590719223022\n",
      "t = 214, loss = 1.3760432004928589\n",
      "t = 215, loss = 1.3759273290634155\n",
      "t = 216, loss = 1.3758114576339722\n",
      "t = 217, loss = 1.3756959438323975\n",
      "t = 218, loss = 1.3755803108215332\n",
      "t = 219, loss = 1.3754647970199585\n",
      "t = 220, loss = 1.375349521636963\n",
      "t = 221, loss = 1.3752343654632568\n",
      "t = 222, loss = 1.3751193284988403\n",
      "t = 223, loss = 1.3750044107437134\n",
      "t = 224, loss = 1.3748894929885864\n",
      "t = 225, loss = 1.374774694442749\n",
      "t = 226, loss = 1.374659776687622\n",
      "t = 227, loss = 1.3745455741882324\n",
      "t = 228, loss = 1.3744308948516846\n",
      "t = 229, loss = 1.374316692352295\n",
      "t = 230, loss = 1.3742023706436157\n",
      "t = 231, loss = 1.3740880489349365\n",
      "t = 232, loss = 1.3739742040634155\n",
      "t = 233, loss = 1.373860239982605\n",
      "t = 234, loss = 1.3737462759017944\n",
      "t = 235, loss = 1.373632550239563\n",
      "t = 236, loss = 1.373518943786621\n",
      "t = 237, loss = 1.3734053373336792\n",
      "t = 238, loss = 1.3732919692993164\n",
      "t = 239, loss = 1.373178482055664\n",
      "t = 240, loss = 1.3730653524398804\n",
      "t = 241, loss = 1.3729523420333862\n",
      "t = 242, loss = 1.3728392124176025\n",
      "t = 243, loss = 1.372726321220398\n",
      "t = 244, loss = 1.3726133108139038\n",
      "t = 245, loss = 1.3725006580352783\n",
      "t = 246, loss = 1.3723880052566528\n",
      "t = 247, loss = 1.3722752332687378\n",
      "t = 248, loss = 1.372162938117981\n",
      "t = 249, loss = 1.3720505237579346\n",
      "t = 250, loss = 1.3719384670257568\n",
      "t = 251, loss = 1.3718260526657104\n",
      "t = 252, loss = 1.3717139959335327\n",
      "t = 253, loss = 1.371602177619934\n",
      "t = 254, loss = 1.371490240097046\n",
      "t = 255, loss = 1.3713785409927368\n",
      "t = 256, loss = 1.3712668418884277\n",
      "t = 257, loss = 1.3711553812026978\n",
      "t = 258, loss = 1.3710439205169678\n",
      "t = 259, loss = 1.3709325790405273\n",
      "t = 260, loss = 1.370821475982666\n",
      "t = 261, loss = 1.3707103729248047\n",
      "t = 262, loss = 1.3705992698669434\n",
      "t = 263, loss = 1.3704882860183716\n",
      "t = 264, loss = 1.3703773021697998\n",
      "t = 265, loss = 1.3702665567398071\n",
      "t = 266, loss = 1.3701560497283936\n",
      "t = 267, loss = 1.3700454235076904\n",
      "t = 268, loss = 1.3699349164962769\n",
      "t = 269, loss = 1.369824767112732\n",
      "t = 270, loss = 1.369714379310608\n",
      "t = 271, loss = 1.369604229927063\n",
      "t = 272, loss = 1.3694943189620972\n",
      "t = 273, loss = 1.3693841695785522\n",
      "t = 274, loss = 1.3692742586135864\n",
      "t = 275, loss = 1.3691644668579102\n",
      "t = 276, loss = 1.369054913520813\n",
      "t = 277, loss = 1.3689454793930054\n",
      "t = 278, loss = 1.3688358068466187\n",
      "t = 279, loss = 1.368726372718811\n",
      "t = 280, loss = 1.3686169385910034\n",
      "t = 281, loss = 1.368507742881775\n",
      "t = 282, loss = 1.368398666381836\n",
      "t = 283, loss = 1.368289589881897\n",
      "t = 284, loss = 1.3681806325912476\n",
      "t = 285, loss = 1.3680719137191772\n",
      "t = 286, loss = 1.3679629564285278\n",
      "t = 287, loss = 1.3678542375564575\n",
      "t = 288, loss = 1.3677455186843872\n",
      "t = 289, loss = 1.3676371574401855\n",
      "t = 290, loss = 1.3675287961959839\n",
      "t = 291, loss = 1.3674201965332031\n",
      "t = 292, loss = 1.3673123121261597\n",
      "t = 293, loss = 1.3672040700912476\n",
      "t = 294, loss = 1.367095947265625\n",
      "t = 295, loss = 1.3669878244400024\n",
      "t = 296, loss = 1.366880178451538\n",
      "t = 297, loss = 1.3667722940444946\n",
      "t = 298, loss = 1.3666646480560303\n",
      "t = 299, loss = 1.3665571212768555\n",
      "t = 300, loss = 1.3664494752883911\n",
      "t = 301, loss = 1.3664387464523315\n",
      "t = 302, loss = 1.3664278984069824\n",
      "t = 303, loss = 1.3664170503616333\n",
      "t = 304, loss = 1.3664063215255737\n",
      "t = 305, loss = 1.3663957118988037\n",
      "t = 306, loss = 1.3663851022720337\n",
      "t = 307, loss = 1.3663742542266846\n",
      "t = 308, loss = 1.3663634061813354\n",
      "t = 309, loss = 1.3663526773452759\n",
      "t = 310, loss = 1.3663420677185059\n",
      "t = 311, loss = 1.3663312196731567\n",
      "t = 312, loss = 1.3663206100463867\n",
      "t = 313, loss = 1.3663097620010376\n",
      "t = 314, loss = 1.366299033164978\n",
      "t = 315, loss = 1.366288423538208\n",
      "t = 316, loss = 1.3662775754928589\n",
      "t = 317, loss = 1.3662668466567993\n",
      "t = 318, loss = 1.3662561178207397\n",
      "t = 319, loss = 1.3662453889846802\n",
      "t = 320, loss = 1.366234540939331\n",
      "t = 321, loss = 1.3662238121032715\n",
      "t = 322, loss = 1.366213321685791\n",
      "t = 323, loss = 1.366202473640442\n",
      "t = 324, loss = 1.3661917448043823\n",
      "t = 325, loss = 1.3661808967590332\n",
      "t = 326, loss = 1.3661702871322632\n",
      "t = 327, loss = 1.3661595582962036\n",
      "t = 328, loss = 1.366148829460144\n",
      "t = 329, loss = 1.3661381006240845\n",
      "t = 330, loss = 1.3661272525787354\n",
      "t = 331, loss = 1.3661165237426758\n",
      "t = 332, loss = 1.3661059141159058\n",
      "t = 333, loss = 1.3660951852798462\n",
      "t = 334, loss = 1.3660844564437866\n",
      "t = 335, loss = 1.3660736083984375\n",
      "t = 336, loss = 1.3660629987716675\n",
      "t = 337, loss = 1.3660523891448975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 338, loss = 1.3660415410995483\n",
      "t = 339, loss = 1.3660308122634888\n",
      "t = 340, loss = 1.3660202026367188\n",
      "t = 341, loss = 1.3660094738006592\n",
      "t = 342, loss = 1.3659987449645996\n",
      "t = 343, loss = 1.3659878969192505\n",
      "t = 344, loss = 1.3659772872924805\n",
      "t = 345, loss = 1.365966558456421\n",
      "t = 346, loss = 1.3659558296203613\n",
      "t = 347, loss = 1.3659451007843018\n",
      "t = 348, loss = 1.3659344911575317\n",
      "t = 349, loss = 1.3659237623214722\n",
      "t = 350, loss = 1.3659130334854126\n",
      "t = 351, loss = 1.3659021854400635\n",
      "t = 352, loss = 1.3658915758132935\n",
      "t = 353, loss = 1.3658809661865234\n",
      "t = 354, loss = 1.3658701181411743\n",
      "t = 355, loss = 1.3658595085144043\n",
      "t = 356, loss = 1.3658487796783447\n",
      "t = 357, loss = 1.3658381700515747\n",
      "t = 358, loss = 1.3658273220062256\n",
      "t = 359, loss = 1.3658167123794556\n",
      "t = 360, loss = 1.3658061027526855\n",
      "t = 361, loss = 1.3657952547073364\n",
      "t = 362, loss = 1.365784764289856\n",
      "t = 363, loss = 1.3657740354537964\n",
      "t = 364, loss = 1.3657631874084473\n",
      "t = 365, loss = 1.3657526969909668\n",
      "t = 366, loss = 1.3657419681549072\n",
      "t = 367, loss = 1.3657312393188477\n",
      "t = 368, loss = 1.365720510482788\n",
      "t = 369, loss = 1.365709900856018\n",
      "t = 370, loss = 1.365699052810669\n",
      "t = 371, loss = 1.3656885623931885\n",
      "t = 372, loss = 1.365677833557129\n",
      "t = 373, loss = 1.3656669855117798\n",
      "t = 374, loss = 1.3656564950942993\n",
      "t = 375, loss = 1.3656457662582397\n",
      "t = 376, loss = 1.3656351566314697\n",
      "t = 377, loss = 1.3656243085861206\n",
      "t = 378, loss = 1.3656136989593506\n",
      "t = 379, loss = 1.3656030893325806\n",
      "t = 380, loss = 1.3655924797058105\n",
      "t = 381, loss = 1.3655816316604614\n",
      "t = 382, loss = 1.365571141242981\n",
      "t = 383, loss = 1.3655604124069214\n",
      "t = 384, loss = 1.3655495643615723\n",
      "t = 385, loss = 1.3655390739440918\n",
      "t = 386, loss = 1.3655283451080322\n",
      "t = 387, loss = 1.3655176162719727\n",
      "t = 388, loss = 1.365506887435913\n",
      "t = 389, loss = 1.365496277809143\n",
      "t = 390, loss = 1.3654855489730835\n",
      "t = 391, loss = 1.3654749393463135\n",
      "t = 392, loss = 1.3654643297195435\n",
      "t = 393, loss = 1.3654537200927734\n",
      "t = 394, loss = 1.3654428720474243\n",
      "t = 395, loss = 1.3654323816299438\n",
      "t = 396, loss = 1.3654216527938843\n",
      "t = 397, loss = 1.3654110431671143\n",
      "t = 398, loss = 1.3654004335403442\n",
      "t = 399, loss = 1.3653897047042847\n",
      "t = 400, loss = 1.3653790950775146\n",
      "t = 401, loss = 1.3653780221939087\n",
      "t = 402, loss = 1.3653768301010132\n",
      "t = 403, loss = 1.3653759956359863\n",
      "t = 404, loss = 1.3653746843338013\n",
      "t = 405, loss = 1.3653737306594849\n",
      "t = 406, loss = 1.3653727769851685\n",
      "t = 407, loss = 1.3653714656829834\n",
      "t = 408, loss = 1.365370512008667\n",
      "t = 409, loss = 1.365369439125061\n",
      "t = 410, loss = 1.3653684854507446\n",
      "t = 411, loss = 1.3653674125671387\n",
      "t = 412, loss = 1.3653662204742432\n",
      "t = 413, loss = 1.3653651475906372\n",
      "t = 414, loss = 1.3653641939163208\n",
      "t = 415, loss = 1.3653632402420044\n",
      "t = 416, loss = 1.3653619289398193\n",
      "t = 417, loss = 1.365360975265503\n",
      "t = 418, loss = 1.365359902381897\n",
      "t = 419, loss = 1.365358829498291\n",
      "t = 420, loss = 1.3653578758239746\n",
      "t = 421, loss = 1.365356683731079\n",
      "t = 422, loss = 1.3653556108474731\n",
      "t = 423, loss = 1.3653546571731567\n",
      "t = 424, loss = 1.3653535842895508\n",
      "t = 425, loss = 1.3653523921966553\n",
      "t = 426, loss = 1.3653514385223389\n",
      "t = 427, loss = 1.365350365638733\n",
      "t = 428, loss = 1.365349292755127\n",
      "t = 429, loss = 1.3653483390808105\n",
      "t = 430, loss = 1.3653470277786255\n",
      "t = 431, loss = 1.365346074104309\n",
      "t = 432, loss = 1.3653450012207031\n",
      "t = 433, loss = 1.3653440475463867\n",
      "t = 434, loss = 1.3653429746627808\n",
      "t = 435, loss = 1.3653417825698853\n",
      "t = 436, loss = 1.3653407096862793\n",
      "t = 437, loss = 1.365339756011963\n",
      "t = 438, loss = 1.3653384447097778\n",
      "t = 439, loss = 1.3653374910354614\n",
      "t = 440, loss = 1.365336298942566\n",
      "t = 441, loss = 1.3653353452682495\n",
      "t = 442, loss = 1.365334391593933\n",
      "t = 443, loss = 1.3653334379196167\n",
      "t = 444, loss = 1.3653321266174316\n",
      "t = 445, loss = 1.3653311729431152\n",
      "t = 446, loss = 1.3653298616409302\n",
      "t = 447, loss = 1.3653289079666138\n",
      "t = 448, loss = 1.3653279542922974\n",
      "t = 449, loss = 1.3653268814086914\n",
      "t = 450, loss = 1.3653258085250854\n",
      "t = 451, loss = 1.365324854850769\n",
      "t = 452, loss = 1.3653236627578735\n",
      "t = 453, loss = 1.3653225898742676\n",
      "t = 454, loss = 1.3653216361999512\n",
      "t = 455, loss = 1.3653204441070557\n",
      "t = 456, loss = 1.3653194904327393\n",
      "t = 457, loss = 1.3653185367584229\n",
      "t = 458, loss = 1.3653172254562378\n",
      "t = 459, loss = 1.3653162717819214\n",
      "t = 460, loss = 1.365315318107605\n",
      "t = 461, loss = 1.36531400680542\n",
      "t = 462, loss = 1.3653130531311035\n",
      "t = 463, loss = 1.365311861038208\n",
      "t = 464, loss = 1.3653109073638916\n",
      "t = 465, loss = 1.3653099536895752\n",
      "t = 466, loss = 1.3653086423873901\n",
      "t = 467, loss = 1.3653076887130737\n",
      "t = 468, loss = 1.3653067350387573\n",
      "t = 469, loss = 1.3653056621551514\n",
      "t = 470, loss = 1.3653044700622559\n",
      "t = 471, loss = 1.3653035163879395\n",
      "t = 472, loss = 1.365302324295044\n",
      "t = 473, loss = 1.3653013706207275\n",
      "t = 474, loss = 1.3653004169464111\n",
      "t = 475, loss = 1.365299105644226\n",
      "t = 476, loss = 1.3652981519699097\n",
      "t = 477, loss = 1.3652970790863037\n",
      "t = 478, loss = 1.3652958869934082\n",
      "t = 479, loss = 1.3652949333190918\n",
      "t = 480, loss = 1.3652937412261963\n",
      "t = 481, loss = 1.3652927875518799\n",
      "t = 482, loss = 1.3652918338775635\n",
      "t = 483, loss = 1.3652905225753784\n",
      "t = 484, loss = 1.365289568901062\n",
      "t = 485, loss = 1.3652886152267456\n",
      "t = 486, loss = 1.36528742313385\n",
      "t = 487, loss = 1.3652864694595337\n",
      "t = 488, loss = 1.3652855157852173\n",
      "t = 489, loss = 1.3652842044830322\n",
      "t = 490, loss = 1.3652832508087158\n",
      "t = 491, loss = 1.3652821779251099\n",
      "t = 492, loss = 1.3652809858322144\n",
      "t = 493, loss = 1.365280032157898\n",
      "t = 494, loss = 1.3652790784835815\n",
      "t = 495, loss = 1.3652780055999756\n",
      "t = 496, loss = 1.3652769327163696\n",
      "t = 497, loss = 1.3652757406234741\n",
      "t = 498, loss = 1.3652746677398682\n",
      "t = 499, loss = 1.3652737140655518\n",
      "Training R2:  0.5300339521030952\n",
      "Test R2:  0.5328244112705286\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_feat = len(features_cols)\n",
    "out_feat = len(targetColumns)\n",
    "model = Linear_Net(in_features=in_feat ,out_features=out_feat).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=100, gamma=0.1)\n",
    "\n",
    "losses,model = train(model, optimiser, scheduler, criterion, epochs = 500)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "    y_pred_train = model(X_train)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "\n",
    "train_r2 = r2_score(y_train.detach().cpu().numpy(), y_pred_train.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "test_r2=r2_score(y_test.detach().cpu().numpy(), y_pred_test.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "\n",
    "print(\"Training R2: \",train_r2)\n",
    "print(\"Test R2: \",test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 2.5655782222747803\n",
      "t = 1, loss = 2.2825512886047363\n",
      "t = 2, loss = 2.2152745723724365\n",
      "t = 3, loss = 2.168558120727539\n",
      "t = 4, loss = 2.1278562545776367\n",
      "t = 5, loss = 2.1023380756378174\n",
      "t = 6, loss = 2.0566565990448\n",
      "t = 7, loss = 2.0189907550811768\n",
      "t = 8, loss = 1.9235605001449585\n",
      "t = 9, loss = 1.8491952419281006\n",
      "t = 10, loss = 1.7690478563308716\n",
      "t = 11, loss = 1.7313916683197021\n",
      "t = 12, loss = 1.673329472541809\n",
      "t = 13, loss = 1.6365251541137695\n",
      "t = 14, loss = 1.5801966190338135\n",
      "t = 15, loss = 1.5730767250061035\n",
      "t = 16, loss = 1.540291666984558\n",
      "t = 17, loss = 1.5311719179153442\n",
      "t = 18, loss = 1.5480622053146362\n",
      "t = 19, loss = 1.5405449867248535\n",
      "t = 20, loss = 1.466984748840332\n",
      "t = 21, loss = 1.4682649374008179\n",
      "t = 22, loss = 1.4678882360458374\n",
      "t = 23, loss = 1.4067209959030151\n",
      "t = 24, loss = 1.4065113067626953\n",
      "t = 25, loss = 1.3920600414276123\n",
      "t = 26, loss = 1.3464971780776978\n",
      "t = 27, loss = 1.3488537073135376\n",
      "t = 28, loss = 1.336113691329956\n",
      "t = 29, loss = 1.31768798828125\n",
      "t = 30, loss = 1.3228280544281006\n",
      "t = 31, loss = 1.287205696105957\n",
      "t = 32, loss = 1.3119951486587524\n",
      "t = 33, loss = 1.2693439722061157\n",
      "t = 34, loss = 1.3260040283203125\n",
      "t = 35, loss = 1.2903923988342285\n",
      "t = 36, loss = 1.362145185470581\n",
      "t = 37, loss = 1.2437026500701904\n",
      "t = 38, loss = 1.4482710361480713\n",
      "t = 39, loss = 1.2826025485992432\n",
      "t = 40, loss = 1.6885510683059692\n",
      "t = 41, loss = 1.2044281959533691\n",
      "t = 42, loss = 1.4122123718261719\n",
      "t = 43, loss = 1.184887409210205\n",
      "t = 44, loss = 1.4659514427185059\n",
      "t = 45, loss = 1.2192844152450562\n",
      "t = 46, loss = 1.5669349431991577\n",
      "t = 47, loss = 1.1702083349227905\n",
      "t = 48, loss = 1.3817665576934814\n",
      "t = 49, loss = 1.1641522645950317\n",
      "t = 50, loss = 1.4408791065216064\n",
      "t = 51, loss = 1.1696538925170898\n",
      "t = 52, loss = 1.4373446702957153\n",
      "t = 53, loss = 1.1640890836715698\n",
      "t = 54, loss = 1.4448649883270264\n",
      "t = 55, loss = 1.1506335735321045\n",
      "t = 56, loss = 1.4393577575683594\n",
      "t = 57, loss = 1.144336462020874\n",
      "t = 58, loss = 1.4247384071350098\n",
      "t = 59, loss = 1.1344654560089111\n",
      "t = 60, loss = 1.3872952461242676\n",
      "t = 61, loss = 1.1474792957305908\n",
      "t = 62, loss = 1.447376012802124\n",
      "t = 63, loss = 1.1280165910720825\n",
      "t = 64, loss = 1.4074046611785889\n",
      "t = 65, loss = 1.1173354387283325\n",
      "t = 66, loss = 1.4053834676742554\n",
      "t = 67, loss = 1.1119012832641602\n",
      "t = 68, loss = 1.350560188293457\n",
      "t = 69, loss = 1.1396950483322144\n",
      "t = 70, loss = 1.46243417263031\n",
      "t = 71, loss = 1.1059165000915527\n",
      "t = 72, loss = 1.3758560419082642\n",
      "t = 73, loss = 1.0822582244873047\n",
      "t = 74, loss = 1.3298453092575073\n",
      "t = 75, loss = 1.112716555595398\n",
      "t = 76, loss = 1.3924552202224731\n",
      "t = 77, loss = 1.0777710676193237\n",
      "t = 78, loss = 1.3640714883804321\n",
      "t = 79, loss = 1.0981048345565796\n",
      "t = 80, loss = 1.402698040008545\n",
      "t = 81, loss = 1.098372459411621\n",
      "t = 82, loss = 1.3589715957641602\n",
      "t = 83, loss = 1.0904260873794556\n",
      "t = 84, loss = 1.3703311681747437\n",
      "t = 85, loss = 1.0828697681427002\n",
      "t = 86, loss = 1.3581597805023193\n",
      "t = 87, loss = 1.07703697681427\n",
      "t = 88, loss = 1.3403992652893066\n",
      "t = 89, loss = 1.086369276046753\n",
      "t = 90, loss = 1.3663177490234375\n",
      "t = 91, loss = 1.073478102684021\n",
      "t = 92, loss = 1.3390867710113525\n",
      "t = 93, loss = 1.063195824623108\n",
      "t = 94, loss = 1.3287138938903809\n",
      "t = 95, loss = 1.0803879499435425\n",
      "t = 96, loss = 1.344663381576538\n",
      "t = 97, loss = 1.0580955743789673\n",
      "t = 98, loss = 1.308610439300537\n",
      "t = 99, loss = 1.0555180311203003\n",
      "t = 100, loss = 1.321471929550171\n",
      "t = 101, loss = 1.2612931728363037\n",
      "t = 102, loss = 1.2047709226608276\n",
      "t = 103, loss = 1.152929663658142\n",
      "t = 104, loss = 1.1062484979629517\n",
      "t = 105, loss = 1.06534743309021\n",
      "t = 106, loss = 1.0303879976272583\n",
      "t = 107, loss = 1.0017023086547852\n",
      "t = 108, loss = 0.9796096682548523\n",
      "t = 109, loss = 0.9638801217079163\n",
      "t = 110, loss = 0.9538716077804565\n",
      "t = 111, loss = 0.9481948614120483\n",
      "t = 112, loss = 0.9450283646583557\n",
      "t = 113, loss = 0.9431788325309753\n",
      "t = 114, loss = 0.942078173160553\n",
      "t = 115, loss = 0.941402018070221\n",
      "t = 116, loss = 0.9409083724021912\n",
      "t = 117, loss = 0.9404908418655396\n",
      "t = 118, loss = 0.9401147365570068\n",
      "t = 119, loss = 0.9397634863853455\n",
      "t = 120, loss = 0.9394294023513794\n",
      "t = 121, loss = 0.9391069412231445\n",
      "t = 122, loss = 0.9387942552566528\n",
      "t = 123, loss = 0.9384897947311401\n",
      "t = 124, loss = 0.9381921887397766\n",
      "t = 125, loss = 0.9379013776779175\n",
      "t = 126, loss = 0.937616765499115\n",
      "t = 127, loss = 0.9373377561569214\n",
      "t = 128, loss = 0.9370636343955994\n",
      "t = 129, loss = 0.9367944002151489\n",
      "t = 130, loss = 0.9365295171737671\n",
      "t = 131, loss = 0.9362687468528748\n",
      "t = 132, loss = 0.9360119104385376\n",
      "t = 133, loss = 0.9357588887214661\n",
      "t = 134, loss = 0.935509443283081\n",
      "t = 135, loss = 0.9352630972862244\n",
      "t = 136, loss = 0.9350197911262512\n",
      "t = 137, loss = 0.9347800016403198\n",
      "t = 138, loss = 0.9345433712005615\n",
      "t = 139, loss = 0.9343095421791077\n",
      "t = 140, loss = 0.9340786337852478\n",
      "t = 141, loss = 0.9338507056236267\n",
      "t = 142, loss = 0.9336254000663757\n",
      "t = 143, loss = 0.9334024786949158\n",
      "t = 144, loss = 0.9331820607185364\n",
      "t = 145, loss = 0.932963490486145\n",
      "t = 146, loss = 0.9327470064163208\n",
      "t = 147, loss = 0.9325328469276428\n",
      "t = 148, loss = 0.9323208332061768\n",
      "t = 149, loss = 0.9321109056472778\n",
      "t = 150, loss = 0.9319029450416565\n",
      "t = 151, loss = 0.9316970109939575\n",
      "t = 152, loss = 0.9314927458763123\n",
      "t = 153, loss = 0.931290328502655\n",
      "t = 154, loss = 0.9310893416404724\n",
      "t = 155, loss = 0.930889904499054\n",
      "t = 156, loss = 0.9306921362876892\n",
      "t = 157, loss = 0.9304958581924438\n",
      "t = 158, loss = 0.9303010702133179\n",
      "t = 159, loss = 0.930107593536377\n",
      "t = 160, loss = 0.9299153685569763\n",
      "t = 161, loss = 0.9297246336936951\n",
      "t = 162, loss = 0.9295352697372437\n",
      "t = 163, loss = 0.9293471574783325\n",
      "t = 164, loss = 0.9291607141494751\n",
      "t = 165, loss = 0.9289754629135132\n",
      "t = 166, loss = 0.9287915229797363\n",
      "t = 167, loss = 0.928608775138855\n",
      "t = 168, loss = 0.9284271001815796\n",
      "t = 169, loss = 0.9282465577125549\n",
      "t = 170, loss = 0.9280669689178467\n",
      "t = 171, loss = 0.9278886914253235\n",
      "t = 172, loss = 0.927711546421051\n",
      "t = 173, loss = 0.9275355935096741\n",
      "t = 174, loss = 0.9273607134819031\n",
      "t = 175, loss = 0.927186906337738\n",
      "t = 176, loss = 0.9270142316818237\n",
      "t = 177, loss = 0.9268426895141602\n",
      "t = 178, loss = 0.9266722798347473\n",
      "t = 179, loss = 0.9265028834342957\n",
      "t = 180, loss = 0.9263344407081604\n",
      "t = 181, loss = 0.926166832447052\n",
      "t = 182, loss = 0.9260004162788391\n",
      "t = 183, loss = 0.9258349537849426\n",
      "t = 184, loss = 0.9256705641746521\n",
      "t = 185, loss = 0.9255067706108093\n",
      "t = 186, loss = 0.925343930721283\n",
      "t = 187, loss = 0.9251819252967834\n",
      "t = 188, loss = 0.9250211119651794\n",
      "t = 189, loss = 0.9248610138893127\n",
      "t = 190, loss = 0.9247016906738281\n",
      "t = 191, loss = 0.924543023109436\n",
      "t = 192, loss = 0.9243850708007812\n",
      "t = 193, loss = 0.9242280125617981\n",
      "t = 194, loss = 0.9240714907646179\n",
      "t = 195, loss = 0.9239158630371094\n",
      "t = 196, loss = 0.9237610697746277\n",
      "t = 197, loss = 0.9236069321632385\n",
      "t = 198, loss = 0.9234533905982971\n",
      "t = 199, loss = 0.9233008027076721\n",
      "t = 200, loss = 0.9231489896774292\n",
      "t = 201, loss = 0.9231337904930115\n",
      "t = 202, loss = 0.9231187105178833\n",
      "t = 203, loss = 0.9231036901473999\n",
      "t = 204, loss = 0.9230886101722717\n",
      "t = 205, loss = 0.9230735301971436\n",
      "t = 206, loss = 0.9230584502220154\n",
      "t = 207, loss = 0.9230433702468872\n",
      "t = 208, loss = 0.9230283498764038\n",
      "t = 209, loss = 0.9230132102966309\n",
      "t = 210, loss = 0.9229981303215027\n",
      "t = 211, loss = 0.9229831099510193\n",
      "t = 212, loss = 0.9229680895805359\n",
      "t = 213, loss = 0.9229531288146973\n",
      "t = 214, loss = 0.9229380488395691\n",
      "t = 215, loss = 0.9229230284690857\n",
      "t = 216, loss = 0.9229081273078918\n",
      "t = 217, loss = 0.9228930473327637\n",
      "t = 218, loss = 0.9228780269622803\n",
      "t = 219, loss = 0.9228630661964417\n",
      "t = 220, loss = 0.922848105430603\n",
      "t = 221, loss = 0.9228331446647644\n",
      "t = 222, loss = 0.922818124294281\n",
      "t = 223, loss = 0.9228032231330872\n",
      "t = 224, loss = 0.9227882027626038\n",
      "t = 225, loss = 0.9227733612060547\n",
      "t = 226, loss = 0.9227582812309265\n",
      "t = 227, loss = 0.9227433800697327\n",
      "t = 228, loss = 0.9227284789085388\n",
      "t = 229, loss = 0.9227135181427002\n",
      "t = 230, loss = 0.9226986169815063\n",
      "t = 231, loss = 0.9226837158203125\n",
      "t = 232, loss = 0.9226687550544739\n",
      "t = 233, loss = 0.92265385389328\n",
      "t = 234, loss = 0.9226389527320862\n",
      "t = 235, loss = 0.9226240515708923\n",
      "t = 236, loss = 0.9226090908050537\n",
      "t = 237, loss = 0.9225942492485046\n",
      "t = 238, loss = 0.9225794076919556\n",
      "t = 239, loss = 0.9225645065307617\n",
      "t = 240, loss = 0.9225496649742126\n",
      "t = 241, loss = 0.9225348234176636\n",
      "t = 242, loss = 0.9225200414657593\n",
      "t = 243, loss = 0.9225051403045654\n",
      "t = 244, loss = 0.9224903583526611\n",
      "t = 245, loss = 0.9224756360054016\n",
      "t = 246, loss = 0.9224607944488525\n",
      "t = 247, loss = 0.9224461317062378\n",
      "t = 248, loss = 0.9224312901496887\n",
      "t = 249, loss = 0.9224165081977844\n",
      "t = 250, loss = 0.9224016666412354\n",
      "t = 251, loss = 0.922386884689331\n",
      "t = 252, loss = 0.9223721623420715\n",
      "t = 253, loss = 0.922357439994812\n",
      "t = 254, loss = 0.9223427176475525\n",
      "t = 255, loss = 0.922327995300293\n",
      "t = 256, loss = 0.9223132729530334\n",
      "t = 257, loss = 0.9222984910011292\n",
      "t = 258, loss = 0.9222837686538696\n",
      "t = 259, loss = 0.9222691655158997\n",
      "t = 260, loss = 0.9222544431686401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 261, loss = 0.9222397804260254\n",
      "t = 262, loss = 0.9222249984741211\n",
      "t = 263, loss = 0.9222103357315063\n",
      "t = 264, loss = 0.9221956729888916\n",
      "t = 265, loss = 0.9221809506416321\n",
      "t = 266, loss = 0.9221662878990173\n",
      "t = 267, loss = 0.9221516847610474\n",
      "t = 268, loss = 0.9221370220184326\n",
      "t = 269, loss = 0.9221222996711731\n",
      "t = 270, loss = 0.9221077561378479\n",
      "t = 271, loss = 0.9220931529998779\n",
      "t = 272, loss = 0.9220783710479736\n",
      "t = 273, loss = 0.9220637679100037\n",
      "t = 274, loss = 0.9220491647720337\n",
      "t = 275, loss = 0.9220345616340637\n",
      "t = 276, loss = 0.9220199584960938\n",
      "t = 277, loss = 0.9220053553581238\n",
      "t = 278, loss = 0.9219907522201538\n",
      "t = 279, loss = 0.9219761490821838\n",
      "t = 280, loss = 0.9219615459442139\n",
      "t = 281, loss = 0.9219469428062439\n",
      "t = 282, loss = 0.9219323992729187\n",
      "t = 283, loss = 0.9219179153442383\n",
      "t = 284, loss = 0.9219033122062683\n",
      "t = 285, loss = 0.9218887090682983\n",
      "t = 286, loss = 0.9218742847442627\n",
      "t = 287, loss = 0.9218597412109375\n",
      "t = 288, loss = 0.9218451976776123\n",
      "t = 289, loss = 0.9218305945396423\n",
      "t = 290, loss = 0.9218161702156067\n",
      "t = 291, loss = 0.9218015670776367\n",
      "t = 292, loss = 0.9217871427536011\n",
      "t = 293, loss = 0.9217725992202759\n",
      "t = 294, loss = 0.9217581748962402\n",
      "t = 295, loss = 0.921743631362915\n",
      "t = 296, loss = 0.9217290878295898\n",
      "t = 297, loss = 0.921714723110199\n",
      "t = 298, loss = 0.921700119972229\n",
      "t = 299, loss = 0.9216856956481934\n",
      "t = 300, loss = 0.9216712713241577\n",
      "t = 301, loss = 0.9216698408126831\n",
      "t = 302, loss = 0.9216684103012085\n",
      "t = 303, loss = 0.9216669201850891\n",
      "t = 304, loss = 0.921665370464325\n",
      "t = 305, loss = 0.9216639399528503\n",
      "t = 306, loss = 0.9216625094413757\n",
      "t = 307, loss = 0.9216610789299011\n",
      "t = 308, loss = 0.9216597676277161\n",
      "t = 309, loss = 0.9216582179069519\n",
      "t = 310, loss = 0.9216568470001221\n",
      "t = 311, loss = 0.9216554164886475\n",
      "t = 312, loss = 0.9216539859771729\n",
      "t = 313, loss = 0.9216525554656982\n",
      "t = 314, loss = 0.9216511249542236\n",
      "t = 315, loss = 0.9216495752334595\n",
      "t = 316, loss = 0.9216480851173401\n",
      "t = 317, loss = 0.9216466546058655\n",
      "t = 318, loss = 0.9216452240943909\n",
      "t = 319, loss = 0.9216437935829163\n",
      "t = 320, loss = 0.9216423630714417\n",
      "t = 321, loss = 0.921640932559967\n",
      "t = 322, loss = 0.9216395020484924\n",
      "t = 323, loss = 0.9216380715370178\n",
      "t = 324, loss = 0.9216366410255432\n",
      "t = 325, loss = 0.9216351509094238\n",
      "t = 326, loss = 0.9216337203979492\n",
      "t = 327, loss = 0.9216322302818298\n",
      "t = 328, loss = 0.9216307997703552\n",
      "t = 329, loss = 0.9216293692588806\n",
      "t = 330, loss = 0.921627938747406\n",
      "t = 331, loss = 0.9216265082359314\n",
      "t = 332, loss = 0.9216250777244568\n",
      "t = 333, loss = 0.9216235876083374\n",
      "t = 334, loss = 0.9216221570968628\n",
      "t = 335, loss = 0.9216207265853882\n",
      "t = 336, loss = 0.921619176864624\n",
      "t = 337, loss = 0.921617865562439\n",
      "t = 338, loss = 0.9216163158416748\n",
      "t = 339, loss = 0.921614944934845\n",
      "t = 340, loss = 0.9216135144233704\n",
      "t = 341, loss = 0.9216120839118958\n",
      "t = 342, loss = 0.9216106534004211\n",
      "t = 343, loss = 0.921609103679657\n",
      "t = 344, loss = 0.9216077923774719\n",
      "t = 345, loss = 0.9216062426567078\n",
      "t = 346, loss = 0.9216049313545227\n",
      "t = 347, loss = 0.9216033816337585\n",
      "t = 348, loss = 0.9216018915176392\n",
      "t = 349, loss = 0.9216004610061646\n",
      "t = 350, loss = 0.9215990304946899\n",
      "t = 351, loss = 0.9215975999832153\n",
      "t = 352, loss = 0.9215961694717407\n",
      "t = 353, loss = 0.9215947389602661\n",
      "t = 354, loss = 0.9215933084487915\n",
      "t = 355, loss = 0.9215918183326721\n",
      "t = 356, loss = 0.9215903878211975\n",
      "t = 357, loss = 0.9215889573097229\n",
      "t = 358, loss = 0.9215875267982483\n",
      "t = 359, loss = 0.9215860366821289\n",
      "t = 360, loss = 0.9215846657752991\n",
      "t = 361, loss = 0.9215831756591797\n",
      "t = 362, loss = 0.9215817451477051\n",
      "t = 363, loss = 0.9215803146362305\n",
      "t = 364, loss = 0.9215788841247559\n",
      "t = 365, loss = 0.9215774536132812\n",
      "t = 366, loss = 0.9215760231018066\n",
      "t = 367, loss = 0.921574592590332\n",
      "t = 368, loss = 0.9215731024742126\n",
      "t = 369, loss = 0.921571671962738\n",
      "t = 370, loss = 0.9215702414512634\n",
      "t = 371, loss = 0.921568751335144\n",
      "t = 372, loss = 0.9215673804283142\n",
      "t = 373, loss = 0.9215659499168396\n",
      "t = 374, loss = 0.9215644598007202\n",
      "t = 375, loss = 0.9215630292892456\n",
      "t = 376, loss = 0.921561598777771\n",
      "t = 377, loss = 0.9215601682662964\n",
      "t = 378, loss = 0.9215587377548218\n",
      "t = 379, loss = 0.9215573072433472\n",
      "t = 380, loss = 0.9215558767318726\n",
      "t = 381, loss = 0.9215543866157532\n",
      "t = 382, loss = 0.9215529561042786\n",
      "t = 383, loss = 0.9215514063835144\n",
      "t = 384, loss = 0.9215500950813293\n",
      "t = 385, loss = 0.9215485453605652\n",
      "t = 386, loss = 0.9215472340583801\n",
      "t = 387, loss = 0.9215457439422607\n",
      "t = 388, loss = 0.9215443134307861\n",
      "t = 389, loss = 0.9215428829193115\n",
      "t = 390, loss = 0.9215414524078369\n",
      "t = 391, loss = 0.9215400218963623\n",
      "t = 392, loss = 0.9215385913848877\n",
      "t = 393, loss = 0.9215371608734131\n",
      "t = 394, loss = 0.9215357303619385\n",
      "t = 395, loss = 0.9215342402458191\n",
      "t = 396, loss = 0.9215326905250549\n",
      "t = 397, loss = 0.9215314388275146\n",
      "t = 398, loss = 0.9215298295021057\n",
      "t = 399, loss = 0.9215285181999207\n",
      "t = 400, loss = 0.921527087688446\n",
      "t = 401, loss = 0.9215269088745117\n",
      "t = 402, loss = 0.9215267300605774\n",
      "t = 403, loss = 0.9215266108512878\n",
      "t = 404, loss = 0.9215264916419983\n",
      "t = 405, loss = 0.9215263724327087\n",
      "t = 406, loss = 0.9215261936187744\n",
      "t = 407, loss = 0.9215260148048401\n",
      "t = 408, loss = 0.9215258955955505\n",
      "t = 409, loss = 0.9215257167816162\n",
      "t = 410, loss = 0.9215256571769714\n",
      "t = 411, loss = 0.9215255379676819\n",
      "t = 412, loss = 0.9215254187583923\n",
      "t = 413, loss = 0.921525239944458\n",
      "t = 414, loss = 0.9215250611305237\n",
      "t = 415, loss = 0.9215249419212341\n",
      "t = 416, loss = 0.9215247631072998\n",
      "t = 417, loss = 0.9215245842933655\n",
      "t = 418, loss = 0.9215244650840759\n",
      "t = 419, loss = 0.9215242862701416\n",
      "t = 420, loss = 0.9215241074562073\n",
      "t = 421, loss = 0.9215240478515625\n",
      "t = 422, loss = 0.9215239882469177\n",
      "t = 423, loss = 0.9215238094329834\n",
      "t = 424, loss = 0.9215236306190491\n",
      "t = 425, loss = 0.9215235114097595\n",
      "t = 426, loss = 0.9215233325958252\n",
      "t = 427, loss = 0.9215231537818909\n",
      "t = 428, loss = 0.9215230345726013\n",
      "t = 429, loss = 0.9215229153633118\n",
      "t = 430, loss = 0.9215227961540222\n",
      "t = 431, loss = 0.9215226173400879\n",
      "t = 432, loss = 0.9215225577354431\n",
      "t = 433, loss = 0.9215223789215088\n",
      "t = 434, loss = 0.9215222001075745\n",
      "t = 435, loss = 0.9215220808982849\n",
      "t = 436, loss = 0.9215219020843506\n",
      "t = 437, loss = 0.9215218424797058\n",
      "t = 438, loss = 0.9215216636657715\n",
      "t = 439, loss = 0.9215214848518372\n",
      "t = 440, loss = 0.9215213656425476\n",
      "t = 441, loss = 0.9215211868286133\n",
      "t = 442, loss = 0.921521008014679\n",
      "t = 443, loss = 0.9215208888053894\n",
      "t = 444, loss = 0.9215207099914551\n",
      "t = 445, loss = 0.9215206503868103\n",
      "t = 446, loss = 0.921520471572876\n",
      "t = 447, loss = 0.9215203523635864\n",
      "t = 448, loss = 0.9215202331542969\n",
      "t = 449, loss = 0.9215201139450073\n",
      "t = 450, loss = 0.9215199947357178\n",
      "t = 451, loss = 0.9215198755264282\n",
      "t = 452, loss = 0.9215196967124939\n",
      "t = 453, loss = 0.9215195178985596\n",
      "t = 454, loss = 0.92151939868927\n",
      "t = 455, loss = 0.9215192198753357\n",
      "t = 456, loss = 0.9215190410614014\n",
      "t = 457, loss = 0.9215189218521118\n",
      "t = 458, loss = 0.9215187430381775\n",
      "t = 459, loss = 0.9215185642242432\n",
      "t = 460, loss = 0.9215185046195984\n",
      "t = 461, loss = 0.9215184450149536\n",
      "t = 462, loss = 0.9215182662010193\n",
      "t = 463, loss = 0.921518087387085\n",
      "t = 464, loss = 0.9215179681777954\n",
      "t = 465, loss = 0.9215177893638611\n",
      "t = 466, loss = 0.9215176105499268\n",
      "t = 467, loss = 0.9215174913406372\n",
      "t = 468, loss = 0.9215173125267029\n",
      "t = 469, loss = 0.9215172529220581\n",
      "t = 470, loss = 0.9215170741081238\n",
      "t = 471, loss = 0.921517014503479\n",
      "t = 472, loss = 0.9215168356895447\n",
      "t = 473, loss = 0.9215166568756104\n",
      "t = 474, loss = 0.9215165376663208\n",
      "t = 475, loss = 0.9215163588523865\n",
      "t = 476, loss = 0.9215161800384521\n",
      "t = 477, loss = 0.9215160608291626\n",
      "t = 478, loss = 0.921515941619873\n",
      "t = 479, loss = 0.9215158224105835\n",
      "t = 480, loss = 0.9215156435966492\n",
      "t = 481, loss = 0.9215154647827148\n",
      "t = 482, loss = 0.9215154051780701\n",
      "t = 483, loss = 0.9215152263641357\n",
      "t = 484, loss = 0.9215151071548462\n",
      "t = 485, loss = 0.9215149283409119\n",
      "t = 486, loss = 0.9215147495269775\n",
      "t = 487, loss = 0.921514630317688\n",
      "t = 488, loss = 0.9215145111083984\n",
      "t = 489, loss = 0.9215142726898193\n",
      "t = 490, loss = 0.9215141534805298\n",
      "t = 491, loss = 0.9215141534805298\n",
      "t = 492, loss = 0.9215139746665955\n",
      "t = 493, loss = 0.9215137958526611\n",
      "t = 494, loss = 0.9215136766433716\n",
      "t = 495, loss = 0.9215134978294373\n",
      "t = 496, loss = 0.9215134382247925\n",
      "t = 497, loss = 0.9215132594108582\n",
      "t = 498, loss = 0.9215131998062134\n",
      "t = 499, loss = 0.921513020992279\n",
      "Training R2:  0.49329664399375783\n",
      "Test R2:  0.5025523652306471\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_feat = len(features_cols)\n",
    "out_feat = len(targetColumns)\n",
    "model = Simple_Net(in_features=in_feat ,out_features=out_feat).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=100, gamma=0.1)\n",
    "\n",
    "losses,model = train(model, optimiser, scheduler, criterion, epochs = 500)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "    y_pred_train = model(X_train)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "\n",
    "train_r2 = r2_score(y_train.detach().cpu().numpy(), y_pred_train.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "test_r2=r2_score(y_test.detach().cpu().numpy(), y_pred_test.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "\n",
    "print(\"Training R2: \",train_r2)\n",
    "print(\"Test R2: \",test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
