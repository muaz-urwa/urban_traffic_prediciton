{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/urwa/Documents/side_projects/urban/data/featureData/lga.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, scheduler, criterion,epochs = 500):\n",
    "    losses = []\n",
    "    # Main optimization loop\n",
    "    for t in range(epochs):\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        y_predicted = model(X_train)\n",
    "\n",
    "        current_loss = criterion(y_predicted, y_train)\n",
    "\n",
    "        current_loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        print(f\"t = {t}, loss = {current_loss}\")\n",
    "\n",
    "        losses.append(current_loss)\n",
    "\n",
    "        scheduler.step()    \n",
    "    return losses,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8757, 1045)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>...</th>\n",
       "      <th>91_lag_3</th>\n",
       "      <th>92_lag_3</th>\n",
       "      <th>93_lag_3</th>\n",
       "      <th>94_lag_3</th>\n",
       "      <th>95_lag_3</th>\n",
       "      <th>96_lag_3</th>\n",
       "      <th>97_lag_3</th>\n",
       "      <th>98_lag_3</th>\n",
       "      <th>99_lag_3</th>\n",
       "      <th>arrival_lag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 1045 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Hour  1  10  100  101  102  106  107  108  ...  91_lag_3  \\\n",
       "0  2018-01-01     3  0   0    0    0    0    0    0    0  ...       0.0   \n",
       "1  2018-01-01     4  1   0    0    0    0    0    0    0  ...       0.0   \n",
       "2  2018-01-01     5  1   0    0    0    0    0    0    0  ...       0.0   \n",
       "\n",
       "   92_lag_3  93_lag_3  94_lag_3  95_lag_3  96_lag_3  97_lag_3  98_lag_3  \\\n",
       "0       1.0       0.0       1.0       0.0       0.0       1.0       0.0   \n",
       "1       1.0       0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "2       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   99_lag_3  arrival_lag_3  \n",
       "0       0.0            3.0  \n",
       "1       0.0            0.0  \n",
       "2       0.0            1.0  \n",
       "\n",
       "[3 rows x 1045 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Net(nn.Module):\n",
    "    def __init__(self, in_features,out_features):\n",
    "        super(Linear_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=out_features, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Net(nn.Module):\n",
    "    def __init__(self, in_features,out_features):\n",
    "        super(Simple_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=1000, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=1000, out_features=500, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=500, out_features=out_features, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = self.fc3(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag_columns = [c for c in dataset.columns if 'lag' in c]\n",
    "len(lag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DateColumns = ['Date']\n",
    "\n",
    "ext_columns = ['Dow', 'arrival','maxtemp', 'mintemp', 'avgtemp', 'departure', 'hdd',\n",
    "       'cdd', 'participation', 'newsnow', 'snowdepth', 'ifSnow']\n",
    "\n",
    "targetColumns = [c for c in dataset.columns if c not in ext_columns and \\\n",
    "                c not in DateColumns and c not in lag_columns and c != 'Hour']\n",
    "len(targetColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "787"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_cols = [c for c in dataset.columns if c not in targetColumns and c not in DateColumns]\n",
    "len(features_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset[features_cols].values\n",
    "y = dataset[targetColumns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_x = StandardScaler()\n",
    "# scaler_y = StandardScaler()\n",
    "\n",
    "# scaler_x.fit(x)\n",
    "# scaler_y.fit(y)\n",
    "\n",
    "# x = scaler_x.transform(x)\n",
    "# y = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8757, 787])\n",
      "torch.Size([8757, 257])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(x).float().to(device)\n",
    "print(x.shape)\n",
    "y = torch.tensor(y).float().to(device)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 4.915871620178223\n",
      "t = 1, loss = 5.286558628082275\n",
      "t = 2, loss = 5.178562164306641\n",
      "t = 3, loss = 5.694545269012451\n",
      "t = 4, loss = 5.298595428466797\n",
      "t = 5, loss = 5.793187141418457\n",
      "t = 6, loss = 5.277612209320068\n",
      "t = 7, loss = 5.843708515167236\n",
      "t = 8, loss = 5.275269031524658\n",
      "t = 9, loss = 5.867095947265625\n",
      "t = 10, loss = 5.26806116104126\n",
      "t = 11, loss = 5.888187408447266\n",
      "t = 12, loss = 5.263960361480713\n",
      "t = 13, loss = 5.903356552124023\n",
      "t = 14, loss = 5.259459018707275\n",
      "t = 15, loss = 5.91526985168457\n",
      "t = 16, loss = 5.255254745483398\n",
      "t = 17, loss = 5.926268577575684\n",
      "t = 18, loss = 5.255317211151123\n",
      "t = 19, loss = 5.937730312347412\n",
      "t = 20, loss = 5.255524158477783\n",
      "t = 21, loss = 5.943543434143066\n",
      "t = 22, loss = 5.252948760986328\n",
      "t = 23, loss = 5.947137832641602\n",
      "t = 24, loss = 5.250796318054199\n",
      "t = 25, loss = 5.950705528259277\n",
      "t = 26, loss = 5.248780727386475\n",
      "t = 27, loss = 5.955406188964844\n",
      "t = 28, loss = 5.248271465301514\n",
      "t = 29, loss = 5.960672855377197\n",
      "t = 30, loss = 5.2501349449157715\n",
      "t = 31, loss = 5.965808868408203\n",
      "t = 32, loss = 5.250547409057617\n",
      "t = 33, loss = 5.9686737060546875\n",
      "t = 34, loss = 5.251699924468994\n",
      "t = 35, loss = 5.970834732055664\n",
      "t = 36, loss = 5.252540111541748\n",
      "t = 37, loss = 5.972610950469971\n",
      "t = 38, loss = 5.251795291900635\n",
      "t = 39, loss = 5.973716735839844\n",
      "t = 40, loss = 5.251389980316162\n",
      "t = 41, loss = 5.974460124969482\n",
      "t = 42, loss = 5.251743793487549\n",
      "t = 43, loss = 5.975030422210693\n",
      "t = 44, loss = 5.251838207244873\n",
      "t = 45, loss = 5.975862503051758\n",
      "t = 46, loss = 5.252202033996582\n",
      "t = 47, loss = 5.975996017456055\n",
      "t = 48, loss = 5.252979755401611\n",
      "t = 49, loss = 5.97685432434082\n",
      "t = 50, loss = 5.2532057762146\n",
      "t = 51, loss = 5.976959705352783\n",
      "t = 52, loss = 5.253929138183594\n",
      "t = 53, loss = 5.976650714874268\n",
      "t = 54, loss = 5.254297256469727\n",
      "t = 55, loss = 5.977147579193115\n",
      "t = 56, loss = 5.25523567199707\n",
      "t = 57, loss = 5.977607250213623\n",
      "t = 58, loss = 5.255377292633057\n",
      "t = 59, loss = 5.9778242111206055\n",
      "t = 60, loss = 5.2557477951049805\n",
      "t = 61, loss = 5.978026390075684\n",
      "t = 62, loss = 5.256492614746094\n",
      "t = 63, loss = 5.978436470031738\n",
      "t = 64, loss = 5.257096767425537\n",
      "t = 65, loss = 5.978853702545166\n",
      "t = 66, loss = 5.257617950439453\n",
      "t = 67, loss = 5.978662967681885\n",
      "t = 68, loss = 5.258337020874023\n",
      "t = 69, loss = 5.978601455688477\n",
      "t = 70, loss = 5.25883150100708\n",
      "t = 71, loss = 5.978580951690674\n",
      "t = 72, loss = 5.259516716003418\n",
      "t = 73, loss = 5.979189395904541\n",
      "t = 74, loss = 5.259825706481934\n",
      "t = 75, loss = 5.979170322418213\n",
      "t = 76, loss = 5.260509967803955\n",
      "t = 77, loss = 5.97920560836792\n",
      "t = 78, loss = 5.261186122894287\n",
      "t = 79, loss = 5.979053974151611\n",
      "t = 80, loss = 5.261728286743164\n",
      "t = 81, loss = 5.9790873527526855\n",
      "t = 82, loss = 5.262336254119873\n",
      "t = 83, loss = 5.978780269622803\n",
      "t = 84, loss = 5.26285457611084\n",
      "t = 85, loss = 5.978643894195557\n",
      "t = 86, loss = 5.263606548309326\n",
      "t = 87, loss = 5.97871208190918\n",
      "t = 88, loss = 5.264271259307861\n",
      "t = 89, loss = 5.978127956390381\n",
      "t = 90, loss = 5.265080451965332\n",
      "t = 91, loss = 5.977905750274658\n",
      "t = 92, loss = 5.265960216522217\n",
      "t = 93, loss = 5.977716445922852\n",
      "t = 94, loss = 5.266393661499023\n",
      "t = 95, loss = 5.977628707885742\n",
      "t = 96, loss = 5.267091751098633\n",
      "t = 97, loss = 5.976794719696045\n",
      "t = 98, loss = 5.267841339111328\n",
      "t = 99, loss = 5.976833343505859\n",
      "t = 100, loss = 5.268211364746094\n",
      "t = 101, loss = 4.23271369934082\n",
      "t = 102, loss = 3.298009157180786\n",
      "t = 103, loss = 2.5509588718414307\n",
      "t = 104, loss = 2.0536692142486572\n",
      "t = 105, loss = 1.7851771116256714\n",
      "t = 106, loss = 1.667905569076538\n",
      "t = 107, loss = 1.6237882375717163\n",
      "t = 108, loss = 1.604844570159912\n",
      "t = 109, loss = 1.5928469896316528\n",
      "t = 110, loss = 1.5832678079605103\n",
      "t = 111, loss = 1.5750232934951782\n",
      "t = 112, loss = 1.5677156448364258\n",
      "t = 113, loss = 1.5611180067062378\n",
      "t = 114, loss = 1.5551197528839111\n",
      "t = 115, loss = 1.549636960029602\n",
      "t = 116, loss = 1.5445842742919922\n",
      "t = 117, loss = 1.5399130582809448\n",
      "t = 118, loss = 1.5355727672576904\n",
      "t = 119, loss = 1.531518578529358\n",
      "t = 120, loss = 1.5277107954025269\n",
      "t = 121, loss = 1.524123191833496\n",
      "t = 122, loss = 1.5207279920578003\n",
      "t = 123, loss = 1.5175073146820068\n",
      "t = 124, loss = 1.5144414901733398\n",
      "t = 125, loss = 1.5115125179290771\n",
      "t = 126, loss = 1.5087116956710815\n",
      "t = 127, loss = 1.5060278177261353\n",
      "t = 128, loss = 1.5034536123275757\n",
      "t = 129, loss = 1.5009812116622925\n",
      "t = 130, loss = 1.498603343963623\n",
      "t = 131, loss = 1.4963113069534302\n",
      "t = 132, loss = 1.4940980672836304\n",
      "t = 133, loss = 1.4919586181640625\n",
      "t = 134, loss = 1.4898864030838013\n",
      "t = 135, loss = 1.4878789186477661\n",
      "t = 136, loss = 1.4859330654144287\n",
      "t = 137, loss = 1.4840437173843384\n",
      "t = 138, loss = 1.4822087287902832\n",
      "t = 139, loss = 1.4804258346557617\n",
      "t = 140, loss = 1.478690505027771\n",
      "t = 141, loss = 1.4770023822784424\n",
      "t = 142, loss = 1.475358486175537\n",
      "t = 143, loss = 1.4737548828125\n",
      "t = 144, loss = 1.4721896648406982\n",
      "t = 145, loss = 1.4706614017486572\n",
      "t = 146, loss = 1.4691696166992188\n",
      "t = 147, loss = 1.467711091041565\n",
      "t = 148, loss = 1.4662848711013794\n",
      "t = 149, loss = 1.4648903608322144\n",
      "t = 150, loss = 1.4635261297225952\n",
      "t = 151, loss = 1.46219003200531\n",
      "t = 152, loss = 1.460880994796753\n",
      "t = 153, loss = 1.4595988988876343\n",
      "t = 154, loss = 1.458341360092163\n",
      "t = 155, loss = 1.4571092128753662\n",
      "t = 156, loss = 1.4559009075164795\n",
      "t = 157, loss = 1.4547148942947388\n",
      "t = 158, loss = 1.453550100326538\n",
      "t = 159, loss = 1.4524056911468506\n",
      "t = 160, loss = 1.451280951499939\n",
      "t = 161, loss = 1.4501757621765137\n",
      "t = 162, loss = 1.4490900039672852\n",
      "t = 163, loss = 1.4480220079421997\n",
      "t = 164, loss = 1.446970820426941\n",
      "t = 165, loss = 1.445935606956482\n",
      "t = 166, loss = 1.4449161291122437\n",
      "t = 167, loss = 1.443912386894226\n",
      "t = 168, loss = 1.442923903465271\n",
      "t = 169, loss = 1.4419504404067993\n",
      "t = 170, loss = 1.4409912824630737\n",
      "t = 171, loss = 1.4400458335876465\n",
      "t = 172, loss = 1.4391134977340698\n",
      "t = 173, loss = 1.4381942749023438\n",
      "t = 174, loss = 1.4372873306274414\n",
      "t = 175, loss = 1.4363930225372314\n",
      "t = 176, loss = 1.435510277748108\n",
      "t = 177, loss = 1.4346390962600708\n",
      "t = 178, loss = 1.433779239654541\n",
      "t = 179, loss = 1.432930827140808\n",
      "t = 180, loss = 1.4320932626724243\n",
      "t = 181, loss = 1.4312663078308105\n",
      "t = 182, loss = 1.4304494857788086\n",
      "t = 183, loss = 1.4296422004699707\n",
      "t = 184, loss = 1.4288443326950073\n",
      "t = 185, loss = 1.4280563592910767\n",
      "t = 186, loss = 1.4272775650024414\n",
      "t = 187, loss = 1.426507830619812\n",
      "t = 188, loss = 1.4257473945617676\n",
      "t = 189, loss = 1.4249956607818604\n",
      "t = 190, loss = 1.4242523908615112\n",
      "t = 191, loss = 1.423517107963562\n",
      "t = 192, loss = 1.422789454460144\n",
      "t = 193, loss = 1.4220694303512573\n",
      "t = 194, loss = 1.4213571548461914\n",
      "t = 195, loss = 1.420652151107788\n",
      "t = 196, loss = 1.4199538230895996\n",
      "t = 197, loss = 1.4192627668380737\n",
      "t = 198, loss = 1.4185787439346313\n",
      "t = 199, loss = 1.417901635169983\n",
      "t = 200, loss = 1.4172312021255493\n",
      "t = 201, loss = 1.4171645641326904\n",
      "t = 202, loss = 1.4170981645584106\n",
      "t = 203, loss = 1.4170315265655518\n",
      "t = 204, loss = 1.4169652462005615\n",
      "t = 205, loss = 1.4168988466262817\n",
      "t = 206, loss = 1.416832685470581\n",
      "t = 207, loss = 1.4167665243148804\n",
      "t = 208, loss = 1.4167003631591797\n",
      "t = 209, loss = 1.4166340827941895\n",
      "t = 210, loss = 1.4165682792663574\n",
      "t = 211, loss = 1.4165023565292358\n",
      "t = 212, loss = 1.4164365530014038\n",
      "t = 213, loss = 1.4163708686828613\n",
      "t = 214, loss = 1.4163049459457397\n",
      "t = 215, loss = 1.4162393808364868\n",
      "t = 216, loss = 1.4161736965179443\n",
      "t = 217, loss = 1.4161081314086914\n",
      "t = 218, loss = 1.4160428047180176\n",
      "t = 219, loss = 1.4159773588180542\n",
      "t = 220, loss = 1.4159120321273804\n",
      "t = 221, loss = 1.415846586227417\n",
      "t = 222, loss = 1.4157814979553223\n",
      "t = 223, loss = 1.415716290473938\n",
      "t = 224, loss = 1.4156512022018433\n",
      "t = 225, loss = 1.4155861139297485\n",
      "t = 226, loss = 1.4155210256576538\n",
      "t = 227, loss = 1.4154561758041382\n",
      "t = 228, loss = 1.415391206741333\n",
      "t = 229, loss = 1.415326476097107\n",
      "t = 230, loss = 1.4152617454528809\n",
      "t = 231, loss = 1.4151971340179443\n",
      "t = 232, loss = 1.4151324033737183\n",
      "t = 233, loss = 1.4150679111480713\n",
      "t = 234, loss = 1.4150035381317139\n",
      "t = 235, loss = 1.4149389266967773\n",
      "t = 236, loss = 1.4148746728897095\n",
      "t = 237, loss = 1.4148101806640625\n",
      "t = 238, loss = 1.4147456884384155\n",
      "t = 239, loss = 1.4146816730499268\n",
      "t = 240, loss = 1.4146175384521484\n",
      "t = 241, loss = 1.4145534038543701\n",
      "t = 242, loss = 1.4144893884658813\n",
      "t = 243, loss = 1.4144254922866821\n",
      "t = 244, loss = 1.414361596107483\n",
      "t = 245, loss = 1.4142976999282837\n",
      "t = 246, loss = 1.4142340421676636\n",
      "t = 247, loss = 1.414170265197754\n",
      "t = 248, loss = 1.4141067266464233\n",
      "t = 249, loss = 1.4140429496765137\n",
      "t = 250, loss = 1.413979411125183\n",
      "t = 251, loss = 1.4139161109924316\n",
      "t = 252, loss = 1.4138526916503906\n",
      "t = 253, loss = 1.41378915309906\n",
      "t = 254, loss = 1.4137258529663086\n",
      "t = 255, loss = 1.4136626720428467\n",
      "t = 256, loss = 1.4135994911193848\n",
      "t = 257, loss = 1.4135363101959229\n",
      "t = 258, loss = 1.413473129272461\n",
      "t = 259, loss = 1.4134101867675781\n",
      "t = 260, loss = 1.4133471250534058\n",
      "t = 261, loss = 1.4132843017578125\n",
      "t = 262, loss = 1.4132214784622192\n",
      "t = 263, loss = 1.413158655166626\n",
      "t = 264, loss = 1.4130959510803223\n",
      "t = 265, loss = 1.4130332469940186\n",
      "t = 266, loss = 1.4129703044891357\n",
      "t = 267, loss = 1.4129078388214111\n",
      "t = 268, loss = 1.412845253944397\n",
      "t = 269, loss = 1.4127827882766724\n",
      "t = 270, loss = 1.4127205610275269\n",
      "t = 271, loss = 1.4126582145690918\n",
      "t = 272, loss = 1.4125957489013672\n",
      "t = 273, loss = 1.4125335216522217\n",
      "t = 274, loss = 1.4124714136123657\n",
      "t = 275, loss = 1.4124093055725098\n",
      "t = 276, loss = 1.4123471975326538\n",
      "t = 277, loss = 1.412285327911377\n",
      "t = 278, loss = 1.4122233390808105\n",
      "t = 279, loss = 1.4121615886688232\n",
      "t = 280, loss = 1.4120995998382568\n",
      "t = 281, loss = 1.41203773021698\n",
      "t = 282, loss = 1.4119762182235718\n",
      "t = 283, loss = 1.4119144678115845\n",
      "t = 284, loss = 1.4118527173995972\n",
      "t = 285, loss = 1.4117913246154785\n",
      "t = 286, loss = 1.4117296934127808\n",
      "t = 287, loss = 1.4116684198379517\n",
      "t = 288, loss = 1.411606788635254\n",
      "t = 289, loss = 1.411545753479004\n",
      "t = 290, loss = 1.4114843606948853\n",
      "t = 291, loss = 1.4114232063293457\n",
      "t = 292, loss = 1.4113619327545166\n",
      "t = 293, loss = 1.4113008975982666\n",
      "t = 294, loss = 1.411239743232727\n",
      "t = 295, loss = 1.4111785888671875\n",
      "t = 296, loss = 1.4111177921295166\n",
      "t = 297, loss = 1.4110568761825562\n",
      "t = 298, loss = 1.4109959602355957\n",
      "t = 299, loss = 1.4109352827072144\n",
      "t = 300, loss = 1.4108744859695435\n",
      "t = 301, loss = 1.410868525505066\n",
      "t = 302, loss = 1.4108623266220093\n",
      "t = 303, loss = 1.4108562469482422\n",
      "t = 304, loss = 1.4108502864837646\n",
      "t = 305, loss = 1.4108442068099976\n",
      "t = 306, loss = 1.41083824634552\n",
      "t = 307, loss = 1.4108320474624634\n",
      "t = 308, loss = 1.4108259677886963\n",
      "t = 309, loss = 1.4108198881149292\n",
      "t = 310, loss = 1.4108136892318726\n",
      "t = 311, loss = 1.4108078479766846\n",
      "t = 312, loss = 1.410801649093628\n",
      "t = 313, loss = 1.4107956886291504\n",
      "t = 314, loss = 1.4107896089553833\n",
      "t = 315, loss = 1.4107836484909058\n",
      "t = 316, loss = 1.4107775688171387\n",
      "t = 317, loss = 1.4107714891433716\n",
      "t = 318, loss = 1.410765290260315\n",
      "t = 319, loss = 1.4107590913772583\n",
      "t = 320, loss = 1.4107531309127808\n",
      "t = 321, loss = 1.4107470512390137\n",
      "t = 322, loss = 1.4107410907745361\n",
      "t = 323, loss = 1.410735011100769\n",
      "t = 324, loss = 1.4107290506362915\n",
      "t = 325, loss = 1.4107229709625244\n",
      "t = 326, loss = 1.4107168912887573\n",
      "t = 327, loss = 1.4107109308242798\n",
      "t = 328, loss = 1.4107048511505127\n",
      "t = 329, loss = 1.4106988906860352\n",
      "t = 330, loss = 1.4106926918029785\n",
      "t = 331, loss = 1.4106866121292114\n",
      "t = 332, loss = 1.4106807708740234\n",
      "t = 333, loss = 1.4106745719909668\n",
      "t = 334, loss = 1.4106684923171997\n",
      "t = 335, loss = 1.410662293434143\n",
      "t = 336, loss = 1.4106563329696655\n",
      "t = 337, loss = 1.4106502532958984\n",
      "t = 338, loss = 1.410644292831421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 339, loss = 1.4106382131576538\n",
      "t = 340, loss = 1.4106321334838867\n",
      "t = 341, loss = 1.4106261730194092\n",
      "t = 342, loss = 1.410620093345642\n",
      "t = 343, loss = 1.4106141328811646\n",
      "t = 344, loss = 1.4106080532073975\n",
      "t = 345, loss = 1.4106019735336304\n",
      "t = 346, loss = 1.4105957746505737\n",
      "t = 347, loss = 1.4105898141860962\n",
      "t = 348, loss = 1.4105839729309082\n",
      "t = 349, loss = 1.4105777740478516\n",
      "t = 350, loss = 1.4105716943740845\n",
      "t = 351, loss = 1.4105658531188965\n",
      "t = 352, loss = 1.4105596542358398\n",
      "t = 353, loss = 1.4105535745620728\n",
      "t = 354, loss = 1.4105476140975952\n",
      "t = 355, loss = 1.4105415344238281\n",
      "t = 356, loss = 1.4105355739593506\n",
      "t = 357, loss = 1.4105294942855835\n",
      "t = 358, loss = 1.4105234146118164\n",
      "t = 359, loss = 1.4105174541473389\n",
      "t = 360, loss = 1.4105113744735718\n",
      "t = 361, loss = 1.4105051755905151\n",
      "t = 362, loss = 1.4104992151260376\n",
      "t = 363, loss = 1.41049325466156\n",
      "t = 364, loss = 1.4104870557785034\n",
      "t = 365, loss = 1.4104810953140259\n",
      "t = 366, loss = 1.410475254058838\n",
      "t = 367, loss = 1.4104690551757812\n",
      "t = 368, loss = 1.4104629755020142\n",
      "t = 369, loss = 1.410456895828247\n",
      "t = 370, loss = 1.410451054573059\n",
      "t = 371, loss = 1.4104448556900024\n",
      "t = 372, loss = 1.410438895225525\n",
      "t = 373, loss = 1.4104329347610474\n",
      "t = 374, loss = 1.4104268550872803\n",
      "t = 375, loss = 1.4104207754135132\n",
      "t = 376, loss = 1.4104149341583252\n",
      "t = 377, loss = 1.4104087352752686\n",
      "t = 378, loss = 1.4104026556015015\n",
      "t = 379, loss = 1.4103968143463135\n",
      "t = 380, loss = 1.4103907346725464\n",
      "t = 381, loss = 1.4103847742080688\n",
      "t = 382, loss = 1.4103786945343018\n",
      "t = 383, loss = 1.4103727340698242\n",
      "t = 384, loss = 1.4103665351867676\n",
      "t = 385, loss = 1.41036057472229\n",
      "t = 386, loss = 1.4103546142578125\n",
      "t = 387, loss = 1.4103485345840454\n",
      "t = 388, loss = 1.4103426933288574\n",
      "t = 389, loss = 1.4103364944458008\n",
      "t = 390, loss = 1.4103304147720337\n",
      "t = 391, loss = 1.4103245735168457\n",
      "t = 392, loss = 1.4103186130523682\n",
      "t = 393, loss = 1.4103124141693115\n",
      "t = 394, loss = 1.4103063344955444\n",
      "t = 395, loss = 1.4103004932403564\n",
      "t = 396, loss = 1.4102944135665894\n",
      "t = 397, loss = 1.4102884531021118\n",
      "t = 398, loss = 1.4102823734283447\n",
      "t = 399, loss = 1.4102762937545776\n",
      "t = 400, loss = 1.4102703332901\n",
      "t = 401, loss = 1.4102696180343628\n",
      "t = 402, loss = 1.4102691411972046\n",
      "t = 403, loss = 1.4102685451507568\n",
      "t = 404, loss = 1.410267949104309\n",
      "t = 405, loss = 1.4102672338485718\n",
      "t = 406, loss = 1.4102667570114136\n",
      "t = 407, loss = 1.4102660417556763\n",
      "t = 408, loss = 1.410265564918518\n",
      "t = 409, loss = 1.4102648496627808\n",
      "t = 410, loss = 1.410264253616333\n",
      "t = 411, loss = 1.4102636575698853\n",
      "t = 412, loss = 1.4102630615234375\n",
      "t = 413, loss = 1.4102623462677002\n",
      "t = 414, loss = 1.410261869430542\n",
      "t = 415, loss = 1.4102613925933838\n",
      "t = 416, loss = 1.410260796546936\n",
      "t = 417, loss = 1.4102602005004883\n",
      "t = 418, loss = 1.410259485244751\n",
      "t = 419, loss = 1.4102588891983032\n",
      "t = 420, loss = 1.4102582931518555\n",
      "t = 421, loss = 1.4102575778961182\n",
      "t = 422, loss = 1.4102569818496704\n",
      "t = 423, loss = 1.4102565050125122\n",
      "t = 424, loss = 1.4102559089660645\n",
      "t = 425, loss = 1.4102554321289062\n",
      "t = 426, loss = 1.410254716873169\n",
      "t = 427, loss = 1.4102541208267212\n",
      "t = 428, loss = 1.4102535247802734\n",
      "t = 429, loss = 1.4102530479431152\n",
      "t = 430, loss = 1.410252332687378\n",
      "t = 431, loss = 1.4102517366409302\n",
      "t = 432, loss = 1.4102511405944824\n",
      "t = 433, loss = 1.4102504253387451\n",
      "t = 434, loss = 1.4102498292922974\n",
      "t = 435, loss = 1.4102492332458496\n",
      "t = 436, loss = 1.4102485179901123\n",
      "t = 437, loss = 1.410248041152954\n",
      "t = 438, loss = 1.4102474451065063\n",
      "t = 439, loss = 1.4102468490600586\n",
      "t = 440, loss = 1.4102461338043213\n",
      "t = 441, loss = 1.4102455377578735\n",
      "t = 442, loss = 1.4102449417114258\n",
      "t = 443, loss = 1.4102445840835571\n",
      "t = 444, loss = 1.4102439880371094\n",
      "t = 445, loss = 1.410243272781372\n",
      "t = 446, loss = 1.4102426767349243\n",
      "t = 447, loss = 1.4102420806884766\n",
      "t = 448, loss = 1.4102413654327393\n",
      "t = 449, loss = 1.4102407693862915\n",
      "t = 450, loss = 1.4102401733398438\n",
      "t = 451, loss = 1.4102394580841064\n",
      "t = 452, loss = 1.4102389812469482\n",
      "t = 453, loss = 1.41023850440979\n",
      "t = 454, loss = 1.4102379083633423\n",
      "t = 455, loss = 1.4102373123168945\n",
      "t = 456, loss = 1.4102367162704468\n",
      "t = 457, loss = 1.4102360010147095\n",
      "t = 458, loss = 1.4102354049682617\n",
      "t = 459, loss = 1.4102349281311035\n",
      "t = 460, loss = 1.4102343320846558\n",
      "t = 461, loss = 1.4102338552474976\n",
      "t = 462, loss = 1.4102331399917603\n",
      "t = 463, loss = 1.4102325439453125\n",
      "t = 464, loss = 1.4102319478988647\n",
      "t = 465, loss = 1.4102312326431274\n",
      "t = 466, loss = 1.4102306365966797\n",
      "t = 467, loss = 1.4102301597595215\n",
      "t = 468, loss = 1.4102295637130737\n",
      "t = 469, loss = 1.4102288484573364\n",
      "t = 470, loss = 1.4102282524108887\n",
      "t = 471, loss = 1.410227656364441\n",
      "t = 472, loss = 1.4102269411087036\n",
      "t = 473, loss = 1.4102263450622559\n",
      "t = 474, loss = 1.4102258682250977\n",
      "t = 475, loss = 1.41022527217865\n",
      "t = 476, loss = 1.4102245569229126\n",
      "t = 477, loss = 1.4102239608764648\n",
      "t = 478, loss = 1.410223364830017\n",
      "t = 479, loss = 1.4102226495742798\n",
      "t = 480, loss = 1.4102221727371216\n",
      "t = 481, loss = 1.4102215766906738\n",
      "t = 482, loss = 1.4102210998535156\n",
      "t = 483, loss = 1.4102205038070679\n",
      "t = 484, loss = 1.4102197885513306\n",
      "t = 485, loss = 1.4102191925048828\n",
      "t = 486, loss = 1.410218596458435\n",
      "t = 487, loss = 1.4102178812026978\n",
      "t = 488, loss = 1.4102174043655396\n",
      "t = 489, loss = 1.4102168083190918\n",
      "t = 490, loss = 1.4102163314819336\n",
      "t = 491, loss = 1.4102157354354858\n",
      "t = 492, loss = 1.4102150201797485\n",
      "t = 493, loss = 1.4102144241333008\n",
      "t = 494, loss = 1.410213828086853\n",
      "t = 495, loss = 1.4102131128311157\n",
      "t = 496, loss = 1.410212516784668\n",
      "t = 497, loss = 1.4102120399475098\n",
      "t = 498, loss = 1.410211443901062\n",
      "t = 499, loss = 1.4102107286453247\n",
      "Training R2:  0.675929643859005\n",
      "Test R2:  0.6729996464835437\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_feat = len(features_cols)\n",
    "out_feat = len(targetColumns)\n",
    "model = Linear_Net(in_features=in_feat ,out_features=out_feat).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=100, gamma=0.1)\n",
    "\n",
    "losses,model = train(model, optimiser, scheduler, criterion, epochs = 500)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "    y_pred_train = model(X_train)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "\n",
    "train_r2 = r2_score(y_train.detach().cpu().numpy(), y_pred_train.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "test_r2=r2_score(y_test.detach().cpu().numpy(), y_pred_test.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "\n",
    "print(\"Training R2: \",train_r2)\n",
    "print(\"Test R2: \",test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 2.7944319248199463\n",
      "t = 1, loss = 2.498420000076294\n",
      "t = 2, loss = 2.4060139656066895\n",
      "t = 3, loss = 2.3215103149414062\n",
      "t = 4, loss = 2.2283782958984375\n",
      "t = 5, loss = 2.1459484100341797\n",
      "t = 6, loss = 2.072981595993042\n",
      "t = 7, loss = 2.0684597492218018\n",
      "t = 8, loss = 1.9331071376800537\n",
      "t = 9, loss = 1.8692506551742554\n",
      "t = 10, loss = 1.753777265548706\n",
      "t = 11, loss = 1.7514225244522095\n",
      "t = 12, loss = 1.676851749420166\n",
      "t = 13, loss = 1.6740343570709229\n",
      "t = 14, loss = 1.6412805318832397\n",
      "t = 15, loss = 1.628854513168335\n",
      "t = 16, loss = 1.5801807641983032\n",
      "t = 17, loss = 1.5616896152496338\n",
      "t = 18, loss = 1.4869636297225952\n",
      "t = 19, loss = 1.505552053451538\n",
      "t = 20, loss = 1.4576793909072876\n",
      "t = 21, loss = 1.462951421737671\n",
      "t = 22, loss = 1.40561842918396\n",
      "t = 23, loss = 1.4260287284851074\n",
      "t = 24, loss = 1.383133888244629\n",
      "t = 25, loss = 1.4035863876342773\n",
      "t = 26, loss = 1.3585346937179565\n",
      "t = 27, loss = 1.4814395904541016\n",
      "t = 28, loss = 1.4916157722473145\n",
      "t = 29, loss = 1.9870115518569946\n",
      "t = 30, loss = 1.311262607574463\n",
      "t = 31, loss = 1.4355729818344116\n",
      "t = 32, loss = 1.3176493644714355\n",
      "t = 33, loss = 1.4906891584396362\n",
      "t = 34, loss = 1.3679991960525513\n",
      "t = 35, loss = 1.7028329372406006\n",
      "t = 36, loss = 1.353692889213562\n",
      "t = 37, loss = 1.6624375581741333\n",
      "t = 38, loss = 1.3275692462921143\n",
      "t = 39, loss = 1.567215085029602\n",
      "t = 40, loss = 1.3096063137054443\n",
      "t = 41, loss = 1.5516247749328613\n",
      "t = 42, loss = 1.3041105270385742\n",
      "t = 43, loss = 1.5435160398483276\n",
      "t = 44, loss = 1.299383282661438\n",
      "t = 45, loss = 1.542301893234253\n",
      "t = 46, loss = 1.2939186096191406\n",
      "t = 47, loss = 1.5390212535858154\n",
      "t = 48, loss = 1.27786123752594\n",
      "t = 49, loss = 1.4852383136749268\n",
      "t = 50, loss = 1.2974305152893066\n",
      "t = 51, loss = 1.555933952331543\n",
      "t = 52, loss = 1.27390456199646\n",
      "t = 53, loss = 1.5096409320831299\n",
      "t = 54, loss = 1.2783278226852417\n",
      "t = 55, loss = 1.5149791240692139\n",
      "t = 56, loss = 1.2635612487792969\n",
      "t = 57, loss = 1.4866949319839478\n",
      "t = 58, loss = 1.2842501401901245\n",
      "t = 59, loss = 1.5418047904968262\n",
      "t = 60, loss = 1.2518041133880615\n",
      "t = 61, loss = 1.4726569652557373\n",
      "t = 62, loss = 1.2820054292678833\n",
      "t = 63, loss = 1.5572502613067627\n",
      "t = 64, loss = 1.2257636785507202\n",
      "t = 65, loss = 1.4201170206069946\n",
      "t = 66, loss = 1.2736190557479858\n",
      "t = 67, loss = 1.5425748825073242\n",
      "t = 68, loss = 1.232612133026123\n",
      "t = 69, loss = 1.4542696475982666\n",
      "t = 70, loss = 1.2584452629089355\n",
      "t = 71, loss = 1.5347557067871094\n",
      "t = 72, loss = 1.2096611261367798\n",
      "t = 73, loss = 1.4220340251922607\n",
      "t = 74, loss = 1.251556396484375\n",
      "t = 75, loss = 1.5185331106185913\n",
      "t = 76, loss = 1.215947151184082\n",
      "t = 77, loss = 1.4449130296707153\n",
      "t = 78, loss = 1.2419047355651855\n",
      "t = 79, loss = 1.5096979141235352\n",
      "t = 80, loss = 1.2002454996109009\n",
      "t = 81, loss = 1.4190428256988525\n",
      "t = 82, loss = 1.2597413063049316\n",
      "t = 83, loss = 1.559383749961853\n",
      "t = 84, loss = 1.171687126159668\n",
      "t = 85, loss = 1.3672220706939697\n",
      "t = 86, loss = 1.2265496253967285\n",
      "t = 87, loss = 1.5112993717193604\n",
      "t = 88, loss = 1.1918621063232422\n",
      "t = 89, loss = 1.4392340183258057\n",
      "t = 90, loss = 1.219590663909912\n",
      "t = 91, loss = 1.505158543586731\n",
      "t = 92, loss = 1.1828546524047852\n",
      "t = 93, loss = 1.4094067811965942\n",
      "t = 94, loss = 1.218888759613037\n",
      "t = 95, loss = 1.5145015716552734\n",
      "t = 96, loss = 1.1589456796646118\n",
      "t = 97, loss = 1.358834981918335\n",
      "t = 98, loss = 1.2217681407928467\n",
      "t = 99, loss = 1.5311477184295654\n",
      "t = 100, loss = 1.1435869932174683\n",
      "t = 101, loss = 1.0950500965118408\n",
      "t = 102, loss = 1.0725244283676147\n",
      "t = 103, loss = 1.0619263648986816\n",
      "t = 104, loss = 1.0564552545547485\n",
      "t = 105, loss = 1.053434133529663\n",
      "t = 106, loss = 1.0516141653060913\n",
      "t = 107, loss = 1.050323724746704\n",
      "t = 108, loss = 1.0493214130401611\n",
      "t = 109, loss = 1.048493504524231\n",
      "t = 110, loss = 1.0477784872055054\n",
      "t = 111, loss = 1.0471470355987549\n",
      "t = 112, loss = 1.0465806722640991\n",
      "t = 113, loss = 1.0460608005523682\n",
      "t = 114, loss = 1.0455741882324219\n",
      "t = 115, loss = 1.0451127290725708\n",
      "t = 116, loss = 1.0446715354919434\n",
      "t = 117, loss = 1.0442465543746948\n",
      "t = 118, loss = 1.0438365936279297\n",
      "t = 119, loss = 1.0434389114379883\n",
      "t = 120, loss = 1.043052315711975\n",
      "t = 121, loss = 1.042675256729126\n",
      "t = 122, loss = 1.0423073768615723\n",
      "t = 123, loss = 1.0419479608535767\n",
      "t = 124, loss = 1.0415959358215332\n",
      "t = 125, loss = 1.041250228881836\n",
      "t = 126, loss = 1.040910005569458\n",
      "t = 127, loss = 1.0405758619308472\n",
      "t = 128, loss = 1.0402480363845825\n",
      "t = 129, loss = 1.039925456047058\n",
      "t = 130, loss = 1.0396080017089844\n",
      "t = 131, loss = 1.0392954349517822\n",
      "t = 132, loss = 1.0389870405197144\n",
      "t = 133, loss = 1.0386829376220703\n",
      "t = 134, loss = 1.038382887840271\n",
      "t = 135, loss = 1.0380864143371582\n",
      "t = 136, loss = 1.037793755531311\n",
      "t = 137, loss = 1.03750479221344\n",
      "t = 138, loss = 1.037219762802124\n",
      "t = 139, loss = 1.0369377136230469\n",
      "t = 140, loss = 1.036658763885498\n",
      "t = 141, loss = 1.0363825559616089\n",
      "t = 142, loss = 1.0361090898513794\n",
      "t = 143, loss = 1.0358386039733887\n",
      "t = 144, loss = 1.0355709791183472\n",
      "t = 145, loss = 1.0353060960769653\n",
      "t = 146, loss = 1.035043716430664\n",
      "t = 147, loss = 1.034783959388733\n",
      "t = 148, loss = 1.0345268249511719\n",
      "t = 149, loss = 1.0342717170715332\n",
      "t = 150, loss = 1.034018874168396\n",
      "t = 151, loss = 1.0337679386138916\n",
      "t = 152, loss = 1.0335196256637573\n",
      "t = 153, loss = 1.0332738161087036\n",
      "t = 154, loss = 1.033030390739441\n",
      "t = 155, loss = 1.0327892303466797\n",
      "t = 156, loss = 1.03255033493042\n",
      "t = 157, loss = 1.0323129892349243\n",
      "t = 158, loss = 1.0320777893066406\n",
      "t = 159, loss = 1.0318444967269897\n",
      "t = 160, loss = 1.0316128730773926\n",
      "t = 161, loss = 1.0313830375671387\n",
      "t = 162, loss = 1.031154751777649\n",
      "t = 163, loss = 1.0309282541275024\n",
      "t = 164, loss = 1.0307039022445679\n",
      "t = 165, loss = 1.0304813385009766\n",
      "t = 166, loss = 1.030260443687439\n",
      "t = 167, loss = 1.030040979385376\n",
      "t = 168, loss = 1.0298233032226562\n",
      "t = 169, loss = 1.0296072959899902\n",
      "t = 170, loss = 1.0293933153152466\n",
      "t = 171, loss = 1.029180884361267\n",
      "t = 172, loss = 1.0289695262908936\n",
      "t = 173, loss = 1.0287598371505737\n",
      "t = 174, loss = 1.0285519361495972\n",
      "t = 175, loss = 1.0283459424972534\n",
      "t = 176, loss = 1.0281414985656738\n",
      "t = 177, loss = 1.0279383659362793\n",
      "t = 178, loss = 1.027737021446228\n",
      "t = 179, loss = 1.027537226676941\n",
      "t = 180, loss = 1.0273387432098389\n",
      "t = 181, loss = 1.0271419286727905\n",
      "t = 182, loss = 1.026946783065796\n",
      "t = 183, loss = 1.0267529487609863\n",
      "t = 184, loss = 1.0265603065490723\n",
      "t = 185, loss = 1.0263690948486328\n",
      "t = 186, loss = 1.0261791944503784\n",
      "t = 187, loss = 1.025990605354309\n",
      "t = 188, loss = 1.0258030891418457\n",
      "t = 189, loss = 1.0256167650222778\n",
      "t = 190, loss = 1.0254316329956055\n",
      "t = 191, loss = 1.0252476930618286\n",
      "t = 192, loss = 1.0250648260116577\n",
      "t = 193, loss = 1.0248830318450928\n",
      "t = 194, loss = 1.024702548980713\n",
      "t = 195, loss = 1.0245230197906494\n",
      "t = 196, loss = 1.0243444442749023\n",
      "t = 197, loss = 1.0241672992706299\n",
      "t = 198, loss = 1.0239912271499634\n",
      "t = 199, loss = 1.0238162279129028\n",
      "t = 200, loss = 1.0236419439315796\n",
      "t = 201, loss = 1.0236246585845947\n",
      "t = 202, loss = 1.0236073732376099\n",
      "t = 203, loss = 1.0235899686813354\n",
      "t = 204, loss = 1.0235726833343506\n",
      "t = 205, loss = 1.0235552787780762\n",
      "t = 206, loss = 1.0235379934310913\n",
      "t = 207, loss = 1.0235207080841064\n",
      "t = 208, loss = 1.0235034227371216\n",
      "t = 209, loss = 1.0234861373901367\n",
      "t = 210, loss = 1.0234689712524414\n",
      "t = 211, loss = 1.0234516859054565\n",
      "t = 212, loss = 1.0234344005584717\n",
      "t = 213, loss = 1.0234171152114868\n",
      "t = 214, loss = 1.0233999490737915\n",
      "t = 215, loss = 1.0233826637268066\n",
      "t = 216, loss = 1.0233654975891113\n",
      "t = 217, loss = 1.023348331451416\n",
      "t = 218, loss = 1.0233310461044312\n",
      "t = 219, loss = 1.0233137607574463\n",
      "t = 220, loss = 1.0232967138290405\n",
      "t = 221, loss = 1.0232794284820557\n",
      "t = 222, loss = 1.02326238155365\n",
      "t = 223, loss = 1.0232452154159546\n",
      "t = 224, loss = 1.0232279300689697\n",
      "t = 225, loss = 1.023210883140564\n",
      "t = 226, loss = 1.0231938362121582\n",
      "t = 227, loss = 1.023176670074463\n",
      "t = 228, loss = 1.0231595039367676\n",
      "t = 229, loss = 1.0231423377990723\n",
      "t = 230, loss = 1.0231252908706665\n",
      "t = 231, loss = 1.0231082439422607\n",
      "t = 232, loss = 1.023091197013855\n",
      "t = 233, loss = 1.0230741500854492\n",
      "t = 234, loss = 1.0230571031570435\n",
      "t = 235, loss = 1.0230401754379272\n",
      "t = 236, loss = 1.0230231285095215\n",
      "t = 237, loss = 1.0230059623718262\n",
      "t = 238, loss = 1.02298903465271\n",
      "t = 239, loss = 1.0229721069335938\n",
      "t = 240, loss = 1.0229551792144775\n",
      "t = 241, loss = 1.0229382514953613\n",
      "t = 242, loss = 1.0229213237762451\n",
      "t = 243, loss = 1.0229042768478394\n",
      "t = 244, loss = 1.0228873491287231\n",
      "t = 245, loss = 1.022870421409607\n",
      "t = 246, loss = 1.0228534936904907\n",
      "t = 247, loss = 1.0228365659713745\n",
      "t = 248, loss = 1.0228196382522583\n",
      "t = 249, loss = 1.022802710533142\n",
      "t = 250, loss = 1.0227859020233154\n",
      "t = 251, loss = 1.0227689743041992\n",
      "t = 252, loss = 1.0227521657943726\n",
      "t = 253, loss = 1.0227352380752563\n",
      "t = 254, loss = 1.0227184295654297\n",
      "t = 255, loss = 1.022701621055603\n",
      "t = 256, loss = 1.0226848125457764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 257, loss = 1.0226681232452393\n",
      "t = 258, loss = 1.0226513147354126\n",
      "t = 259, loss = 1.022634506225586\n",
      "t = 260, loss = 1.0226176977157593\n",
      "t = 261, loss = 1.0226010084152222\n",
      "t = 262, loss = 1.022584319114685\n",
      "t = 263, loss = 1.0225675106048584\n",
      "t = 264, loss = 1.0225507020950317\n",
      "t = 265, loss = 1.0225341320037842\n",
      "t = 266, loss = 1.0225173234939575\n",
      "t = 267, loss = 1.0225006341934204\n",
      "t = 268, loss = 1.0224840641021729\n",
      "t = 269, loss = 1.0224673748016357\n",
      "t = 270, loss = 1.0224508047103882\n",
      "t = 271, loss = 1.022434115409851\n",
      "t = 272, loss = 1.0224175453186035\n",
      "t = 273, loss = 1.0224008560180664\n",
      "t = 274, loss = 1.0223842859268188\n",
      "t = 275, loss = 1.0223677158355713\n",
      "t = 276, loss = 1.0223510265350342\n",
      "t = 277, loss = 1.0223344564437866\n",
      "t = 278, loss = 1.0223180055618286\n",
      "t = 279, loss = 1.022301435470581\n",
      "t = 280, loss = 1.0222848653793335\n",
      "t = 281, loss = 1.022268295288086\n",
      "t = 282, loss = 1.022251844406128\n",
      "t = 283, loss = 1.0222352743148804\n",
      "t = 284, loss = 1.0222188234329224\n",
      "t = 285, loss = 1.0222022533416748\n",
      "t = 286, loss = 1.0221858024597168\n",
      "t = 287, loss = 1.0221692323684692\n",
      "t = 288, loss = 1.0221526622772217\n",
      "t = 289, loss = 1.0221362113952637\n",
      "t = 290, loss = 1.0221197605133057\n",
      "t = 291, loss = 1.0221033096313477\n",
      "t = 292, loss = 1.0220868587493896\n",
      "t = 293, loss = 1.0220704078674316\n",
      "t = 294, loss = 1.0220539569854736\n",
      "t = 295, loss = 1.0220375061035156\n",
      "t = 296, loss = 1.0220211744308472\n",
      "t = 297, loss = 1.0220047235488892\n",
      "t = 298, loss = 1.0219883918762207\n",
      "t = 299, loss = 1.0219719409942627\n",
      "t = 300, loss = 1.0219556093215942\n",
      "t = 301, loss = 1.0219539403915405\n",
      "t = 302, loss = 1.0219522714614868\n",
      "t = 303, loss = 1.0219507217407227\n",
      "t = 304, loss = 1.021949052810669\n",
      "t = 305, loss = 1.0219473838806152\n",
      "t = 306, loss = 1.021945834159851\n",
      "t = 307, loss = 1.0219441652297974\n",
      "t = 308, loss = 1.0219424962997437\n",
      "t = 309, loss = 1.02194082736969\n",
      "t = 310, loss = 1.0219392776489258\n",
      "t = 311, loss = 1.021937608718872\n",
      "t = 312, loss = 1.0219359397888184\n",
      "t = 313, loss = 1.0219342708587646\n",
      "t = 314, loss = 1.0219327211380005\n",
      "t = 315, loss = 1.0219310522079468\n",
      "t = 316, loss = 1.0219295024871826\n",
      "t = 317, loss = 1.021927833557129\n",
      "t = 318, loss = 1.0219261646270752\n",
      "t = 319, loss = 1.0219244956970215\n",
      "t = 320, loss = 1.0219228267669678\n",
      "t = 321, loss = 1.0219212770462036\n",
      "t = 322, loss = 1.02191960811615\n",
      "t = 323, loss = 1.0219179391860962\n",
      "t = 324, loss = 1.0219162702560425\n",
      "t = 325, loss = 1.0219147205352783\n",
      "t = 326, loss = 1.0219130516052246\n",
      "t = 327, loss = 1.021911382675171\n",
      "t = 328, loss = 1.0219098329544067\n",
      "t = 329, loss = 1.021908164024353\n",
      "t = 330, loss = 1.0219064950942993\n",
      "t = 331, loss = 1.0219048261642456\n",
      "t = 332, loss = 1.0219032764434814\n",
      "t = 333, loss = 1.0219016075134277\n",
      "t = 334, loss = 1.021899938583374\n",
      "t = 335, loss = 1.0218983888626099\n",
      "t = 336, loss = 1.0218967199325562\n",
      "t = 337, loss = 1.0218950510025024\n",
      "t = 338, loss = 1.0218933820724487\n",
      "t = 339, loss = 1.0218918323516846\n",
      "t = 340, loss = 1.0218901634216309\n",
      "t = 341, loss = 1.0218886137008667\n",
      "t = 342, loss = 1.021886944770813\n",
      "t = 343, loss = 1.0218851566314697\n",
      "t = 344, loss = 1.0218837261199951\n",
      "t = 345, loss = 1.0218820571899414\n",
      "t = 346, loss = 1.0218803882598877\n",
      "t = 347, loss = 1.021878719329834\n",
      "t = 348, loss = 1.0218770503997803\n",
      "t = 349, loss = 1.0218755006790161\n",
      "t = 350, loss = 1.0218738317489624\n",
      "t = 351, loss = 1.0218722820281982\n",
      "t = 352, loss = 1.0218706130981445\n",
      "t = 353, loss = 1.0218689441680908\n",
      "t = 354, loss = 1.021867275238037\n",
      "t = 355, loss = 1.0218656063079834\n",
      "t = 356, loss = 1.0218640565872192\n",
      "t = 357, loss = 1.021862506866455\n",
      "t = 358, loss = 1.0218607187271118\n",
      "t = 359, loss = 1.0218591690063477\n",
      "t = 360, loss = 1.0218576192855835\n",
      "t = 361, loss = 1.0218558311462402\n",
      "t = 362, loss = 1.021854281425476\n",
      "t = 363, loss = 1.0218526124954224\n",
      "t = 364, loss = 1.0218510627746582\n",
      "t = 365, loss = 1.0218493938446045\n",
      "t = 366, loss = 1.0218477249145508\n",
      "t = 367, loss = 1.0218461751937866\n",
      "t = 368, loss = 1.021844506263733\n",
      "t = 369, loss = 1.0218428373336792\n",
      "t = 370, loss = 1.0218411684036255\n",
      "t = 371, loss = 1.0218394994735718\n",
      "t = 372, loss = 1.0218379497528076\n",
      "t = 373, loss = 1.021836280822754\n",
      "t = 374, loss = 1.0218347311019897\n",
      "t = 375, loss = 1.021833062171936\n",
      "t = 376, loss = 1.0218313932418823\n",
      "t = 377, loss = 1.0218298435211182\n",
      "t = 378, loss = 1.0218281745910645\n",
      "t = 379, loss = 1.0218265056610107\n",
      "t = 380, loss = 1.0218249559402466\n",
      "t = 381, loss = 1.0218232870101929\n",
      "t = 382, loss = 1.0218217372894287\n",
      "t = 383, loss = 1.021820068359375\n",
      "t = 384, loss = 1.0218183994293213\n",
      "t = 385, loss = 1.0218167304992676\n",
      "t = 386, loss = 1.0218150615692139\n",
      "t = 387, loss = 1.0218133926391602\n",
      "t = 388, loss = 1.021811842918396\n",
      "t = 389, loss = 1.0218102931976318\n",
      "t = 390, loss = 1.0218086242675781\n",
      "t = 391, loss = 1.0218069553375244\n",
      "t = 392, loss = 1.0218052864074707\n",
      "t = 393, loss = 1.0218037366867065\n",
      "t = 394, loss = 1.0218020677566528\n",
      "t = 395, loss = 1.0218005180358887\n",
      "t = 396, loss = 1.021798849105835\n",
      "t = 397, loss = 1.0217972993850708\n",
      "t = 398, loss = 1.021795630455017\n",
      "t = 399, loss = 1.0217939615249634\n",
      "t = 400, loss = 1.0217922925949097\n",
      "t = 401, loss = 1.0217921733856201\n",
      "t = 402, loss = 1.0217920541763306\n",
      "t = 403, loss = 1.0217918157577515\n",
      "t = 404, loss = 1.021791696548462\n",
      "t = 405, loss = 1.0217915773391724\n",
      "t = 406, loss = 1.0217913389205933\n",
      "t = 407, loss = 1.0217912197113037\n",
      "t = 408, loss = 1.0217911005020142\n",
      "t = 409, loss = 1.021790862083435\n",
      "t = 410, loss = 1.0217907428741455\n",
      "t = 411, loss = 1.021790623664856\n",
      "t = 412, loss = 1.0217903852462769\n",
      "t = 413, loss = 1.0217901468276978\n",
      "t = 414, loss = 1.0217901468276978\n",
      "t = 415, loss = 1.0217899084091187\n",
      "t = 416, loss = 1.021789789199829\n",
      "t = 417, loss = 1.0217896699905396\n",
      "t = 418, loss = 1.0217894315719604\n",
      "t = 419, loss = 1.021789312362671\n",
      "t = 420, loss = 1.0217891931533813\n",
      "t = 421, loss = 1.0217889547348022\n",
      "t = 422, loss = 1.0217888355255127\n",
      "t = 423, loss = 1.0217887163162231\n",
      "t = 424, loss = 1.021788477897644\n",
      "t = 425, loss = 1.021788239479065\n",
      "t = 426, loss = 1.021788239479065\n",
      "t = 427, loss = 1.0217880010604858\n",
      "t = 428, loss = 1.0217878818511963\n",
      "t = 429, loss = 1.0217877626419067\n",
      "t = 430, loss = 1.0217875242233276\n",
      "t = 431, loss = 1.021787405014038\n",
      "t = 432, loss = 1.0217872858047485\n",
      "t = 433, loss = 1.0217870473861694\n",
      "t = 434, loss = 1.0217868089675903\n",
      "t = 435, loss = 1.0217866897583008\n",
      "t = 436, loss = 1.0217865705490112\n",
      "t = 437, loss = 1.0217863321304321\n",
      "t = 438, loss = 1.0217862129211426\n",
      "t = 439, loss = 1.021786093711853\n",
      "t = 440, loss = 1.021785855293274\n",
      "t = 441, loss = 1.0217857360839844\n",
      "t = 442, loss = 1.0217856168746948\n",
      "t = 443, loss = 1.0217853784561157\n",
      "t = 444, loss = 1.0217852592468262\n",
      "t = 445, loss = 1.0217851400375366\n",
      "t = 446, loss = 1.0217849016189575\n",
      "t = 447, loss = 1.021784782409668\n",
      "t = 448, loss = 1.0217846632003784\n",
      "t = 449, loss = 1.0217844247817993\n",
      "t = 450, loss = 1.0217841863632202\n",
      "t = 451, loss = 1.0217841863632202\n",
      "t = 452, loss = 1.0217839479446411\n",
      "t = 453, loss = 1.021783709526062\n",
      "t = 454, loss = 1.0217835903167725\n",
      "t = 455, loss = 1.021783471107483\n",
      "t = 456, loss = 1.0217833518981934\n",
      "t = 457, loss = 1.0217832326889038\n",
      "t = 458, loss = 1.0217829942703247\n",
      "t = 459, loss = 1.0217827558517456\n",
      "t = 460, loss = 1.021782636642456\n",
      "t = 461, loss = 1.0217825174331665\n",
      "t = 462, loss = 1.021782398223877\n",
      "t = 463, loss = 1.0217821598052979\n",
      "t = 464, loss = 1.0217820405960083\n",
      "t = 465, loss = 1.0217818021774292\n",
      "t = 466, loss = 1.0217816829681396\n",
      "t = 467, loss = 1.02178156375885\n",
      "t = 468, loss = 1.021781325340271\n",
      "t = 469, loss = 1.0217812061309814\n",
      "t = 470, loss = 1.021781086921692\n",
      "t = 471, loss = 1.0217809677124023\n",
      "t = 472, loss = 1.0217807292938232\n",
      "t = 473, loss = 1.0217806100845337\n",
      "t = 474, loss = 1.0217803716659546\n",
      "t = 475, loss = 1.021780252456665\n",
      "t = 476, loss = 1.0217801332473755\n",
      "t = 477, loss = 1.0217798948287964\n",
      "t = 478, loss = 1.0217797756195068\n",
      "t = 479, loss = 1.0217796564102173\n",
      "t = 480, loss = 1.0217794179916382\n",
      "t = 481, loss = 1.0217792987823486\n",
      "t = 482, loss = 1.021779179573059\n",
      "t = 483, loss = 1.02177894115448\n",
      "t = 484, loss = 1.0217788219451904\n",
      "t = 485, loss = 1.0217787027359009\n",
      "t = 486, loss = 1.0217784643173218\n",
      "t = 487, loss = 1.0217783451080322\n",
      "t = 488, loss = 1.0217782258987427\n",
      "t = 489, loss = 1.0217779874801636\n",
      "t = 490, loss = 1.021777868270874\n",
      "t = 491, loss = 1.0217777490615845\n",
      "t = 492, loss = 1.0217775106430054\n",
      "t = 493, loss = 1.0217773914337158\n",
      "t = 494, loss = 1.0217772722244263\n",
      "t = 495, loss = 1.0217770338058472\n",
      "t = 496, loss = 1.0217769145965576\n",
      "t = 497, loss = 1.021776795387268\n",
      "t = 498, loss = 1.021776556968689\n",
      "t = 499, loss = 1.0217764377593994\n",
      "Training R2:  0.7045601853977743\n",
      "Test R2:  0.70327322088197\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_feat = len(features_cols)\n",
    "out_feat = len(targetColumns)\n",
    "model = Simple_Net(in_features=in_feat ,out_features=out_feat).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=100, gamma=0.1)\n",
    "\n",
    "losses,model = train(model, optimiser, scheduler, criterion, epochs = 500)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "    y_pred_train = model(X_train)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "\n",
    "train_r2 = r2_score(y_train.detach().cpu().numpy(), y_pred_train.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "test_r2=r2_score(y_test.detach().cpu().numpy(), y_pred_test.detach().cpu().numpy(), multioutput='variance_weighted')\n",
    "\n",
    "print(\"Training R2: \",train_r2)\n",
    "print(\"Test R2: \",test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
