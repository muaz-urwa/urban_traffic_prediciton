{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0012098123619624396,\n",
       " 'layers': 2,\n",
       " 'step_size': 21,\n",
       " 'gamma': 0.5302067528042456,\n",
       " 'bptt': 12,\n",
       " 'dropout': 0.35583243487203325}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'lr': 0.0012098123619624396,\n",
    " 'layers': 2,\n",
    " 'step_size': 21,\n",
    " 'gamma': 0.5302067528042456,\n",
    " 'bptt': 12,\n",
    " 'dropout': 0.35583243487203325}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/urwa/Documents/side_projects/urban/data/featureData/lga.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8757, 1045)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>...</th>\n",
       "      <th>91_lag_3</th>\n",
       "      <th>92_lag_3</th>\n",
       "      <th>93_lag_3</th>\n",
       "      <th>94_lag_3</th>\n",
       "      <th>95_lag_3</th>\n",
       "      <th>96_lag_3</th>\n",
       "      <th>97_lag_3</th>\n",
       "      <th>98_lag_3</th>\n",
       "      <th>99_lag_3</th>\n",
       "      <th>arrival_lag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 1045 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Hour  1  10  100  101  102  106  107  108  ...  91_lag_3  \\\n",
       "0  2018-01-01     3  0   0    0    0    0    0    0    0  ...       0.0   \n",
       "1  2018-01-01     4  1   0    0    0    0    0    0    0  ...       0.0   \n",
       "2  2018-01-01     5  1   0    0    0    0    0    0    0  ...       0.0   \n",
       "\n",
       "   92_lag_3  93_lag_3  94_lag_3  95_lag_3  96_lag_3  97_lag_3  98_lag_3  \\\n",
       "0       1.0       0.0       1.0       0.0       0.0       1.0       0.0   \n",
       "1       1.0       0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "2       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   99_lag_3  arrival_lag_3  \n",
       "0       0.0            3.0  \n",
       "1       0.0            0.0  \n",
       "2       0.0            1.0  \n",
       "\n",
       "[3 rows x 1045 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag_columns = [c for c in dataset.columns if 'lag' in c]\n",
    "len(lag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8757, 271)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[[c for c in dataset.columns if c not in lag_columns]]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DateColumns = ['Date']\n",
    "\n",
    "ext_columns = ['Dow', 'arrival','maxtemp', 'mintemp', 'avgtemp', 'departure', 'hdd',\n",
    "       'cdd', 'participation', 'newsnow', 'snowdepth', 'ifSnow']\n",
    "\n",
    "targetColumns = [c for c in dataset.columns if c not in ext_columns and \\\n",
    "                c not in DateColumns and c not in lag_columns and c != 'Hour']\n",
    "len(targetColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_cols = [c for c in dataset.columns if c not in targetColumns and c not in DateColumns]\n",
    "len(features_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6567\n",
      "(6567, 271)\n",
      "(2190, 271)\n"
     ]
    }
   ],
   "source": [
    "sep = int(0.75*len(dataset))\n",
    "print(sep)\n",
    "\n",
    "\n",
    "trainData = dataset[:sep]\n",
    "testData = dataset[sep:]\n",
    "\n",
    "print(trainData.shape)\n",
    "print(testData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6567, 13])\n",
      "torch.Size([6567, 257])\n",
      "torch.Size([2190, 13])\n",
      "torch.Size([2190, 257])\n"
     ]
    }
   ],
   "source": [
    "X_train = trainData[features_cols].values\n",
    "X_train = torch.tensor(X_train).float().to(device)\n",
    "print(X_train.shape)\n",
    "\n",
    "y_train = trainData[targetColumns].values\n",
    "y_train = torch.tensor(y_train).float().to(device)\n",
    "print(y_train.shape)\n",
    "\n",
    "X_test = testData[features_cols].values\n",
    "X_test = torch.tensor(X_test).float().to(device)\n",
    "print(X_test.shape)\n",
    "\n",
    "y_test = testData[targetColumns].values\n",
    "y_test = torch.tensor(y_test).float().to(device)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inout_sequences(x,y, tw):\n",
    "    inout_seq = []\n",
    "    L = len(x)\n",
    "    for i in range(L-tw):\n",
    "        train_seq_x = x[i:i+tw]\n",
    "        train_seq_y = y[i:i+tw]\n",
    "#         train_seq = torch.cat((train_seq_x,train_seq_y),axis=1)\n",
    "        \n",
    "#         train_label = y[i+tw:i+tw+1]\n",
    "        train_label = y[i+1:i+tw+1]\n",
    "        inout_seq.append((train_seq_x, train_seq_y ,train_label))\n",
    "    return inout_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inout_seq = create_inout_sequences(X_train,y_train, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 13]), torch.Size([12, 257]), torch.Size([12, 257]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inout_seq[0][0].shape,train_inout_seq[0][1].shape, train_inout_seq[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inout_seq = create_inout_sequences(X_test,y_test, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=100, num_layers=1, dropout=0):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.hidden_cell = (torch.zeros(num_layers,1,self.hidden_size).to(device),\n",
    "                    torch.zeros(num_layers,1,self.hidden_size).to(device))\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.hidden_cell = (torch.zeros(self.num_layers,1,self.hidden_size).to(device),\n",
    "                    torch.zeros(self.num_layers,1,self.hidden_size).to(device))\n",
    "           \n",
    "        lstm_out, self.hidden_cell = self.lstm(x.view(len(x) ,1, -1), self.hidden_cell)\n",
    "        \n",
    "        return lstm_out, self.hidden_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphPrediction(nn.Module):\n",
    "    def __init__(self, feat_size=1, hidden_layer_size=100, network_size=1, layers=1, communities=10, ensembles=1, dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # aggregation\n",
    "        self.attachment_matrix = torch.nn.Parameter(torch.randn(network_size,communities))\n",
    "        self.attachment_matrix.requires_grad = True\n",
    "        \n",
    "        lstm_input = communities + feat_size\n",
    "        \n",
    "        self.ensembles = ensembles\n",
    "        self.lstms = nn.ModuleList()\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(ensembles):\n",
    "             self.lstms.append(LSTM(input_size=lstm_input, hidden_size=hidden_layer_size, num_layers=layers))\n",
    "             self.linears.append(nn.Linear(hidden_layer_size, network_size))\n",
    "            \n",
    "\n",
    "    def forward(self, input_seq, feat):\n",
    "        \n",
    "        w = F.softmax(self.attachment_matrix, dim=1)\n",
    "        x = torch.matmul(input_seq, self.attachment_matrix)\n",
    "        x = torch.cat((x,feat),axis=1)\n",
    "        x = x.view(len(input_seq) ,1, -1)\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(self.ensembles):\n",
    "            if torch.randn(1) < 0.7 or i==0 or not self.training:\n",
    "                lstm_out, self.hidden_cell = self.lstms[i](x)\n",
    "                y = self.linears[i](lstm_out.view(len(input_seq), -1))\n",
    "                predictions.append(y)\n",
    "        \n",
    "        predictions = torch.stack(predictions)\n",
    "#         print(predictions.shape)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        for feat,seq, labels in test_inout_seq:\n",
    "#             model.hidden = (torch.zeros(layers, 1, model.hidden_layer_size),\n",
    "#                             torch.zeros(layers, 1, model.hidden_layer_size))\n",
    "            y = model(seq,feat).mean(dim=0)[-1]\n",
    "    \n",
    "            prediction.append(y)\n",
    "\n",
    "    y_test_ = torch.stack([labels[-1] for feat,seq, labels in test_inout_seq], axis=0).detach().cpu().numpy()\n",
    "    y_pred_ = torch.stack(prediction).detach().cpu().numpy()\n",
    "\n",
    "    r2 = r2_score(y_test_, y_pred_, multioutput='variance_weighted')\n",
    "    rmse = mean_squared_error(y_test_, y_pred_)\n",
    "    mae = mean_absolute_error(y_test_, y_pred_)\n",
    "#     print(\"r2: \",r2)\n",
    "    return (r2, rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 2\n",
    "communities = 24\n",
    "network_size = len(targetColumns)\n",
    "feat_size = len(features_cols)\n",
    "ensembles=10\n",
    "dropout = 0.35583243487203325\n",
    "\n",
    "model = GraphPrediction(feat_size = feat_size, hidden_layer_size=100,\n",
    "             network_size=network_size, layers=layers,\n",
    "            communities=communities, ensembles=ensembles, dropout=dropout).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.L1Loss()   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0012098123619624396)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=21, gamma=0.53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 loss: 1.38645327 r2: 0.687 rmse: 7.290 mae: 1.122\n",
      "epoch:   1 loss: 1.33436620 r2: 0.710 rmse: 6.769 mae: 1.080\n",
      "epoch:   2 loss: 1.32500970 r2: 0.717 rmse: 6.597 mae: 1.075\n",
      "epoch:   3 loss: 1.27309620 r2: 0.731 rmse: 6.281 mae: 1.056\n",
      "epoch:   4 loss: 1.28326321 r2: 0.734 rmse: 6.202 mae: 1.047\n",
      "epoch:   5 loss: 1.25893557 r2: 0.737 rmse: 6.124 mae: 1.045\n",
      "epoch:   6 loss: 1.23533165 r2: 0.742 rmse: 6.008 mae: 1.034\n",
      "epoch:   7 loss: 1.24017417 r2: 0.739 rmse: 6.075 mae: 1.033\n",
      "epoch:   8 loss: 1.25892913 r2: 0.744 rmse: 5.957 mae: 1.033\n",
      "epoch:   9 loss: 1.25921524 r2: 0.746 rmse: 5.929 mae: 1.031\n",
      "epoch:  10 loss: 1.20652556 r2: 0.752 rmse: 5.781 mae: 1.022\n",
      "epoch:  11 loss: 1.24395871 r2: 0.748 rmse: 5.868 mae: 1.024\n",
      "epoch:  12 loss: 1.22825789 r2: 0.751 rmse: 5.799 mae: 1.024\n",
      "epoch:  13 loss: 1.22031760 r2: 0.749 rmse: 5.854 mae: 1.022\n",
      "epoch:  14 loss: 1.19806409 r2: 0.755 rmse: 5.719 mae: 1.018\n",
      "epoch:  15 loss: 1.22132039 r2: 0.754 rmse: 5.743 mae: 1.019\n",
      "epoch:  16 loss: 1.22987258 r2: 0.752 rmse: 5.782 mae: 1.021\n",
      "epoch:  17 loss: 1.21044028 r2: 0.754 rmse: 5.735 mae: 1.022\n",
      "epoch:  18 loss: 1.20380080 r2: 0.754 rmse: 5.725 mae: 1.022\n",
      "epoch:  19 loss: 1.21957850 r2: 0.756 rmse: 5.683 mae: 1.015\n",
      "epoch:  20 loss: 1.24122918 r2: 0.757 rmse: 5.668 mae: 1.014\n",
      "epoch:  21 loss: 1.17049837 r2: 0.761 rmse: 5.578 mae: 1.005\n",
      "epoch:  22 loss: 1.15926242 r2: 0.760 rmse: 5.585 mae: 1.008\n",
      "epoch:  23 loss: 1.15113211 r2: 0.767 rmse: 5.443 mae: 1.002\n",
      "epoch:  24 loss: 1.14622366 r2: 0.763 rmse: 5.515 mae: 1.002\n",
      "epoch:  25 loss: 1.16461360 r2: 0.763 rmse: 5.534 mae: 1.003\n",
      "epoch:  26 loss: 1.16128075 r2: 0.764 rmse: 5.499 mae: 1.004\n",
      "epoch:  27 loss: 1.15323341 r2: 0.764 rmse: 5.490 mae: 1.001\n",
      "epoch:  28 loss: 1.16109633 r2: 0.764 rmse: 5.491 mae: 1.001\n",
      "epoch:  29 loss: 1.15615773 r2: 0.762 rmse: 5.550 mae: 1.005\n",
      "epoch:  30 loss: 1.13882661 r2: 0.768 rmse: 5.418 mae: 0.996\n",
      "epoch:  31 loss: 1.15277886 r2: 0.766 rmse: 5.465 mae: 1.002\n",
      "epoch:  32 loss: 1.14112842 r2: 0.766 rmse: 5.445 mae: 0.998\n",
      "epoch:  33 loss: 1.14362419 r2: 0.767 rmse: 5.421 mae: 1.000\n",
      "epoch:  34 loss: 1.15302944 r2: 0.765 rmse: 5.470 mae: 0.999\n",
      "epoch:  35 loss: 1.14244807 r2: 0.766 rmse: 5.455 mae: 0.998\n",
      "epoch:  36 loss: 1.13696539 r2: 0.768 rmse: 5.406 mae: 0.996\n",
      "epoch:  37 loss: 1.15244913 r2: 0.767 rmse: 5.433 mae: 0.999\n",
      "epoch:  38 loss: 1.13320363 r2: 0.768 rmse: 5.416 mae: 1.000\n",
      "epoch:  39 loss: 1.13616109 r2: 0.767 rmse: 5.430 mae: 1.000\n",
      "epoch:  40 loss: 1.14834213 r2: 0.766 rmse: 5.456 mae: 0.999\n",
      "epoch:  41 loss: 1.13328969 r2: 0.766 rmse: 5.453 mae: 1.001\n",
      "epoch:  42 loss: 1.12074685 r2: 0.769 rmse: 5.383 mae: 0.994\n",
      "epoch:  43 loss: 1.11848021 r2: 0.768 rmse: 5.413 mae: 0.994\n",
      "epoch:  44 loss: 1.13040435 r2: 0.768 rmse: 5.401 mae: 0.993\n",
      "epoch:  45 loss: 1.13138735 r2: 0.770 rmse: 5.371 mae: 0.992\n",
      "epoch:  46 loss: 1.11220896 r2: 0.771 rmse: 5.336 mae: 0.991\n",
      "epoch:  47 loss: 1.13081992 r2: 0.771 rmse: 5.349 mae: 0.992\n",
      "epoch:  48 loss: 1.11975515 r2: 0.768 rmse: 5.414 mae: 0.997\n",
      "epoch:  49 loss: 1.11335909 r2: 0.771 rmse: 5.349 mae: 0.993\n",
      "epoch:  50 loss: 1.10921347 r2: 0.770 rmse: 5.356 mae: 0.992\n",
      "epoch:  51 loss: 1.10796571 r2: 0.770 rmse: 5.372 mae: 0.994\n",
      "epoch:  52 loss: 1.11432528 r2: 0.770 rmse: 5.370 mae: 0.992\n",
      "epoch:  53 loss: 1.09998620 r2: 0.770 rmse: 5.359 mae: 0.993\n",
      "epoch:  54 loss: 1.10401988 r2: 0.769 rmse: 5.382 mae: 0.994\n",
      "epoch:  55 loss: 1.09631813 r2: 0.769 rmse: 5.386 mae: 0.994\n",
      "epoch:  56 loss: 1.09647274 r2: 0.771 rmse: 5.330 mae: 0.993\n",
      "epoch:  57 loss: 1.10171902 r2: 0.769 rmse: 5.373 mae: 0.993\n",
      "epoch:  58 loss: 1.10253775 r2: 0.769 rmse: 5.376 mae: 0.993\n",
      "epoch:  59 loss: 1.10810137 r2: 0.770 rmse: 5.355 mae: 0.993\n",
      "epoch:  60 loss: 1.10228097 r2: 0.770 rmse: 5.367 mae: 0.992\n",
      "epoch:  61 loss: 1.09438288 r2: 0.770 rmse: 5.364 mae: 0.993\n",
      "epoch:  62 loss: 1.09780371 r2: 0.770 rmse: 5.372 mae: 0.994\n",
      "epoch:  63 loss: 1.09561932 r2: 0.769 rmse: 5.377 mae: 0.993\n",
      "epoch:  64 loss: 1.09335232 r2: 0.770 rmse: 5.373 mae: 0.993\n",
      "epoch:  65 loss: 1.08102667 r2: 0.769 rmse: 5.377 mae: 0.993\n",
      "epoch:  66 loss: 1.08999312 r2: 0.769 rmse: 5.377 mae: 0.993\n",
      "epoch:  67 loss: 1.08407414 r2: 0.769 rmse: 5.383 mae: 0.993\n",
      "epoch:  68 loss: 1.08047152 r2: 0.768 rmse: 5.408 mae: 0.993\n",
      "epoch:  69 loss: 1.08004224 r2: 0.769 rmse: 5.383 mae: 0.994\n",
      "epoch:  70 loss: 1.08303189 r2: 0.768 rmse: 5.415 mae: 0.994\n",
      "epoch:  71 loss: 1.08550906 r2: 0.768 rmse: 5.400 mae: 0.994\n",
      "epoch:  72 loss: 1.08005893 r2: 0.768 rmse: 5.402 mae: 0.994\n",
      "epoch:  73 loss: 1.07026112 r2: 0.769 rmse: 5.391 mae: 0.993\n",
      "epoch:  74 loss: 1.07311916 r2: 0.768 rmse: 5.402 mae: 0.994\n",
      "epoch:  75 loss: 1.07101870 r2: 0.769 rmse: 5.374 mae: 0.993\n",
      "epoch:  76 loss: 1.07381558 r2: 0.768 rmse: 5.401 mae: 0.994\n",
      "epoch:  77 loss: 1.06194830 r2: 0.769 rmse: 5.393 mae: 0.993\n",
      "epoch:  78 loss: 1.06698883 r2: 0.769 rmse: 5.374 mae: 0.993\n",
      "epoch:  79 loss: 1.07346177 r2: 0.768 rmse: 5.405 mae: 0.993\n",
      "epoch:  80 loss: 1.06968176 r2: 0.769 rmse: 5.392 mae: 0.993\n",
      "epoch:  81 loss: 1.06619287 r2: 0.769 rmse: 5.379 mae: 0.993\n",
      "epoch:  82 loss: 1.06766438 r2: 0.769 rmse: 5.393 mae: 0.993\n",
      "epoch:  83 loss: 1.06496835 r2: 0.769 rmse: 5.386 mae: 0.993\n",
      "epoch:  84 loss: 1.06946754 r2: 0.768 rmse: 5.405 mae: 0.994\n",
      "epoch:  85 loss: 1.06380355 r2: 0.768 rmse: 5.411 mae: 0.994\n",
      "epoch:  86 loss: 1.06357801 r2: 0.768 rmse: 5.398 mae: 0.994\n",
      "epoch:  87 loss: 1.06694520 r2: 0.768 rmse: 5.409 mae: 0.994\n",
      "epoch:  88 loss: 1.06503487 r2: 0.768 rmse: 5.404 mae: 0.994\n",
      "epoch:  89 loss: 1.05933261 r2: 0.768 rmse: 5.411 mae: 0.994\n",
      "epoch:  90 loss: 1.06099808 r2: 0.768 rmse: 5.403 mae: 0.994\n",
      "epoch:  91 loss: 1.05723631 r2: 0.768 rmse: 5.404 mae: 0.994\n",
      "epoch:  92 loss: 1.05715275 r2: 0.768 rmse: 5.398 mae: 0.994\n",
      "epoch:  93 loss: 1.06093085 r2: 0.768 rmse: 5.399 mae: 0.993\n",
      "epoch:  94 loss: 1.05753195 r2: 0.768 rmse: 5.415 mae: 0.994\n",
      "epoch:  95 loss: 1.05918455 r2: 0.768 rmse: 5.412 mae: 0.994\n",
      "epoch:  96 loss: 1.05560338 r2: 0.768 rmse: 5.399 mae: 0.994\n",
      "epoch:  97 loss: 1.05751252 r2: 0.768 rmse: 5.405 mae: 0.994\n",
      "epoch:  98 loss: 1.05073559 r2: 0.768 rmse: 5.405 mae: 0.993\n",
      "epoch:  99 loss: 1.05906427 r2: 0.768 rmse: 5.417 mae: 0.994\n",
      "epoch: 100 loss: 1.05222726 r2: 0.768 rmse: 5.408 mae: 0.994\n",
      "epoch: 101 loss: 1.05605888 r2: 0.768 rmse: 5.398 mae: 0.993\n",
      "epoch: 102 loss: 1.05510068 r2: 0.768 rmse: 5.405 mae: 0.994\n",
      "epoch: 103 loss: 1.05325484 r2: 0.768 rmse: 5.408 mae: 0.994\n",
      "epoch: 104 loss: 1.05442393 r2: 0.768 rmse: 5.408 mae: 0.994\n",
      "epoch: 105 loss: 1.05648923 r2: 0.768 rmse: 5.403 mae: 0.994\n",
      "epoch: 106 loss: 1.05728674 r2: 0.768 rmse: 5.400 mae: 0.993\n",
      "epoch: 107 loss: 1.05543303 r2: 0.769 rmse: 5.395 mae: 0.993\n",
      "epoch: 108 loss: 1.05468655 r2: 0.768 rmse: 5.397 mae: 0.993\n",
      "epoch: 109 loss: 1.05561912 r2: 0.768 rmse: 5.397 mae: 0.993\n",
      "epoch: 110 loss: 1.05698693 r2: 0.769 rmse: 5.393 mae: 0.993\n",
      "epoch: 111 loss: 1.05320537 r2: 0.769 rmse: 5.392 mae: 0.993\n",
      "epoch: 112 loss: 1.05567646 r2: 0.769 rmse: 5.394 mae: 0.993\n",
      "epoch: 113 loss: 1.05279052 r2: 0.769 rmse: 5.395 mae: 0.993\n",
      "epoch: 114 loss: 1.05553687 r2: 0.769 rmse: 5.394 mae: 0.993\n",
      "epoch: 115 loss: 1.05673110 r2: 0.769 rmse: 5.391 mae: 0.993\n",
      "epoch: 116 loss: 1.05351150 r2: 0.768 rmse: 5.405 mae: 0.994\n",
      "epoch: 117 loss: 1.05182040 r2: 0.769 rmse: 5.394 mae: 0.993\n",
      "epoch: 118 loss: 1.04926050 r2: 0.768 rmse: 5.403 mae: 0.993\n",
      "epoch: 119 loss: 1.05419874 r2: 0.768 rmse: 5.398 mae: 0.993\n",
      "epoch: 120 loss: 1.05205750 r2: 0.768 rmse: 5.401 mae: 0.993\n",
      "epoch: 121 loss: 1.04754853 r2: 0.768 rmse: 5.399 mae: 0.993\n",
      "epoch: 122 loss: 1.05173922 r2: 0.768 rmse: 5.401 mae: 0.993\n",
      "epoch: 123 loss: 1.04990852 r2: 0.768 rmse: 5.400 mae: 0.993\n",
      "epoch: 124 loss: 1.05155551 r2: 0.768 rmse: 5.403 mae: 0.993\n",
      "epoch: 125 loss: 1.05258214 r2: 0.768 rmse: 5.401 mae: 0.993\n",
      "epoch: 126 loss: 1.05816615 r2: 0.769 rmse: 5.389 mae: 0.993\n",
      "epoch: 127 loss: 1.05664098 r2: 0.769 rmse: 5.386 mae: 0.993\n",
      "epoch: 128 loss: 1.05485034 r2: 0.769 rmse: 5.390 mae: 0.993\n",
      "epoch: 129 loss: 1.05268157 r2: 0.769 rmse: 5.391 mae: 0.993\n",
      "epoch: 130 loss: 1.05342436 r2: 0.769 rmse: 5.387 mae: 0.993\n",
      "epoch: 131 loss: 1.05281520 r2: 0.769 rmse: 5.391 mae: 0.993\n",
      "epoch: 132 loss: 1.05295002 r2: 0.769 rmse: 5.391 mae: 0.993\n",
      "epoch: 133 loss: 1.05779719 r2: 0.769 rmse: 5.391 mae: 0.993\n",
      "epoch: 134 loss: 1.05170763 r2: 0.769 rmse: 5.394 mae: 0.993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 135 loss: 1.05706239 r2: 0.769 rmse: 5.393 mae: 0.993\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    for feat,seq, labels in train_inout_seq:\n",
    "        optimizer.zero_grad()\n",
    "#         model.hidden_cell = (torch.zeros(layers, 1, model.hidden_layer_size).to(device),\n",
    "#                         torch.zeros(layers, 1, model.hidden_layer_size).to(device))\n",
    "\n",
    "        y_pred = model(seq, feat)\n",
    "        labels = labels.repeat(y_pred.shape[0],1,1)\n",
    "        \n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "#     if i%1 == 1:\n",
    "    r2, rmse, mae = evaluate(model)\n",
    "    print(f'epoch: {i:3} loss: {single_loss.item():10.8f} r2: {r2:5.3f} rmse: {rmse:5.3f} mae: {mae:5.3f}')\n",
    "    torch.save(model.state_dict(), 'lga.pt')\n",
    "    \n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best value 0.771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attachment = torch.argmax(F.softmax(model.attachment_matrix, dim=1), dim=1).detach().cpu().numpy().astype(str)\n",
    "community_assignment = dict(zip(targetColumns, attachment))\n",
    "community_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('lga_attachment.json', 'w') as fp:\n",
    "    json.dump(community_assignment, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(attachment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 linear\n",
    "# 0.47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 linear\n",
    "# 0.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 linear + RELU\n",
    "# 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 linear bptt = 24\n",
    "# 0.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble\n",
    "#0.53228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO\n",
    "# other hubs\n",
    "# ensemble model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
